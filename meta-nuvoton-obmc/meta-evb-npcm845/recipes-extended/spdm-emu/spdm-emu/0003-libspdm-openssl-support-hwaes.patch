From e33059d9b0d8dc8ed9de7b0afa6ca6c929ebef7d Mon Sep 17 00:00:00 2001
From: Stanley Chu <yschu@nuvoton.com>
Date: Thu, 8 Jun 2023 10:21:33 +0800
Subject: [PATCH] patch libspdm

Signed-off-by: Stanley Chu <yschu@nuvoton.com>
---
 .../libspdm_secmes_encode_decode.c            |   19 +-
 os_stub/cryptlib_openssl/CMakeLists.txt       |   18 +-
 os_stub/openssllib/CMakeLists.txt             |   25 +-
 os_stub/openssllib/include/crt_support.h      |    3 +-
 .../include/openssl/configuration.h           |    3 +
 .../openssllib/openssl_asm/aes-gcm-armv8_64.S | 6017 +++++++++++++++++
 os_stub/openssllib/openssl_asm/aesv8-armx.S   | 3178 +++++++++
 os_stub/openssllib/openssl_asm/arm64cpuid.S   |  129 +
 os_stub/openssllib/openssl_asm/ghashv8-armx.S |  552 ++
 os_stub/openssllib/process_files.pl           |    1 -
 10 files changed, 9934 insertions(+), 11 deletions(-)
 create mode 100644 os_stub/openssllib/openssl_asm/aes-gcm-armv8_64.S
 create mode 100644 os_stub/openssllib/openssl_asm/aesv8-armx.S
 create mode 100644 os_stub/openssllib/openssl_asm/arm64cpuid.S
 create mode 100644 os_stub/openssllib/openssl_asm/ghashv8-armx.S

diff --git a/library/spdm_secured_message_lib/libspdm_secmes_encode_decode.c b/library/spdm_secured_message_lib/libspdm_secmes_encode_decode.c
index 818a30f6f..ce695f9f6 100644
--- a/library/spdm_secured_message_lib/libspdm_secmes_encode_decode.c
+++ b/library/spdm_secured_message_lib/libspdm_secmes_encode_decode.c
@@ -4,6 +4,7 @@
  *  License: BSD 3-Clause License. For full text see link: https://github.com/DMTF/libspdm/blob/main/LICENSE.md
  **/
 
+#include <sys/time.h>
 #include "internal/libspdm_secured_message_lib.h"
 
 /**
@@ -35,7 +36,7 @@ libspdm_return_t libspdm_encode_secured_message(
     libspdm_secured_message_context_t *secured_message_context;
     size_t total_secured_message_size;
     size_t plain_text_size;
-    size_t cipher_text_size;
+    size_t cipher_text_size, data_size;
     size_t aead_tag_size;
     size_t aead_key_size;
     size_t aead_iv_size;
@@ -60,6 +61,8 @@ libspdm_return_t libspdm_encode_secured_message(
     libspdm_session_state_t session_state;
     spdm_version_number_t secured_spdm_version;
     uint8_t version;
+    struct timeval start, end;
+    unsigned int diff;
 
     secured_message_context = spdm_secured_message_context;
     secured_spdm_version = spdm_secured_message_callbacks->get_secured_spdm_version(
@@ -214,12 +217,17 @@ libspdm_return_t libspdm_encode_secured_message(
         tag = (uint8_t *)record_header1 + record_header_size +
               cipher_text_size;
 
+        gettimeofday(&start,NULL);
+	data_size = cipher_text_size;
         result = libspdm_aead_encryption(
             secured_message_context->secured_message_version,
             secured_message_context->aead_cipher_suite, key,
             aead_key_size, salt, aead_iv_size, (uint8_t *)a_data,
             record_header_size, dec_msg, cipher_text_size, tag,
             aead_tag_size, enc_msg, &cipher_text_size);
+	gettimeofday(&end,NULL);
+	diff = 1000000 * (end.tv_sec-start.tv_sec)+ (end.tv_usec-start.tv_usec);
+	LIBSPDM_DEBUG((LIBSPDM_DEBUG_ERROR,"libspdm encrypt %lu bytes spends %u us\n", data_size, diff));
         break;
 
     case LIBSPDM_SESSION_TYPE_MAC_ONLY:
@@ -299,7 +307,7 @@ libspdm_return_t libspdm_decode_secured_message(
 {
     libspdm_secured_message_context_t *secured_message_context;
     size_t plain_text_size;
-    size_t cipher_text_size;
+    size_t cipher_text_size, data_size;
     size_t aead_tag_size;
     size_t aead_key_size;
     size_t aead_iv_size;
@@ -323,6 +331,8 @@ libspdm_return_t libspdm_decode_secured_message(
     libspdm_error_struct_t spdm_error;
     spdm_version_number_t secured_spdm_version;
     uint8_t version;
+    struct timeval start, end;
+    unsigned int diff;
 
     spdm_error.error_code = 0;
     spdm_error.session_id = 0;
@@ -476,12 +486,17 @@ libspdm_return_t libspdm_decode_secured_message(
         dec_msg = (uint8_t *)*app_message;
         enc_msg_header = (void *)dec_msg;
         tag = (const uint8_t *)record_header1 + record_header_size + cipher_text_size;
+        gettimeofday(&start,NULL);
+	data_size = cipher_text_size;
         result = libspdm_aead_decryption(
             secured_message_context->secured_message_version,
             secured_message_context->aead_cipher_suite, key,
             aead_key_size, salt, aead_iv_size, a_data,
             record_header_size, enc_msg, cipher_text_size, tag,
             aead_tag_size, dec_msg, &cipher_text_size);
+	gettimeofday(&end,NULL);
+	diff = 1000000 * (end.tv_sec-start.tv_sec)+ (end.tv_usec-start.tv_usec);
+	LIBSPDM_DEBUG((LIBSPDM_DEBUG_ERROR,"libspdm decrypt %lu bytes spends %u us\n", data_size, diff));
         if (!result) {
             /* Backup keys are valid, fail and alert rollback and retry is possible. */
             if ((is_requester && secured_message_context->requester_backup_valid) ||
diff --git a/os_stub/cryptlib_openssl/CMakeLists.txt b/os_stub/cryptlib_openssl/CMakeLists.txt
index 87e16b3e6..1d246712f 100644
--- a/os_stub/cryptlib_openssl/CMakeLists.txt
+++ b/os_stub/cryptlib_openssl/CMakeLists.txt
@@ -11,10 +11,18 @@ INCLUDE_DIRECTORIES(${LIBSPDM_DIR}/include
                     ${LIBSPDM_DIR}/os_stub/openssllib/openssl
 )
 
-if (ARCH STREQUAL "ia32")
-    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_IA32)
-elseif (ARCH STREQUAL "x64")
+if(ARCH STREQUAL "x64")
     ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_X64)
+elseif(ARCH STREQUAL "ia32")
+    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_IA32)
+elseif(ARCH STREQUAL "aarch64")
+    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_AARCH64)
+elseif(ARCH STREQUAL "riscv32")
+    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_RISCV32)
+elseif(ARCH STREQUAL "riscv64")
+    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_RISCV64)
+else()
+    MESSAGE(FATAL_ERROR "Unknown ARCH")
 endif()
 
 SET(src_cryptlib_openssl
@@ -43,5 +51,9 @@ SET(src_cryptlib_openssl
     sys_call/crt_wrapper_host.c
 )
 
+if ((ARCH STREQUAL "arm") OR (ARCH STREQUAL "aarch64"))
+    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_ARM)
+endif()
+
 ADD_LIBRARY(cryptlib_openssl STATIC ${src_cryptlib_openssl})
 TARGET_COMPILE_OPTIONS(cryptlib_openssl PRIVATE ${OPENSSL_FLAGS})
diff --git a/os_stub/openssllib/CMakeLists.txt b/os_stub/openssllib/CMakeLists.txt
index f9070f9d1..abccf8f3d 100644
--- a/os_stub/openssllib/CMakeLists.txt
+++ b/os_stub/openssllib/CMakeLists.txt
@@ -16,8 +16,13 @@ if (ARCH STREQUAL "ia32")
     ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_IA32)
 elseif (ARCH STREQUAL "x64")
     ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_X64)
+elseif(ARCH STREQUAL "aarch64")
+    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_AARCH64 -DOPENSSL_CPUID_OBJ -DLIBSPDM_CPU_ARM)
+else()
+    MESSAGE(FATAL_ERROR "Unknown ARCH")
 endif()
 
+
 INCLUDE_DIRECTORIES(${LIBSPDM_DIR}/include
                     ${LIBSPDM_DIR}/include/hal
                     ${LIBSPDM_DIR}/os_stub/include
@@ -78,6 +83,8 @@ INCLUDE_DIRECTORIES(${LIBSPDM_DIR}/include
 )
 
 SET(src_openssllib
+    openssl/crypto/cpuid.c
+    openssl/crypto/armcap.c
     openssl/crypto/aes/aes_cbc.c
     openssl/crypto/aes/aes_cfb.c
     openssl/crypto/aes/aes_core.c
@@ -398,7 +405,7 @@ SET(src_openssllib
     openssl/crypto/md5/md5_one.c
     openssl/crypto/md5/md5_sha1.c
     openssl/crypto/mem.c
-    openssl/crypto/mem_clr.c
+#    openssl/crypto/mem_clr.c
     openssl/crypto/mem_sec.c
     openssl/crypto/modes/cbc128.c
     openssl/crypto/modes/ccm128.c
@@ -742,8 +749,18 @@ SET(src_openssllib
     ossl_store.c
 )
 
-if ((ARCH STREQUAL "arm") OR (ARCH STREQUAL "aarch64"))
-    ADD_COMPILE_OPTIONS(-DLIBSPDM_CPU_ARM)
+if(ARCH STREQUAL "aarch64")
+    enable_language(C ASM)
+    SET(other_src_openssllib
+        openssl_asm/arm64cpuid.S
+        openssl_asm/aesv8-armx.S
+        openssl_asm/ghashv8-armx.S
+        openssl_asm/aes-gcm-armv8_64.S
+    )
+else()
+    SET(other_src_openssllib
+        openssl/crypto/mem_clr.c
+    )
 endif()
 
-ADD_LIBRARY(openssllib STATIC ${src_openssllib})
+ADD_LIBRARY(openssllib STATIC ${src_openssllib} ${other_src_openssllib})
diff --git a/os_stub/openssllib/include/crt_support.h b/os_stub/openssllib/include/crt_support.h
index 6b2ec92c4..76f4bef27 100644
--- a/os_stub/openssllib/include/crt_support.h
+++ b/os_stub/openssllib/include/crt_support.h
@@ -237,7 +237,8 @@ typedef char *LIBSPDM_VA_LIST;
 
 typedef size_t u_int;
 #if defined(__GNUC__) && !defined(__MINGW64__)
-typedef size_t time_t; /* time_t is 4 bytes for 32bit machine and 8 bytes for 64bit machine */
+//typedef size_t time_t; /* time_t is 4 bytes for 32bit machine and 8 bytes for 64bit machine */
+typedef __time_t time_t; /* time_t is 4 bytes for 32bit machine and 8 bytes for 64bit machine */
 #endif
 typedef uint8_t __uint8_t;
 typedef uint8_t sa_family_t;
diff --git a/os_stub/openssllib/include/openssl/configuration.h b/os_stub/openssllib/include/openssl/configuration.h
index bbff00539..be2920d1f 100644
--- a/os_stub/openssllib/include/openssl/configuration.h
+++ b/os_stub/openssllib/include/openssl/configuration.h
@@ -12,6 +12,9 @@
 extern "C" {
 # endif
 
+#include <stdint.h>
+#include <stddef.h>
+
 # ifdef OPENSSL_ALGORITHM_DEFINES
 #  error OPENSSL_ALGORITHM_DEFINES no longer supported
 # endif
diff --git a/os_stub/openssllib/openssl_asm/aes-gcm-armv8_64.S b/os_stub/openssllib/openssl_asm/aes-gcm-armv8_64.S
new file mode 100644
index 000000000..3caabfa59
--- /dev/null
+++ b/os_stub/openssllib/openssl_asm/aes-gcm-armv8_64.S
@@ -0,0 +1,6017 @@
+#include "arm_arch.h"
+
+#if __ARM_MAX_ARCH__>=8
+.arch	armv8-a+crypto
+.text
+.globl	aes_gcm_enc_128_kernel
+.type	aes_gcm_enc_128_kernel,%function
+.align	4
+aes_gcm_enc_128_kernel:
+	cbz	x1, .L128_enc_ret
+	stp	x19, x20, [sp, #-112]!
+	mov	x16, x4
+	mov	x8, x5
+	stp	x21, x22, [sp, #16]
+	stp	x23, x24, [sp, #32]
+	stp	d8, d9, [sp, #48]
+	stp	d10, d11, [sp, #64]
+	stp	d12, d13, [sp, #80]
+	stp	d14, d15, [sp, #96]
+
+	ldp	x10, x11, [x16]              //ctr96_b64, ctr96_t32
+	ldp	x13, x14, [x8, #160]                     //load rk10
+
+	ld1	{v11.16b}, [x3]
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	lsr	x5, x1, #3              //byte_len
+	mov	x15, x5
+
+	ldr	q27, [x8, #144]                                //load rk9
+	add	x4, x0, x1, lsr #3   //end_input_ptr
+	sub	x5, x5, #1      //byte_len - 1
+
+	lsr	x12, x11, #32
+	ldr	q15, [x3, #112]                        //load h4l | h4h
+	ext	v15.16b, v15.16b, v15.16b, #8
+
+	fmov	d1, x10                               //CTR block 1
+	rev	w12, w12                                //rev_ctr32
+
+	add	w12, w12, #1                            //increment rev_ctr32
+	orr	w11, w11, w11
+	ldr	q18, [x8, #0]                                  //load rk0
+
+	rev	w9, w12                                 //CTR block 1
+	add	w12, w12, #1                            //CTR block 1
+	fmov	d3, x10                               //CTR block 3
+
+	orr	x9, x11, x9, lsl #32            //CTR block 1
+	ld1	{ v0.16b}, [x16]                             //special case vector load initial counter so we can start first AES block as quickly as possible
+
+	fmov	v1.d[1], x9                               //CTR block 1
+	rev	w9, w12                                 //CTR block 2
+
+	fmov	d2, x10                               //CTR block 2
+	orr	x9, x11, x9, lsl #32            //CTR block 2
+	add	w12, w12, #1                            //CTR block 2
+
+	fmov	v2.d[1], x9                               //CTR block 2
+	rev	w9, w12                                 //CTR block 3
+
+	orr	x9, x11, x9, lsl #32            //CTR block 3
+	ldr	q19, [x8, #16]                                 //load rk1
+
+	add	w12, w12, #1                            //CTR block 3
+	fmov	v3.d[1], x9                               //CTR block 3
+
+	ldr	q14, [x3, #80]                         //load h3l | h3h
+	ext	v14.16b, v14.16b, v14.16b, #8
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 0
+	ldr	q20, [x8, #32]                                 //load rk2
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 0
+	ldr	q12, [x3, #32]                         //load h1l | h1h
+	ext	v12.16b, v12.16b, v12.16b, #8
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 0
+	ldr	q26, [x8, #128]                                //load rk8
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 0
+	ldr	q21, [x8, #48]                                 //load rk3
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 1
+	trn2	v17.2d,  v14.2d,    v15.2d                      //h4l | h3l
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 1
+	ldr	q24, [x8, #96]                                 //load rk6
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 1
+	ldr	q25, [x8, #112]                                //load rk7
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 1
+	trn1	v9.2d, v14.2d,    v15.2d                      //h4h | h3h
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 2
+	ldr	q23, [x8, #80]                                 //load rk5
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 2
+	ldr	q13, [x3, #64]                         //load h2l | h2h
+	ext	v13.16b, v13.16b, v13.16b, #8
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 2
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 2
+	eor	v17.16b, v17.16b, v9.16b                  //h4k | h3k
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 3
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 3
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 3
+	ldr	q22, [x8, #64]                                 //load rk4
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 3
+
+	and	x5, x5, #0xffffffffffffffc0    //number of bytes to be processed in main loop (at least 1 byte must be handled by tail)
+	trn2	v16.2d,  v12.2d,    v13.2d                      //h2l | h1l
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 4
+	add	x5, x5, x0
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 4
+	cmp	x0, x5                   //check if we have <= 4 blocks
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 4
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 5
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 5
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 6
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 4
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 6
+	trn1	v8.2d,    v12.2d,    v13.2d                      //h2h | h1h
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 6
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 5
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 7
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 7
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 6
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 7
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 8
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 7
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 8
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 8
+
+	aese	v2.16b, v27.16b                                      //AES block 2 - round 9
+
+	aese	v0.16b, v27.16b                                      //AES block 0 - round 9
+
+	eor	v16.16b, v16.16b, v8.16b                     //h2k | h1k
+
+	aese	v1.16b, v27.16b                                      //AES block 1 - round 9
+
+	aese	v3.16b, v27.16b                                      //AES block 3 - round 9
+	b.ge	.L128_enc_tail                                    //handle tail
+
+	ldp	x6, x7, [x0, #0]            //AES block 0 - load plaintext
+
+	ldp	x21, x22, [x0, #32]           //AES block 2 - load plaintext
+
+	ldp	x19, x20, [x0, #16]           //AES block 1 - load plaintext
+
+	ldp	x23, x24, [x0, #48]           //AES block 3 - load plaintext
+
+	eor	x6, x6, x13                     //AES block 0 - round 10 low
+	eor	x7, x7, x14                     //AES block 0 - round 10 high
+
+	eor	x21, x21, x13                     //AES block 2 - round 10 low
+	fmov	d4, x6                               //AES block 0 - mov low
+
+	eor	x19, x19, x13                     //AES block 1 - round 10 low
+	eor	x22, x22, x14                     //AES block 2 - round 10 high
+	fmov	v4.d[1], x7                           //AES block 0 - mov high
+
+	fmov	d5, x19                               //AES block 1 - mov low
+	eor	x20, x20, x14                     //AES block 1 - round 10 high
+
+	eor	x23, x23, x13                     //AES block 3 - round 10 low
+	fmov	v5.d[1], x20                           //AES block 1 - mov high
+
+	fmov	d6, x21                               //AES block 2 - mov low
+	eor	x24, x24, x14                     //AES block 3 - round 10 high
+	rev	w9, w12                                 //CTR block 4
+
+	fmov	v6.d[1], x22                           //AES block 2 - mov high
+	orr	x9, x11, x9, lsl #32            //CTR block 4
+
+	eor	v4.16b, v4.16b, v0.16b                          //AES block 0 - result
+	fmov	d0, x10                               //CTR block 4
+	add	w12, w12, #1                            //CTR block 4
+
+	fmov	v0.d[1], x9                               //CTR block 4
+	rev	w9, w12                                 //CTR block 5
+
+	eor	v5.16b, v5.16b, v1.16b                          //AES block 1 - result
+	fmov	d1, x10                               //CTR block 5
+	orr	x9, x11, x9, lsl #32            //CTR block 5
+
+	add	w12, w12, #1                            //CTR block 5
+	add	x0, x0, #64                       //AES input_ptr update
+	fmov	v1.d[1], x9                               //CTR block 5
+
+	fmov	d7, x23                               //AES block 3 - mov low
+	rev	w9, w12                                 //CTR block 6
+	st1	{ v4.16b}, [x2], #16                     //AES block 0 - store result
+
+	fmov	v7.d[1], x24                           //AES block 3 - mov high
+	orr	x9, x11, x9, lsl #32            //CTR block 6
+
+	add	w12, w12, #1                            //CTR block 6
+	eor	v6.16b, v6.16b, v2.16b                          //AES block 2 - result
+	st1	{ v5.16b}, [x2], #16                     //AES block 1 - store result
+
+	fmov	d2, x10                               //CTR block 6
+	cmp	x0, x5                   //check if we have <= 8 blocks
+
+	fmov	v2.d[1], x9                               //CTR block 6
+	rev	w9, w12                                 //CTR block 7
+	st1	{ v6.16b}, [x2], #16                     //AES block 2 - store result
+
+	orr	x9, x11, x9, lsl #32            //CTR block 7
+
+	eor	v7.16b, v7.16b, v3.16b                          //AES block 3 - result
+	st1	{ v7.16b}, [x2], #16                     //AES block 3 - store result
+	b.ge	.L128_enc_prepretail                              //do prepretail
+
+.L128_enc_main_loop:	//main	loop start
+	ldp	x23, x24, [x0, #48]           //AES block 4k+3 - load plaintext
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k (only t0 is free)
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2 (t0, t1, and t2 free)
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	fmov	d3, x10                               //CTR block 4k+3
+
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+1 (t0 and t1 free)
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	add	w12, w12, #1                            //CTR block 4k+3
+	fmov	v3.d[1], x9                               //CTR block 4k+3
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	mov	d31, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	mov	d30, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	eor	x24, x24, x14                     //AES block 4k+3 - round 10 high
+
+	pmull2	v28.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+	eor	v31.8b, v31.8b, v6.8b                          //GHASH block 4k+2 - mid
+	ldp	x6, x7, [x0, #0]            //AES block 4k+4 - load plaintext
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	rev	w9, w12                                 //CTR block 4k+8
+
+	eor	v30.8b, v30.8b, v5.8b                          //GHASH block 4k+1 - mid
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+8
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	add	w12, w12, #1                            //CTR block 4k+8
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	eor	v9.16b, v9.16b, v28.16b                         //GHASH block 4k+1 - high
+
+	pmull	v28.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3 (t0, t1, t2 and t3 free)
+
+	pmull	v30.1q, v30.1d, v17.1d                          //GHASH block 4k+1 - mid
+
+	pmull	v29.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+	ins	v31.d[1], v31.d[0]                                //GHASH block 4k+2 - mid
+
+	pmull2	v8.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+	eor	x7, x7, x14                     //AES block 4k+4 - round 10 high
+
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+1 - mid
+	mov	d30, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+1 - low
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+	eor	x6, x6, x13                     //AES block 4k+4 - round 10 low
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+	eor	v30.8b, v30.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	pmull2	v4.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+	eor	v9.16b, v9.16b, v8.16b                         //GHASH block 4k+2 - high
+
+	pmull2	v31.1q, v31.2d, v16.2d                          //GHASH block 4k+2 - mid
+
+	pmull	v29.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+	movi	v8.8b, #0xc2
+
+	pmull	v30.1q, v30.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	v11.16b, v11.16b, v28.16b                         //GHASH block 4k+2 - low
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+3 - high
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+	ldp	x19, x20, [x0, #16]           //AES block 4k+5 - load plaintext
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	eor	v10.16b, v10.16b, v31.16b                         //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	ldp	x21, x22, [x0, #32]           //AES block 4k+6 - load plaintext
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+3 - low
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	eor	x19, x19, x13                     //AES block 4k+5 - round 10 low
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+	eor	x23, x23, x13                     //AES block 4k+3 - round 10 low
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	fmov	d4, x6                               //AES block 4k+4 - mov low
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	fmov	v4.d[1], x7                           //AES block 4k+4 - mov high
+
+	add	x0, x0, #64                       //AES input_ptr update
+	fmov	d7, x23                               //AES block 4k+3 - mov low
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+	fmov	d5, x19                               //AES block 4k+5 - mov low
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+	eor	x20, x20, x14                     //AES block 4k+5 - round 10 high
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+	fmov	v5.d[1], x20                           //AES block 4k+5 - mov high
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	fmov	v7.d[1], x24                           //AES block 4k+3 - mov high
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	cmp	x0, x5                   //.LOOP CONTROL
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	aese	v0.16b, v27.16b                                      //AES block 4k+4 - round 9
+	eor	x21, x21, x13                     //AES block 4k+6 - round 10 low
+	eor	x22, x22, x14                     //AES block 4k+6 - round 10 high
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	fmov	d6, x21                               //AES block 4k+6 - mov low
+
+	aese	v1.16b, v27.16b                                      //AES block 4k+5 - round 9
+	fmov	v6.d[1], x22                           //AES block 4k+6 - mov high
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	eor	v4.16b, v4.16b, v0.16b                          //AES block 4k+4 - result
+
+	fmov	d0, x10                               //CTR block 4k+8
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+
+	fmov	v0.d[1], x9                               //CTR block 4k+8
+	rev	w9, w12                                 //CTR block 4k+9
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+	eor	v5.16b, v5.16b, v1.16b                          //AES block 4k+5 - result
+
+	add	w12, w12, #1                            //CTR block 4k+9
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+9
+	fmov	d1, x10                               //CTR block 4k+9
+
+	pmull	v9.1q, v10.1d, v8.1d            //MODULO - mid 64b align with low
+	fmov	v1.d[1], x9                               //CTR block 4k+9
+	rev	w9, w12                                 //CTR block 4k+10
+
+	aese	v2.16b, v27.16b                                      //AES block 4k+6 - round 9
+	st1	{ v4.16b}, [x2], #16                     //AES block 4k+4 - store result
+	eor	v6.16b, v6.16b, v2.16b                          //AES block 4k+6 - result
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+10
+
+	aese	v3.16b, v27.16b                                      //AES block 4k+7 - round 9
+	add	w12, w12, #1                            //CTR block 4k+10
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+	fmov	d2, x10                               //CTR block 4k+10
+
+	eor	v11.16b, v11.16b, v9.16b                         //MODULO - fold into low
+	st1	{ v5.16b}, [x2], #16                     //AES block 4k+5 - store result
+
+	fmov	v2.d[1], x9                               //CTR block 4k+10
+	st1	{ v6.16b}, [x2], #16                     //AES block 4k+6 - store result
+	rev	w9, w12                                 //CTR block 4k+11
+
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+11
+	eor	v7.16b, v7.16b, v3.16b                          //AES block 4k+3 - result
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	st1	{ v7.16b}, [x2], #16                     //AES block 4k+3 - store result
+	b.lt	.L128_enc_main_loop
+
+.L128_enc_prepretail:	//PREPRETAIL
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k (only t0 is free)
+	fmov	d3, x10                               //CTR block 4k+3
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+1 (t0 and t1 free)
+
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	add	w12, w12, #1                            //CTR block 4k+3
+	fmov	v3.d[1], x9                               //CTR block 4k+3
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2 (t0, t1, and t2 free)
+
+	pmull	v29.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3 (t0, t1, t2 and t3 free)
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+
+	pmull2	v28.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	mov	d30, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+
+	mov	d31, v6.d[1]                                  //GHASH block 4k+2 - mid
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	eor	v30.8b, v30.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	eor	v31.8b, v31.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+
+	pmull	v30.1q, v30.1d, v17.1d                          //GHASH block 4k+1 - mid
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+1 - low
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	ins	v31.d[1], v31.d[0]                                //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+1 - mid
+	mov	d30, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	eor	v9.16b, v9.16b, v28.16b                         //GHASH block 4k+1 - high
+
+	pmull2	v31.1q, v31.2d, v16.2d                          //GHASH block 4k+2 - mid
+
+	pmull2	v8.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+	eor	v30.8b, v30.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	pmull2	v4.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	pmull	v28.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	eor	v9.16b, v9.16b, v8.16b                         //GHASH block 4k+2 - high
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+
+	pmull	v29.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+	movi	v8.8b, #0xc2
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+	eor	v11.16b, v11.16b, v28.16b                         //GHASH block 4k+2 - low
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+
+	pmull	v30.1q, v30.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	v10.16b, v10.16b, v31.16b                         //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+3 - high
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+3 - mid
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+3 - low
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+
+	pmull	v28.1q, v9.1d, v8.1d
+	eor	v10.16b, v10.16b, v9.16b                         //karatsuba tidy up
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	ext	v9.16b, v9.16b, v9.16b, #8
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	eor	v10.16b, v10.16b, v11.16b
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+	eor	v10.16b, v10.16b, v28.16b
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	eor	v10.16b, v10.16b, v9.16b
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+
+	pmull	v28.1q, v10.1d, v8.1d
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+	ext	v10.16b, v10.16b, v10.16b, #8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	eor	v11.16b, v11.16b, v28.16b
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+
+	aese	v3.16b, v27.16b                                      //AES block 4k+7 - round 9
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+
+	aese	v0.16b, v27.16b                                      //AES block 4k+4 - round 9
+
+	aese	v1.16b, v27.16b                                      //AES block 4k+5 - round 9
+	eor	v11.16b, v11.16b, v10.16b
+
+	aese	v2.16b, v27.16b                                      //AES block 4k+6 - round 9
+.L128_enc_tail:	//TAIL
+
+	sub	x5, x4, x0   //main_end_input_ptr is number of bytes left to process
+	ldp	x6, x7, [x0], #16           //AES block 4k+4 - load plaintext
+
+	cmp	x5, #48
+
+	ext	v8.16b, v11.16b, v11.16b, #8                     //prepare final partial tag
+	eor	x6, x6, x13                     //AES block 4k+4 - round 10 low
+	eor	x7, x7, x14                     //AES block 4k+4 - round 10 high
+
+	fmov	d4, x6                               //AES block 4k+4 - mov low
+
+	fmov	v4.d[1], x7                           //AES block 4k+4 - mov high
+
+	eor	v5.16b, v4.16b, v0.16b                          //AES block 4k+4 - result
+
+	b.gt	.L128_enc_blocks_more_than_3
+
+	sub	w12, w12, #1
+	movi	v11.8b, #0
+	mov	v3.16b, v2.16b
+
+	cmp	x5, #32
+	mov	v2.16b, v1.16b
+	movi	v9.8b, #0
+
+	movi	v10.8b, #0
+	b.gt	.L128_enc_blocks_more_than_2
+
+	mov	v3.16b, v1.16b
+	cmp	x5, #16
+
+	sub	w12, w12, #1
+	b.gt	.L128_enc_blocks_more_than_1
+
+	sub	w12, w12, #1
+	b	.L128_enc_blocks_less_than_1
+.L128_enc_blocks_more_than_3:	//blocks	left >  3
+	st1	{ v5.16b}, [x2], #16                     //AES final-3 block  - store result
+
+	ldp	x6, x7, [x0], #16           //AES final-2 block - load input low & high
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-3 block
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+	eor	x7, x7, x14                     //AES final-2 block - round 10 high
+	eor	x6, x6, x13                     //AES final-2 block - round 10 low
+
+	fmov	d5, x6                                 //AES final-2 block - mov low
+
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+	fmov	v5.d[1], x7                             //AES final-2 block - mov high
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH final-3 block - low
+	mov	d22, v4.d[1]                                 //GHASH final-3 block - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH final-3 block - high
+
+	mov	d10, v17.d[1]                               //GHASH final-3 block - mid
+
+	eor	v5.16b, v5.16b, v1.16b                            //AES final-2 block - result
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-3 block - mid
+
+	pmull	v10.1q, v22.1d, v10.1d                    //GHASH final-3 block - mid
+.L128_enc_blocks_more_than_2:	//blocks	left >  2
+
+	st1	{ v5.16b}, [x2], #16                     //AES final-2 block - store result
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-2 block
+	ldp	x6, x7, [x0], #16           //AES final-1 block - load input low & high
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	eor	x6, x6, x13                     //AES final-1 block - round 10 low
+
+	fmov	d5, x6                                 //AES final-1 block - mov low
+	eor	x7, x7, x14                     //AES final-1 block - round 10 high
+
+	pmull2	v20.1q, v4.2d, v14.2d                          //GHASH final-2 block - high
+	fmov	v5.d[1], x7                             //AES final-1 block - mov high
+
+	mov	d22, v4.d[1]                                 //GHASH final-2 block - mid
+
+	pmull	v21.1q, v4.1d, v14.1d                          //GHASH final-2 block - low
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final-2 block - high
+
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-2 block - mid
+
+	eor	v5.16b, v5.16b, v2.16b                            //AES final-1 block - result
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final-2 block - low
+
+	pmull	v22.1q, v22.1d, v17.1d                      //GHASH final-2 block - mid
+
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+
+	eor	v10.16b, v10.16b, v22.16b                       //GHASH final-2 block - mid
+.L128_enc_blocks_more_than_1:	//blocks	left >  1
+
+	st1	{ v5.16b}, [x2], #16                     //AES final-1 block - store result
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-1 block
+	ldp	x6, x7, [x0], #16           //AES final block - load input low & high
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	eor	x7, x7, x14                     //AES final block - round 10 high
+	eor	x6, x6, x13                     //AES final block - round 10 low
+
+	fmov	d5, x6                                 //AES final block - mov low
+
+	pmull2	v20.1q, v4.2d, v13.2d                          //GHASH final-1 block - high
+	fmov	v5.d[1], x7                             //AES final block - mov high
+
+	mov	d22, v4.d[1]                                 //GHASH final-1 block - mid
+
+	pmull	v21.1q, v4.1d, v13.1d                          //GHASH final-1 block - low
+
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-1 block - mid
+
+	eor	v5.16b, v5.16b, v3.16b                            //AES final block - result
+
+	ins	v22.d[1], v22.d[0]                            //GHASH final-1 block - mid
+
+	pmull2	v22.1q, v22.2d, v16.2d                      //GHASH final-1 block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final-1 block - low
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final-1 block - high
+
+	eor	v10.16b, v10.16b, v22.16b                       //GHASH final-1 block - mid
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+.L128_enc_blocks_less_than_1:	//blocks	left <= 1
+
+	and	x1, x1, #127                    //bit_length %= 128
+	mvn	x13, xzr                                      //rk10_l = 0xffffffffffffffff
+
+	mvn	x14, xzr                                      //rk10_h = 0xffffffffffffffff
+	sub	x1, x1, #128                    //bit_length -= 128
+
+	neg	x1, x1                          //bit_length = 128 - #bits in input (in range [1,128])
+
+	and	x1, x1, #127                    //bit_length %= 128
+
+	lsr	x14, x14, x1                     //rk10_h is mask for top 64b of last block
+	cmp	x1, #64
+
+	csel	x6, x13, x14, lt
+	csel	x7, x14, xzr, lt
+
+	fmov	d0, x6                                 //ctr0b is mask for last block
+
+	fmov	v0.d[1], x7
+
+	and	v5.16b, v5.16b, v0.16b                            //possibly partial last block has zeroes in highest bits
+
+	rev64	v4.16b, v5.16b                                    //GHASH final block
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	mov	d8, v4.d[1]                                  //GHASH final block - mid
+
+	pmull	v21.1q, v4.1d, v12.1d                          //GHASH final block - low
+	ld1	{ v18.16b}, [x2]                            //load existing bytes where the possibly partial last block is to be stored
+
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH final block - mid
+
+	rev	w9, w12
+
+	pmull2	v20.1q, v4.2d, v12.2d                          //GHASH final block - high
+
+	pmull	v8.1q, v8.1d, v16.1d                          //GHASH final block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final block - low
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final block - high
+
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH final block - mid
+	movi	v8.8b, #0xc2
+
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	shl	d8, d8, #56               //mod_constant
+
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	pmull	v9.1q, v10.1d, v8.1d            //MODULO - mid 64b align with low
+
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	bif	v5.16b, v18.16b, v0.16b                              //insert existing bytes in top end of result before storing
+
+	eor	v11.16b, v11.16b, v9.16b                         //MODULO - fold into low
+	st1	{ v5.16b}, [x2]                          //store all 16B
+
+	str	w9, [x16, #12]                          //store the updated counter
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	mov	x0, x15
+	st1	{ v11.16b }, [x3]
+	ldp	x21, x22, [sp, #16]
+	ldp	x23, x24, [sp, #32]
+	ldp	d8, d9, [sp, #48]
+	ldp	d10, d11, [sp, #64]
+	ldp	d12, d13, [sp, #80]
+	ldp	d14, d15, [sp, #96]
+	ldp	x19, x20, [sp], #112
+	ret
+
+.L128_enc_ret:
+	mov	w0, #0x0
+	ret
+.size	aes_gcm_enc_128_kernel,.-aes_gcm_enc_128_kernel
+.globl	aes_gcm_dec_128_kernel
+.type	aes_gcm_dec_128_kernel,%function
+.align	4
+aes_gcm_dec_128_kernel:
+	cbz	x1, .L128_dec_ret
+	stp	x19, x20, [sp, #-112]!
+	mov	x16, x4
+	mov	x8, x5
+	stp	x21, x22, [sp, #16]
+	stp	x23, x24, [sp, #32]
+	stp	d8, d9, [sp, #48]
+	stp	d10, d11, [sp, #64]
+	stp	d12, d13, [sp, #80]
+	stp	d14, d15, [sp, #96]
+
+	lsr	x5, x1, #3              //byte_len
+	mov	x15, x5
+	ldp	x10, x11, [x16]              //ctr96_b64, ctr96_t32
+
+	sub	x5, x5, #1      //byte_len - 1
+	ldr	q18, [x8, #0]                                  //load rk0
+
+	and	x5, x5, #0xffffffffffffffc0 //number of bytes to be processed in main loop (at least 1 byte must be handled by tail)
+	ld1	{ v0.16b}, [x16]                             //special case vector load initial counter so we can start first AES block as quickly as possible
+
+	ldr	q13, [x3, #64]                         //load h2l | h2h
+	ext	v13.16b, v13.16b, v13.16b, #8
+
+	lsr	x12, x11, #32
+	fmov	d2, x10                               //CTR block 2
+
+	ldr	q19, [x8, #16]                                 //load rk1
+	orr	w11, w11, w11
+	rev	w12, w12                                //rev_ctr32
+
+	fmov	d1, x10                               //CTR block 1
+	add	w12, w12, #1                            //increment rev_ctr32
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 0
+	rev	w9, w12                                 //CTR block 1
+
+	orr	x9, x11, x9, lsl #32            //CTR block 1
+	ldr	q20, [x8, #32]                                 //load rk2
+	add	w12, w12, #1                            //CTR block 1
+
+	fmov	v1.d[1], x9                               //CTR block 1
+	rev	w9, w12                                 //CTR block 2
+	add	w12, w12, #1                            //CTR block 2
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 1
+	orr	x9, x11, x9, lsl #32            //CTR block 2
+
+	fmov	v2.d[1], x9                               //CTR block 2
+	rev	w9, w12                                 //CTR block 3
+
+	fmov	d3, x10                               //CTR block 3
+	orr	x9, x11, x9, lsl #32            //CTR block 3
+	add	w12, w12, #1                            //CTR block 3
+
+	fmov	v3.d[1], x9                               //CTR block 3
+	add	x4, x0, x1, lsr #3   //end_input_ptr
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 0
+	ldr	q21, [x8, #48]                                 //load rk3
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 2
+	ldr	q24, [x8, #96]                                 //load rk6
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 0
+	ldr	q25, [x8, #112]                                //load rk7
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 1
+	ldr	q22, [x8, #64]                                 //load rk4
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 0
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 1
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 2
+	ldp	x13, x14, [x8, #160]                     //load rk10
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 1
+	ld1	{ v11.16b}, [x3]
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 3
+	ldr	q23, [x8, #80]                                 //load rk5
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 3
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 2
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 2
+	ldr	q27, [x8, #144]                                //load rk9
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 4
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 3
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 3
+	ldr	q14, [x3, #80]                         //load h3l | h3h
+	ext	v14.16b, v14.16b, v14.16b, #8
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 4
+	ldr	q26, [x8, #128]                                //load rk8
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 5
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 4
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 4
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 5
+	ldr	q12, [x3, #32]                         //load h1l | h1h
+	ext	v12.16b, v12.16b, v12.16b, #8
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 5
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 6
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 6
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 6
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 6
+	trn1	v8.2d,    v12.2d,    v13.2d                      //h2h | h1h
+
+	ldr	q15, [x3, #112]                        //load h4l | h4h
+	ext	v15.16b, v15.16b, v15.16b, #8
+	trn2	v16.2d,  v12.2d,    v13.2d                      //h2l | h1l
+	add	x5, x5, x0
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 7
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 7
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 7
+	eor	v16.16b, v16.16b, v8.16b                     //h2k | h1k
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 7
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 8
+	trn2	v17.2d,  v14.2d,    v15.2d                      //h4l | h3l
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 8
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 8
+	trn1	v9.2d, v14.2d,    v15.2d                      //h4h | h3h
+
+	aese	v2.16b, v27.16b                                      //AES block 2 - round 9
+
+	aese	v3.16b, v27.16b                                      //AES block 3 - round 9
+
+	aese	v0.16b, v27.16b                                      //AES block 0 - round 9
+	cmp	x0, x5                   //check if we have <= 4 blocks
+
+	aese	v1.16b, v27.16b                                      //AES block 1 - round 9
+	eor	v17.16b, v17.16b, v9.16b                  //h4k | h3k
+	b.ge	.L128_dec_tail                                    //handle tail
+
+	ldr	q5, [x0, #16]                         //AES block 1 - load ciphertext
+
+	ldr	q4, [x0, #0]                          //AES block 0 - load ciphertext
+
+	eor	v1.16b, v5.16b, v1.16b                            //AES block 1 - result
+	ldr	q6, [x0, #32]                         //AES block 2 - load ciphertext
+
+	eor	v0.16b, v4.16b, v0.16b                            //AES block 0 - result
+	rev64	v4.16b, v4.16b                                    //GHASH block 0
+	rev	w9, w12                                 //CTR block 4
+
+	orr	x9, x11, x9, lsl #32            //CTR block 4
+	add	w12, w12, #1                            //CTR block 4
+	ldr	q7, [x0, #48]                         //AES block 3 - load ciphertext
+
+	rev64	v5.16b, v5.16b                                    //GHASH block 1
+	add	x0, x0, #64                       //AES input_ptr update
+	mov	x19, v1.d[0]                            //AES block 1 - mov low
+
+	mov	x20, v1.d[1]                            //AES block 1 - mov high
+
+	mov	x6, v0.d[0]                            //AES block 0 - mov low
+	cmp	x0, x5                   //check if we have <= 8 blocks
+
+	mov	x7, v0.d[1]                            //AES block 0 - mov high
+
+	fmov	d0, x10                               //CTR block 4
+
+	fmov	v0.d[1], x9                               //CTR block 4
+	rev	w9, w12                                 //CTR block 5
+	eor	x19, x19, x13                   //AES block 1 - round 10 low
+
+	fmov	d1, x10                               //CTR block 5
+	add	w12, w12, #1                            //CTR block 5
+	orr	x9, x11, x9, lsl #32            //CTR block 5
+
+	fmov	v1.d[1], x9                               //CTR block 5
+	rev	w9, w12                                 //CTR block 6
+	add	w12, w12, #1                            //CTR block 6
+
+	orr	x9, x11, x9, lsl #32            //CTR block 6
+
+	eor	x20, x20, x14                   //AES block 1 - round 10 high
+	eor	x6, x6, x13                   //AES block 0 - round 10 low
+	eor	v2.16b, v6.16b, v2.16b                            //AES block 2 - result
+
+	eor	x7, x7, x14                   //AES block 0 - round 10 high
+	stp	x6, x7, [x2], #16        //AES block 0 - store result
+
+	stp	x19, x20, [x2], #16        //AES block 1 - store result
+	b.ge	.L128_dec_prepretail                              //do prepretail
+
+.L128_dec_main_loop:	//main	loop start
+	eor	v3.16b, v7.16b, v3.16b                            //AES block 4k+3 - result
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	mov	x21, v2.d[0]                            //AES block 4k+2 - mov low
+
+	pmull2	v28.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+	mov	x22, v2.d[1]                            //AES block 4k+2 - mov high
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	fmov	d2, x10                               //CTR block 4k+6
+
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2
+	fmov	v2.d[1], x9                               //CTR block 4k+6
+	rev	w9, w12                                 //CTR block 4k+7
+
+	mov	x23, v3.d[0]                            //AES block 4k+3 - mov low
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+	mov	d30, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3
+
+	pmull	v29.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+	mov	x24, v3.d[1]                            //AES block 4k+3 - mov high
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+7
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	fmov	d3, x10                               //CTR block 4k+7
+	eor	v30.8b, v30.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	fmov	v3.d[1], x9                               //CTR block 4k+7
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+1 - low
+
+	pmull	v29.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	eor	v9.16b, v9.16b, v28.16b                         //GHASH block 4k+1 - high
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+
+	pmull	v28.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	eor	x23, x23, x13                   //AES block 4k+3 - round 10 low
+
+	pmull	v30.1q, v30.1d, v17.1d                          //GHASH block 4k+1 - mid
+	eor	x22, x22, x14                   //AES block 4k+2 - round 10 high
+	mov	d31, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	eor	v11.16b, v11.16b, v28.16b                         //GHASH block 4k+2 - low
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	eor	v31.8b, v31.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+1 - mid
+
+	pmull2	v8.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	ins	v31.d[1], v31.d[0]                                //GHASH block 4k+2 - mid
+
+	pmull2	v4.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	mov	d30, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+	eor	v9.16b, v9.16b, v8.16b                         //GHASH block 4k+2 - high
+
+	pmull2	v31.1q, v31.2d, v16.2d                          //GHASH block 4k+2 - mid
+	eor	x24, x24, x14                   //AES block 4k+3 - round 10 high
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+	eor	v30.8b, v30.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+	eor	x21, x21, x13                   //AES block 4k+2 - round 10 low
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	movi	v8.8b, #0xc2
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+3 - low
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	eor	v10.16b, v10.16b, v31.16b                         //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	stp	x21, x22, [x2], #16        //AES block 4k+2 - store result
+
+	pmull	v30.1q, v30.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+3 - high
+	ldr	q4, [x0, #0]                          //AES block 4k+4 - load ciphertext
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+	add	w12, w12, #1                            //CTR block 4k+7
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+	stp	x23, x24, [x2], #16        //AES block 4k+3 - store result
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	rev	w9, w12                                 //CTR block 4k+8
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	ldr	q5, [x0, #16]                         //AES block 4k+5 - load ciphertext
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v0.16b, v27.16b                                      //AES block 4k+4 - round 9
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+8
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	aese	v1.16b, v27.16b                                      //AES block 4k+5 - round 9
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+	eor	v0.16b, v4.16b, v0.16b                            //AES block 4k+4 - result
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+	ldr	q6, [x0, #32]                         //AES block 4k+6 - load ciphertext
+
+	add	w12, w12, #1                            //CTR block 4k+8
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+	eor	v1.16b, v5.16b, v1.16b                            //AES block 4k+5 - result
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	ldr	q7, [x0, #48]                         //AES block 4k+3 - load ciphertext
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	add	x0, x0, #64                       //AES input_ptr update
+
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+5
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+	mov	x7, v0.d[1]                            //AES block 4k+4 - mov high
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+	mov	x6, v0.d[0]                            //AES block 4k+4 - mov low
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	fmov	d0, x10                               //CTR block 4k+8
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+	fmov	v0.d[1], x9                               //CTR block 4k+8
+	rev	w9, w12                                 //CTR block 4k+9
+
+	aese	v2.16b, v27.16b                                      //AES block 4k+6 - round 9
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+9
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+	eor	x7, x7, x14                   //AES block 4k+4 - round 10 high
+
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+	mov	x20, v1.d[1]                            //AES block 4k+5 - mov high
+	eor	x6, x6, x13                   //AES block 4k+4 - round 10 low
+
+	eor	v2.16b, v6.16b, v2.16b                            //AES block 4k+6 - result
+	mov	x19, v1.d[0]                            //AES block 4k+5 - mov low
+	add	w12, w12, #1                            //CTR block 4k+9
+
+	aese	v3.16b, v27.16b                                      //AES block 4k+7 - round 9
+	fmov	d1, x10                               //CTR block 4k+9
+	cmp	x0, x5                   //.LOOP CONTROL
+
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k+4
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	fmov	v1.d[1], x9                               //CTR block 4k+9
+
+	rev	w9, w12                                 //CTR block 4k+10
+	add	w12, w12, #1                            //CTR block 4k+10
+
+	eor	x20, x20, x14                   //AES block 4k+5 - round 10 high
+	stp	x6, x7, [x2], #16        //AES block 4k+4 - store result
+
+	eor	x19, x19, x13                   //AES block 4k+5 - round 10 low
+	stp	x19, x20, [x2], #16        //AES block 4k+5 - store result
+
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+10
+	b.lt	.L128_dec_main_loop
+
+.L128_dec_prepretail:	//PREPRETAIL
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	mov	x21, v2.d[0]                            //AES block 4k+2 - mov low
+	mov	d30, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	eor	v3.16b, v7.16b, v3.16b                            //AES block 4k+3 - result
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	mov	x22, v2.d[1]                            //AES block 4k+2 - mov high
+
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+	fmov	d2, x10                               //CTR block 4k+6
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	fmov	v2.d[1], x9                               //CTR block 4k+6
+
+	rev	w9, w12                                 //CTR block 4k+7
+	mov	x23, v3.d[0]                            //AES block 4k+3 - mov low
+	eor	v30.8b, v30.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+	mov	x24, v3.d[1]                            //AES block 4k+3 - mov high
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	mov	d31, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+7
+
+	pmull	v29.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+	fmov	d3, x10                               //CTR block 4k+7
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	fmov	v3.d[1], x9                               //CTR block 4k+7
+
+	pmull	v30.1q, v30.1d, v17.1d                          //GHASH block 4k+1 - mid
+	eor	v31.8b, v31.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	ins	v31.d[1], v31.d[0]                                //GHASH block 4k+2 - mid
+
+	pmull2	v28.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+1 - low
+
+	pmull	v29.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+
+	pmull2	v31.1q, v31.2d, v16.2d                          //GHASH block 4k+2 - mid
+	eor	v9.16b, v9.16b, v28.16b                         //GHASH block 4k+1 - high
+
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+1 - mid
+
+	pmull2	v4.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	pmull2	v8.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+	mov	d30, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	eor	v10.16b, v10.16b, v31.16b                         //GHASH block 4k+2 - mid
+
+	pmull	v28.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	eor	v9.16b, v9.16b, v8.16b                         //GHASH block 4k+2 - high
+	movi	v8.8b, #0xc2
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	eor	v30.8b, v30.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	eor	v11.16b, v11.16b, v28.16b                         //GHASH block 4k+2 - low
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+3 - high
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	eor	x23, x23, x13                   //AES block 4k+3 - round 10 low
+
+	pmull	v30.1q, v30.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	x21, x21, x13                   //AES block 4k+2 - round 10 low
+	eor	v11.16b, v11.16b, v29.16b                         //GHASH block 4k+3 - low
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+
+	aese	v1.16b, v27.16b                                      //AES block 4k+5 - round 9
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+	eor	x24, x24, x14                   //AES block 4k+3 - round 10 high
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+	eor	x22, x22, x14                   //AES block 4k+2 - round 10 high
+
+	aese	v0.16b, v27.16b                                      //AES block 4k+4 - round 9
+	stp	x21, x22, [x2], #16        //AES block 4k+2 - store result
+
+	aese	v2.16b, v27.16b                                      //AES block 4k+6 - round 9
+	add	w12, w12, #1                            //CTR block 4k+7
+	stp	x23, x24, [x2], #16        //AES block 4k+3 - store result
+
+	aese	v3.16b, v27.16b                                      //AES block 4k+7 - round 9
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+.L128_dec_tail:	//TAIL
+
+	sub	x5, x4, x0   //main_end_input_ptr is number of bytes left to process
+	ld1	{ v5.16b}, [x0], #16                      //AES block 4k+4 - load ciphertext
+
+	eor	v0.16b, v5.16b, v0.16b                            //AES block 4k+4 - result
+
+	mov	x7, v0.d[1]                            //AES block 4k+4 - mov high
+
+	mov	x6, v0.d[0]                            //AES block 4k+4 - mov low
+
+	cmp	x5, #48
+
+	eor	x7, x7, x14                   //AES block 4k+4 - round 10 high
+
+	ext	v8.16b, v11.16b, v11.16b, #8                     //prepare final partial tag
+	eor	x6, x6, x13                   //AES block 4k+4 - round 10 low
+	b.gt	.L128_dec_blocks_more_than_3
+
+	mov	v3.16b, v2.16b
+	sub	w12, w12, #1
+	movi	v11.8b, #0
+
+	movi	v9.8b, #0
+	mov	v2.16b, v1.16b
+
+	movi	v10.8b, #0
+	cmp	x5, #32
+	b.gt	.L128_dec_blocks_more_than_2
+
+	cmp	x5, #16
+
+	mov	v3.16b, v1.16b
+	sub	w12, w12, #1
+	b.gt	.L128_dec_blocks_more_than_1
+
+	sub	w12, w12, #1
+	b	.L128_dec_blocks_less_than_1
+.L128_dec_blocks_more_than_3:	//blocks	left >  3
+	rev64	v4.16b, v5.16b                                    //GHASH final-3 block
+	ld1	{ v5.16b}, [x0], #16                      //AES final-2 block - load ciphertext
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	mov	d10, v17.d[1]                               //GHASH final-3 block - mid
+	stp	x6, x7, [x2], #16        //AES final-3 block  - store result
+	eor	v0.16b, v5.16b, v1.16b                            //AES final-2 block - result
+
+	mov	d22, v4.d[1]                                 //GHASH final-3 block - mid
+	mov	x7, v0.d[1]                            //AES final-2 block - mov high
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH final-3 block - low
+	mov	x6, v0.d[0]                            //AES final-2 block - mov low
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH final-3 block - high
+
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-3 block - mid
+
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+	eor	x7, x7, x14                   //AES final-2 block - round 10 high
+
+	pmull	v10.1q, v22.1d, v10.1d                    //GHASH final-3 block - mid
+	eor	x6, x6, x13                   //AES final-2 block - round 10 low
+.L128_dec_blocks_more_than_2:	//blocks	left >  2
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-2 block
+	ld1	{ v5.16b}, [x0], #16                      //AES final-1 block - load ciphertext
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	eor	v0.16b, v5.16b, v2.16b                            //AES final-1 block - result
+	stp	x6, x7, [x2], #16        //AES final-2 block  - store result
+
+	mov	d22, v4.d[1]                                 //GHASH final-2 block - mid
+
+	pmull	v21.1q, v4.1d, v14.1d                          //GHASH final-2 block - low
+
+	pmull2	v20.1q, v4.2d, v14.2d                          //GHASH final-2 block - high
+	mov	x6, v0.d[0]                            //AES final-1 block - mov low
+
+	mov	x7, v0.d[1]                            //AES final-1 block - mov high
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-2 block - mid
+
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+
+	pmull	v22.1q, v22.1d, v17.1d                      //GHASH final-2 block - mid
+
+	eor	x6, x6, x13                   //AES final-1 block - round 10 low
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final-2 block - low
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final-2 block - high
+
+	eor	v10.16b, v10.16b, v22.16b                       //GHASH final-2 block - mid
+	eor	x7, x7, x14                   //AES final-1 block - round 10 high
+.L128_dec_blocks_more_than_1:	//blocks	left >  1
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-1 block
+
+	ld1	{ v5.16b}, [x0], #16                      //AES final block - load ciphertext
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	mov	d22, v4.d[1]                                 //GHASH final-1 block - mid
+
+	eor	v0.16b, v5.16b, v3.16b                            //AES final block - result
+
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-1 block - mid
+
+	stp	x6, x7, [x2], #16        //AES final-1 block  - store result
+	mov	x6, v0.d[0]                            //AES final block - mov low
+
+	mov	x7, v0.d[1]                            //AES final block - mov high
+	ins	v22.d[1], v22.d[0]                            //GHASH final-1 block - mid
+
+	pmull	v21.1q, v4.1d, v13.1d                          //GHASH final-1 block - low
+
+	pmull2	v20.1q, v4.2d, v13.2d                          //GHASH final-1 block - high
+
+	pmull2	v22.1q, v22.2d, v16.2d                      //GHASH final-1 block - mid
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final-1 block - low
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final-1 block - high
+	eor	x7, x7, x14                   //AES final block - round 10 high
+
+	eor	x6, x6, x13                   //AES final block - round 10 low
+	eor	v10.16b, v10.16b, v22.16b                       //GHASH final-1 block - mid
+.L128_dec_blocks_less_than_1:	//blocks	left <= 1
+
+	mvn	x14, xzr                                      //rk10_h = 0xffffffffffffffff
+	and	x1, x1, #127                    //bit_length %= 128
+
+	mvn	x13, xzr                                      //rk10_l = 0xffffffffffffffff
+	sub	x1, x1, #128                    //bit_length -= 128
+
+	neg	x1, x1                          //bit_length = 128 - #bits in input (in range [1,128])
+
+	and	x1, x1, #127                    //bit_length %= 128
+
+	lsr	x14, x14, x1                     //rk10_h is mask for top 64b of last block
+	cmp	x1, #64
+
+	csel	x10, x14, xzr, lt
+	csel	x9, x13, x14, lt
+
+	fmov	d0, x9                                   //ctr0b is mask for last block
+
+	mov	v0.d[1], x10
+
+	and	v5.16b, v5.16b, v0.16b                            //possibly partial last block has zeroes in highest bits
+
+	rev64	v4.16b, v5.16b                                    //GHASH final block
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	ldp	x4, x5, [x2] //load existing bytes we need to not overwrite
+
+	and	x7, x7, x10
+
+	pmull2	v20.1q, v4.2d, v12.2d                          //GHASH final block - high
+	mov	d8, v4.d[1]                                  //GHASH final block - mid
+
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH final block - mid
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final block - high
+
+	pmull	v8.1q, v8.1d, v16.1d                          //GHASH final block - mid
+
+	pmull	v21.1q, v4.1d, v12.1d                          //GHASH final block - low
+	bic	x4, x4, x9           //mask out low existing bytes
+	and	x6, x6, x9
+
+	rev	w9, w12
+
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH final block - mid
+	movi	v8.8b, #0xc2
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final block - low
+
+	bic	x5, x5, x10   //mask out high existing bytes
+	shl	d8, d8, #56               //mod_constant
+
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	orr	x6, x6, x4
+	str	w9, [x16, #12]                          //store the updated counter
+
+	orr	x7, x7, x5
+	stp	x6, x7, [x2]
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	mov	x0, x15
+	st1	{ v11.16b }, [x3]
+
+	ldp	x21, x22, [sp, #16]
+	ldp	x23, x24, [sp, #32]
+	ldp	d8, d9, [sp, #48]
+	ldp	d10, d11, [sp, #64]
+	ldp	d12, d13, [sp, #80]
+	ldp	d14, d15, [sp, #96]
+	ldp	x19, x20, [sp], #112
+	ret
+
+.L128_dec_ret:
+	mov	w0, #0x0
+	ret
+.size	aes_gcm_dec_128_kernel,.-aes_gcm_dec_128_kernel
+.globl	aes_gcm_enc_192_kernel
+.type	aes_gcm_enc_192_kernel,%function
+.align	4
+aes_gcm_enc_192_kernel:
+	cbz	x1, .L192_enc_ret
+	stp	x19, x20, [sp, #-112]!
+	mov	x16, x4
+	mov	x8, x5
+	stp	x21, x22, [sp, #16]
+	stp	x23, x24, [sp, #32]
+	stp	d8, d9, [sp, #48]
+	stp	d10, d11, [sp, #64]
+	stp	d12, d13, [sp, #80]
+	stp	d14, d15, [sp, #96]
+
+	ldp	x10, x11, [x16]             //ctr96_b64, ctr96_t32
+
+	ldr	q23, [x8, #80]                                //load rk5
+
+	ldr	q22, [x8, #64]                                //load rk4
+
+	ldr	q26, [x8, #128]                               //load rk8
+
+	lsr	x12, x11, #32
+	ldr	q24, [x8, #96]                                //load rk6
+	orr	w11, w11, w11
+
+	ldr	q25, [x8, #112]                               //load rk7
+	rev	w12, w12                               //rev_ctr32
+
+	add	w12, w12, #1                           //increment rev_ctr32
+	fmov	d3, x10                              //CTR block 3
+
+	rev	w9, w12                                //CTR block 1
+	add	w12, w12, #1                           //CTR block 1
+	fmov	d1, x10                              //CTR block 1
+
+	orr	x9, x11, x9, lsl #32           //CTR block 1
+	ld1	{ v0.16b}, [x16]                            //special case vector load initial counter so we can start first AES block as quickly as possible
+
+	fmov	v1.d[1], x9                              //CTR block 1
+	rev	w9, w12                                //CTR block 2
+	add	w12, w12, #1                           //CTR block 2
+
+	fmov	d2, x10                              //CTR block 2
+	orr	x9, x11, x9, lsl #32           //CTR block 2
+
+	fmov	v2.d[1], x9                              //CTR block 2
+	rev	w9, w12                                //CTR block 3
+
+	orr	x9, x11, x9, lsl #32           //CTR block 3
+	ldr	q18, [x8, #0]                                 //load rk0
+
+	fmov	v3.d[1], x9                              //CTR block 3
+
+	ldr	q21, [x8, #48]                                //load rk3
+
+	ldp	x13, x14, [x8, #192]                    //load rk12
+
+	ldr	q19, [x8, #16]                                //load rk1
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 0
+	ld1	{ v11.16b}, [x3]
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 0
+	ldr	q29, [x8, #176]                              //load rk11
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 0
+	ldr	q15, [x3, #112]                       //load h4l | h4h
+	ext	v15.16b, v15.16b, v15.16b, #8
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 0
+	ldr	q20, [x8, #32]                                //load rk2
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 1
+	ldr	q28, [x8, #160]                              //load rk10
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 1
+	ldr	q12, [x3, #32]                        //load h1l | h1h
+	ext	v12.16b, v12.16b, v12.16b, #8
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 1
+	ldr	q27, [x8, #144]                               //load rk9
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 1
+	ldr	q14, [x3, #80]                        //load h3l | h3h
+	ext	v14.16b, v14.16b, v14.16b, #8
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 2
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 2
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 2
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 3
+	trn1	v9.2d, v14.2d,    v15.2d                     //h4h | h3h
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 3
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 2
+	trn2	v17.2d,  v14.2d,    v15.2d                     //h4l | h3l
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 4
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 3
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 3
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 5
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 4
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 4
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 6
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 4
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 5
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 5
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 5
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 6
+	ldr	q13, [x3, #64]                        //load h2l | h2h
+	ext	v13.16b, v13.16b, v13.16b, #8
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 6
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 6
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 7
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 7
+	trn2	v16.2d,  v12.2d,    v13.2d                     //h2l | h1l
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 7
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 8
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 7
+	trn1	v8.2d,    v12.2d,    v13.2d                     //h2h | h1h
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 8
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 8
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 9
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 9
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 9
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 9
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b         //AES block 0 - round 10
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b         //AES block 2 - round 10
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b         //AES block 1 - round 10
+	lsr	x5, x1, #3             //byte_len
+	mov	x15, x5
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b         //AES block 3 - round 10
+	sub	x5, x5, #1     //byte_len - 1
+
+	eor	v16.16b, v16.16b, v8.16b                    //h2k | h1k
+	and	x5, x5, #0xffffffffffffffc0   //number of bytes to be processed in main loop (at least 1 byte must be handled by tail)
+
+	eor	v17.16b, v17.16b, v9.16b                 //h4k | h3k
+
+	aese	v2.16b, v29.16b                                    //AES block 2 - round 11
+	add	x4, x0, x1, lsr #3  //end_input_ptr
+	add	x5, x5, x0
+
+	aese	v1.16b, v29.16b                                    //AES block 1 - round 11
+	cmp	x0, x5                  //check if we have <= 4 blocks
+
+	aese	v0.16b, v29.16b                                    //AES block 0 - round 11
+	add	w12, w12, #1                           //CTR block 3
+
+	aese	v3.16b, v29.16b                                    //AES block 3 - round 11
+	b.ge	.L192_enc_tail                                   //handle tail
+
+	rev	w9, w12                                //CTR block 4
+	ldp	x6, x7, [x0, #0]           //AES block 0 - load plaintext
+
+	orr	x9, x11, x9, lsl #32           //CTR block 4
+	ldp	x21, x22, [x0, #32]          //AES block 2 - load plaintext
+
+	ldp	x23, x24, [x0, #48]          //AES block 3 - load plaintext
+
+	ldp	x19, x20, [x0, #16]          //AES block 1 - load plaintext
+	add	x0, x0, #64                      //AES input_ptr update
+	cmp	x0, x5                  //check if we have <= 8 blocks
+
+	eor	x6, x6, x13                    //AES block 0 - round 12 low
+
+	eor	x7, x7, x14                    //AES block 0 - round 12 high
+	eor	x22, x22, x14                    //AES block 2 - round 12 high
+	fmov	d4, x6                              //AES block 0 - mov low
+
+	eor	x24, x24, x14                    //AES block 3 - round 12 high
+	fmov	v4.d[1], x7                          //AES block 0 - mov high
+
+	eor	x21, x21, x13                    //AES block 2 - round 12 low
+	eor	x19, x19, x13                    //AES block 1 - round 12 low
+
+	fmov	d5, x19                              //AES block 1 - mov low
+	eor	x20, x20, x14                    //AES block 1 - round 12 high
+
+	fmov	v5.d[1], x20                          //AES block 1 - mov high
+
+	eor	x23, x23, x13                    //AES block 3 - round 12 low
+	fmov	d6, x21                              //AES block 2 - mov low
+
+	add	w12, w12, #1                           //CTR block 4
+	eor	v4.16b, v4.16b, v0.16b                         //AES block 0 - result
+	fmov	d0, x10                              //CTR block 4
+
+	fmov	v0.d[1], x9                              //CTR block 4
+	rev	w9, w12                                //CTR block 5
+
+	orr	x9, x11, x9, lsl #32           //CTR block 5
+	add	w12, w12, #1                           //CTR block 5
+
+	fmov	d7, x23                              //AES block 3 - mov low
+	st1	{ v4.16b}, [x2], #16                    //AES block 0 - store result
+
+	fmov	v6.d[1], x22                          //AES block 2 - mov high
+
+	eor	v5.16b, v5.16b, v1.16b                         //AES block 1 - result
+	fmov	d1, x10                              //CTR block 5
+	st1	{ v5.16b}, [x2], #16                    //AES block 1 - store result
+
+	fmov	v7.d[1], x24                          //AES block 3 - mov high
+
+	fmov	v1.d[1], x9                              //CTR block 5
+	rev	w9, w12                                //CTR block 6
+
+	orr	x9, x11, x9, lsl #32           //CTR block 6
+
+	add	w12, w12, #1                           //CTR block 6
+	eor	v6.16b, v6.16b, v2.16b                         //AES block 2 - result
+	fmov	d2, x10                              //CTR block 6
+
+	fmov	v2.d[1], x9                              //CTR block 6
+	rev	w9, w12                                //CTR block 7
+
+	orr	x9, x11, x9, lsl #32           //CTR block 7
+	st1	{ v6.16b}, [x2], #16                    //AES block 2 - store result
+
+	eor	v7.16b, v7.16b, v3.16b                         //AES block 3 - result
+	st1	{ v7.16b}, [x2], #16                    //AES block 3 - store result
+	b.ge	.L192_enc_prepretail                             //do prepretail
+
+.L192_enc_main_loop:	//main	loop start
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 0
+	rev64	v5.16b, v5.16b                                   //GHASH block 4k+1 (t0 and t1 free)
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 0
+	ldp	x19, x20, [x0, #16]          //AES block 4k+5 - load plaintext
+
+	ext	v11.16b, v11.16b, v11.16b, #8                    //PRE 0
+	fmov	d3, x10                              //CTR block 4k+3
+	rev64	v4.16b, v4.16b                                   //GHASH block 4k (only t0 is free)
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 1
+	fmov	v3.d[1], x9                              //CTR block 4k+3
+
+	pmull2	v30.1q, v5.2d, v14.2d                         //GHASH block 4k+1 - high
+	rev64	v7.16b, v7.16b                                   //GHASH block 4k+3 (t0, t1, t2 and t3 free)
+	ldp	x21, x22, [x0, #32]          //AES block 4k+6 - load plaintext
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 0
+	ldp	x23, x24, [x0, #48]          //AES block 4k+3 - load plaintext
+
+	pmull	v31.1q, v5.1d, v14.1d                         //GHASH block 4k+1 - low
+	eor	v4.16b, v4.16b, v11.16b                          //PRE 1
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 1
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 1
+	rev64	v6.16b, v6.16b                                   //GHASH block 4k+2 (t0, t1, and t2 free)
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 0
+	eor	x24, x24, x14                    //AES block 4k+3 - round 12 high
+
+	pmull	v11.1q, v4.1d, v15.1d                      //GHASH block 4k - low
+	mov	d8, v4.d[1]                                 //GHASH block 4k - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 2
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 1
+	eor	x21, x21, x13                    //AES block 4k+6 - round 12 low
+
+	eor	v8.8b, v8.8b, v4.8b                         //GHASH block 4k - mid
+	eor	v11.16b, v11.16b, v31.16b                        //GHASH block 4k+1 - low
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 3
+	eor	x19, x19, x13                    //AES block 4k+5 - round 12 low
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 2
+	mov	d31, v6.d[1]                                 //GHASH block 4k+2 - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                      //GHASH block 4k - high
+	mov	d4, v5.d[1]                                 //GHASH block 4k+1 - mid
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 2
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 3
+
+	mov	d10, v17.d[1]                              //GHASH block 4k - mid
+	eor	v9.16b, v9.16b, v30.16b                        //GHASH block 4k+1 - high
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 2
+	eor	v31.8b, v31.8b, v6.8b                         //GHASH block 4k+2 - mid
+
+	pmull2	v30.1q, v6.2d, v13.2d                         //GHASH block 4k+2 - high
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 4
+	eor	v4.8b, v4.8b, v5.8b                         //GHASH block 4k+1 - mid
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 3
+
+	pmull2	v5.1q, v7.2d, v12.2d                         //GHASH block 4k+3 - high
+	eor	x20, x20, x14                    //AES block 4k+5 - round 12 high
+	ins	v31.d[1], v31.d[0]                               //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 5
+	add	w12, w12, #1                           //CTR block 4k+3
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 4
+	eor	v9.16b, v9.16b, v30.16b                        //GHASH block 4k+2 - high
+
+	pmull	v4.1q, v4.1d, v17.1d                         //GHASH block 4k+1 - mid
+	eor	x22, x22, x14                    //AES block 4k+6 - round 12 high
+
+	pmull2	v31.1q, v31.2d, v16.2d                         //GHASH block 4k+2 - mid
+	eor	x23, x23, x13                    //AES block 4k+3 - round 12 low
+	mov	d30, v7.d[1]                                 //GHASH block 4k+3 - mid
+
+	pmull	v10.1q, v8.1d, v10.1d                     //GHASH block 4k - mid
+	rev	w9, w12                                //CTR block 4k+8
+
+	pmull	v8.1q, v6.1d, v13.1d                         //GHASH block 4k+2 - low
+	orr	x9, x11, x9, lsl #32           //CTR block 4k+8
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 3
+	eor	v30.8b, v30.8b, v7.8b                         //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 4
+	ldp	x6, x7, [x0, #0]           //AES block 4k+4 - load plaintext
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 6
+	eor	v11.16b, v11.16b, v8.16b                        //GHASH block 4k+2 - low
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 4
+	add	x0, x0, #64                      //AES input_ptr update
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 5
+	movi	v8.8b, #0xc2
+
+	pmull	v6.1q, v7.1d, v12.1d                         //GHASH block 4k+3 - low
+	eor	x7, x7, x14                    //AES block 4k+4 - round 12 high
+	eor	v10.16b, v10.16b, v4.16b                        //GHASH block 4k+1 - mid
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 5
+	eor	x6, x6, x13                    //AES block 4k+4 - round 12 low
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 6
+	shl	d8, d8, #56              //mod_constant
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 5
+	eor	v9.16b, v9.16b, v5.16b                        //GHASH block 4k+3 - high
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 7
+	fmov	d5, x19                              //AES block 4k+5 - mov low
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 7
+	eor	v10.16b, v10.16b, v31.16b                        //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 6
+	fmov	v5.d[1], x20                          //AES block 4k+5 - mov high
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 8
+	eor	v11.16b, v11.16b, v6.16b                        //GHASH block 4k+3 - low
+
+	pmull	v30.1q, v30.1d, v16.1d                         //GHASH block 4k+3 - mid
+	cmp	x0, x5                  //.LOOP CONTROL
+	fmov	d4, x6                              //AES block 4k+4 - mov low
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 6
+	fmov	v4.d[1], x7                          //AES block 4k+4 - mov high
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 8
+	fmov	d7, x23                              //AES block 4k+3 - mov low
+
+	eor	v10.16b, v10.16b, v30.16b                        //GHASH block 4k+3 - mid
+	eor	v30.16b, v11.16b, v9.16b                        //MODULO - karatsuba tidy up
+	add	w12, w12, #1                           //CTR block 4k+8
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 7
+	fmov	v7.d[1], x24                          //AES block 4k+3 - mov high
+
+	pmull	v31.1q, v9.1d, v8.1d           //MODULO - top 64b align with mid
+	ext	v9.16b, v9.16b, v9.16b, #8                    //MODULO - other top alignment
+	fmov	d6, x21                              //AES block 4k+6 - mov low
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 7
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 9
+	eor	v10.16b, v10.16b, v30.16b                        //MODULO - karatsuba tidy up
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 8
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 9
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 10
+	eor	v10.16b, v10.16b, v31.16b                     //MODULO - fold into mid
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 9
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 9
+
+	aese	v0.16b, v29.16b                                    //AES block 4k+4 - round 11
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 10
+	eor	v10.16b, v10.16b, v9.16b                        //MODULO - fold into mid
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 10
+
+	eor	v4.16b, v4.16b, v0.16b                         //AES block 4k+4 - result
+	fmov	d0, x10                              //CTR block 4k+8
+
+	aese	v1.16b, v29.16b                                    //AES block 4k+5 - round 11
+	fmov	v0.d[1], x9                              //CTR block 4k+8
+	rev	w9, w12                                //CTR block 4k+9
+
+	pmull	v9.1q, v10.1d, v8.1d           //MODULO - mid 64b align with low
+	fmov	v6.d[1], x22                          //AES block 4k+6 - mov high
+	st1	{ v4.16b}, [x2], #16                    //AES block 4k+4 - store result
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 10
+	orr	x9, x11, x9, lsl #32           //CTR block 4k+9
+
+	eor	v5.16b, v5.16b, v1.16b                         //AES block 4k+5 - result
+	add	w12, w12, #1                           //CTR block 4k+9
+	fmov	d1, x10                              //CTR block 4k+9
+
+	aese	v2.16b, v29.16b                                    //AES block 4k+6 - round 11
+	fmov	v1.d[1], x9                              //CTR block 4k+9
+	rev	w9, w12                                //CTR block 4k+10
+
+	add	w12, w12, #1                           //CTR block 4k+10
+	ext	v10.16b, v10.16b, v10.16b, #8                    //MODULO - other mid alignment
+	orr	x9, x11, x9, lsl #32           //CTR block 4k+10
+
+	st1	{ v5.16b}, [x2], #16                    //AES block 4k+5 - store result
+	eor	v11.16b, v11.16b, v9.16b                        //MODULO - fold into low
+
+	aese	v3.16b, v29.16b                                    //AES block 4k+7 - round 11
+	eor	v6.16b, v6.16b, v2.16b                         //AES block 4k+6 - result
+	fmov	d2, x10                              //CTR block 4k+10
+
+	st1	{ v6.16b}, [x2], #16                    //AES block 4k+6 - store result
+	fmov	v2.d[1], x9                              //CTR block 4k+10
+	rev	w9, w12                                //CTR block 4k+11
+
+	eor	v11.16b, v11.16b, v10.16b                        //MODULO - fold into low
+	orr	x9, x11, x9, lsl #32           //CTR block 4k+11
+
+	eor	v7.16b, v7.16b, v3.16b                         //AES block 4k+3 - result
+	st1	{ v7.16b}, [x2], #16                    //AES block 4k+3 - store result
+	b.lt	.L192_enc_main_loop
+
+.L192_enc_prepretail:	//PREPRETAIL
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 0
+	rev64	v4.16b, v4.16b                                   //GHASH block 4k (only t0 is free)
+
+	fmov	d3, x10                              //CTR block 4k+3
+	ext	v11.16b, v11.16b, v11.16b, #8                    //PRE 0
+	add	w12, w12, #1                           //CTR block 4k+3
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 0
+	rev64	v5.16b, v5.16b                                   //GHASH block 4k+1 (t0 and t1 free)
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 0
+
+	fmov	v3.d[1], x9                              //CTR block 4k+3
+	eor	v4.16b, v4.16b, v11.16b                          //PRE 1
+	mov	d10, v17.d[1]                              //GHASH block 4k - mid
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 1
+	rev64	v6.16b, v6.16b                                   //GHASH block 4k+2 (t0, t1, and t2 free)
+
+	pmull2	v30.1q, v5.2d, v14.2d                         //GHASH block 4k+1 - high
+
+	pmull	v11.1q, v4.1d, v15.1d                      //GHASH block 4k - low
+	mov	d8, v4.d[1]                                 //GHASH block 4k - mid
+
+	pmull	v31.1q, v5.1d, v14.1d                         //GHASH block 4k+1 - low
+	rev64	v7.16b, v7.16b                                   //GHASH block 4k+3 (t0, t1, t2 and t3 free)
+
+	pmull2	v9.1q, v4.2d, v15.2d                      //GHASH block 4k - high
+
+	eor	v8.8b, v8.8b, v4.8b                         //GHASH block 4k - mid
+	mov	d4, v5.d[1]                                 //GHASH block 4k+1 - mid
+
+	eor	v11.16b, v11.16b, v31.16b                        //GHASH block 4k+1 - low
+	mov	d31, v6.d[1]                                 //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 0
+	eor	v9.16b, v9.16b, v30.16b                        //GHASH block 4k+1 - high
+
+	pmull2	v30.1q, v6.2d, v13.2d                         //GHASH block 4k+2 - high
+
+	eor	v4.8b, v4.8b, v5.8b                         //GHASH block 4k+1 - mid
+	eor	v31.8b, v31.8b, v6.8b                         //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 1
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 1
+	eor	v9.16b, v9.16b, v30.16b                        //GHASH block 4k+2 - high
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 1
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 2
+	mov	d30, v7.d[1]                                 //GHASH block 4k+3 - mid
+
+	pmull2	v5.1q, v7.2d, v12.2d                         //GHASH block 4k+3 - high
+	ins	v31.d[1], v31.d[0]                               //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 2
+
+	pmull	v10.1q, v8.1d, v10.1d                     //GHASH block 4k - mid
+	eor	v30.8b, v30.8b, v7.8b                         //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 3
+
+	pmull2	v31.1q, v31.2d, v16.2d                         //GHASH block 4k+2 - mid
+
+	pmull	v4.1q, v4.1d, v17.1d                         //GHASH block 4k+1 - mid
+
+	pmull	v30.1q, v30.1d, v16.1d                         //GHASH block 4k+3 - mid
+	eor	v9.16b, v9.16b, v5.16b                        //GHASH block 4k+3 - high
+
+	pmull	v8.1q, v6.1d, v13.1d                         //GHASH block 4k+2 - low
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 3
+	eor	v10.16b, v10.16b, v4.16b                        //GHASH block 4k+1 - mid
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 2
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 2
+	eor	v11.16b, v11.16b, v8.16b                        //GHASH block 4k+2 - low
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 4
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 3
+	eor	v10.16b, v10.16b, v31.16b                        //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 3
+
+	pmull	v6.1q, v7.1d, v12.1d                         //GHASH block 4k+3 - low
+	movi	v8.8b, #0xc2
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 4
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 4
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 4
+	eor	v10.16b, v10.16b, v30.16b                        //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 5
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 5
+	eor	v11.16b, v11.16b, v6.16b                        //GHASH block 4k+3 - low
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 5
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 6
+	eor	v10.16b, v10.16b, v9.16b                        //karatsuba tidy up
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 6
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 6
+	shl	d8, d8, #56              //mod_constant
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 7
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 7
+	eor	v10.16b, v10.16b, v11.16b
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 7
+
+	pmull	v30.1q, v9.1d, v8.1d
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 6
+	ext	v9.16b, v9.16b, v9.16b, #8
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 8
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 8
+	eor	v10.16b, v10.16b, v30.16b
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 7
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 8
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 9
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 8
+	eor	v10.16b, v10.16b, v9.16b
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 9
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 9
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 9
+
+	pmull	v30.1q, v10.1d, v8.1d
+
+	ext	v10.16b, v10.16b, v10.16b, #8
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b         //AES block 4k+7 - round 10
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b         //AES block 4k+4 - round 10
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b         //AES block 4k+6 - round 10
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b         //AES block 4k+5 - round 10
+	eor	v11.16b, v11.16b, v30.16b
+
+	aese	v0.16b, v29.16b                                    //AES block 4k+4 - round 11
+
+	aese	v3.16b, v29.16b                                    //AES block 4k+7 - round 11
+
+	aese	v2.16b, v29.16b                                    //AES block 4k+6 - round 11
+
+	aese	v1.16b, v29.16b                                    //AES block 4k+5 - round 11
+	eor	v11.16b, v11.16b, v10.16b
+.L192_enc_tail:	//TAIL
+
+	sub	x5, x4, x0  //main_end_input_ptr is number of bytes left to process
+	ldp	x6, x7, [x0], #16          //AES block 4k+4 - load plaintext
+
+	eor	x6, x6, x13                    //AES block 4k+4 - round 12 low
+	eor	x7, x7, x14                    //AES block 4k+4 - round 12 high
+
+	fmov	d4, x6                              //AES block 4k+4 - mov low
+
+	fmov	v4.d[1], x7                          //AES block 4k+4 - mov high
+	cmp	x5, #48
+
+	eor	v5.16b, v4.16b, v0.16b                         //AES block 4k+4 - result
+
+	ext	v8.16b, v11.16b, v11.16b, #8                    //prepare final partial tag
+	b.gt	.L192_enc_blocks_more_than_3
+
+	sub	w12, w12, #1
+	movi	v10.8b, #0
+
+	mov	v3.16b, v2.16b
+	movi	v9.8b, #0
+	cmp	x5, #32
+
+	mov	v2.16b, v1.16b
+	movi	v11.8b, #0
+	b.gt	.L192_enc_blocks_more_than_2
+
+	sub	w12, w12, #1
+
+	mov	v3.16b, v1.16b
+	cmp	x5, #16
+	b.gt	.L192_enc_blocks_more_than_1
+
+	sub	w12, w12, #1
+	b	.L192_enc_blocks_less_than_1
+.L192_enc_blocks_more_than_3:	//blocks	left >  3
+	st1	{ v5.16b}, [x2], #16                    //AES final-3 block  - store result
+
+	ldp	x6, x7, [x0], #16          //AES final-2 block - load input low & high
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-3 block
+
+	eor	x6, x6, x13                    //AES final-2 block - round 12 low
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	eor	x7, x7, x14                    //AES final-2 block - round 12 high
+	fmov	d5, x6                                //AES final-2 block - mov low
+
+	fmov	v5.d[1], x7                            //AES final-2 block - mov high
+
+	mov	d22, v4.d[1]                                //GHASH final-3 block - mid
+
+	pmull	v11.1q, v4.1d, v15.1d                      //GHASH final-3 block - low
+
+	mov	d10, v17.d[1]                              //GHASH final-3 block - mid
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-3 block - mid
+
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	pmull2	v9.1q, v4.2d, v15.2d                      //GHASH final-3 block - high
+
+	pmull	v10.1q, v22.1d, v10.1d                   //GHASH final-3 block - mid
+	eor	v5.16b, v5.16b, v1.16b                           //AES final-2 block - result
+.L192_enc_blocks_more_than_2:	//blocks	left >  2
+
+	st1	{ v5.16b}, [x2], #16                    //AES final-2 block - store result
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-2 block
+	ldp	x6, x7, [x0], #16          //AES final-1 block - load input low & high
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	eor	x7, x7, x14                    //AES final-1 block - round 12 high
+
+	pmull2	v20.1q, v4.2d, v14.2d                         //GHASH final-2 block - high
+	mov	d22, v4.d[1]                                //GHASH final-2 block - mid
+
+	pmull	v21.1q, v4.1d, v14.1d                         //GHASH final-2 block - low
+	eor	x6, x6, x13                    //AES final-1 block - round 12 low
+
+	fmov	d5, x6                                //AES final-1 block - mov low
+
+	fmov	v5.d[1], x7                            //AES final-1 block - mov high
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final-2 block - high
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-2 block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final-2 block - low
+
+	pmull	v22.1q, v22.1d, v17.1d                     //GHASH final-2 block - mid
+
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	eor	v5.16b, v5.16b, v2.16b                           //AES final-1 block - result
+
+	eor	v10.16b, v10.16b, v22.16b                      //GHASH final-2 block - mid
+.L192_enc_blocks_more_than_1:	//blocks	left >  1
+
+	st1	{ v5.16b}, [x2], #16                    //AES final-1 block - store result
+
+	ldp	x6, x7, [x0], #16          //AES final block - load input low & high
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-1 block
+
+	eor	x6, x6, x13                    //AES final block - round 12 low
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	mov	d22, v4.d[1]                                //GHASH final-1 block - mid
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-1 block - mid
+	eor	x7, x7, x14                    //AES final block - round 12 high
+	fmov	d5, x6                                //AES final block - mov low
+
+	pmull2	v20.1q, v4.2d, v13.2d                         //GHASH final-1 block - high
+	fmov	v5.d[1], x7                            //AES final block - mov high
+
+	ins	v22.d[1], v22.d[0]                           //GHASH final-1 block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final-1 block - high
+
+	pmull	v21.1q, v4.1d, v13.1d                         //GHASH final-1 block - low
+
+	pmull2	v22.1q, v22.2d, v16.2d                     //GHASH final-1 block - mid
+
+	eor	v5.16b, v5.16b, v3.16b                           //AES final block - result
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final-1 block - low
+
+	eor	v10.16b, v10.16b, v22.16b                      //GHASH final-1 block - mid
+.L192_enc_blocks_less_than_1:	//blocks	left <= 1
+
+	ld1	{ v18.16b}, [x2]                           //load existing bytes where the possibly partial last block is to be stored
+	rev	w9, w12
+	and	x1, x1, #127                   //bit_length %= 128
+
+	sub	x1, x1, #128                   //bit_length -= 128
+	mvn	x14, xzr                                     //rk12_h = 0xffffffffffffffff
+
+	neg	x1, x1                         //bit_length = 128 - #bits in input (in range [1,128])
+	mvn	x13, xzr                                     //rk12_l = 0xffffffffffffffff
+
+	and	x1, x1, #127                   //bit_length %= 128
+
+	lsr	x14, x14, x1                    //rk12_h is mask for top 64b of last block
+	cmp	x1, #64
+
+	csel	x6, x13, x14, lt
+	csel	x7, x14, xzr, lt
+
+	fmov	d0, x6                                //ctr0b is mask for last block
+
+	fmov	v0.d[1], x7
+
+	and	v5.16b, v5.16b, v0.16b                           //possibly partial last block has zeroes in highest bits
+
+	rev64	v4.16b, v5.16b                                   //GHASH final block
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	mov	d8, v4.d[1]                                 //GHASH final block - mid
+
+	pmull	v21.1q, v4.1d, v12.1d                         //GHASH final block - low
+
+	pmull2	v20.1q, v4.2d, v12.2d                         //GHASH final block - high
+
+	eor	v8.8b, v8.8b, v4.8b                         //GHASH final block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final block - low
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final block - high
+
+	pmull	v8.1q, v8.1d, v16.1d                         //GHASH final block - mid
+
+	eor	v10.16b, v10.16b, v8.16b                        //GHASH final block - mid
+	movi	v8.8b, #0xc2
+
+	eor	v30.16b, v11.16b, v9.16b                        //MODULO - karatsuba tidy up
+
+	shl	d8, d8, #56              //mod_constant
+
+	bif	v5.16b, v18.16b, v0.16b                             //insert existing bytes in top end of result before storing
+
+	eor	v10.16b, v10.16b, v30.16b                        //MODULO - karatsuba tidy up
+
+	pmull	v31.1q, v9.1d, v8.1d           //MODULO - top 64b align with mid
+
+	ext	v9.16b, v9.16b, v9.16b, #8                    //MODULO - other top alignment
+
+	eor	v10.16b, v10.16b, v31.16b                     //MODULO - fold into mid
+
+	eor	v10.16b, v10.16b, v9.16b                        //MODULO - fold into mid
+
+	pmull	v9.1q, v10.1d, v8.1d           //MODULO - mid 64b align with low
+
+	ext	v10.16b, v10.16b, v10.16b, #8                    //MODULO - other mid alignment
+
+	eor	v11.16b, v11.16b, v9.16b                        //MODULO - fold into low
+	str	w9, [x16, #12]                         //store the updated counter
+
+	st1	{ v5.16b}, [x2]                         //store all 16B
+
+	eor	v11.16b, v11.16b, v10.16b                        //MODULO - fold into low
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	mov	x0, x15
+	st1	{ v11.16b }, [x3]
+
+	ldp	x21, x22, [sp, #16]
+	ldp	x23, x24, [sp, #32]
+	ldp	d8, d9, [sp, #48]
+	ldp	d10, d11, [sp, #64]
+	ldp	d12, d13, [sp, #80]
+	ldp	d14, d15, [sp, #96]
+	ldp	x19, x20, [sp], #112
+	ret
+
+.L192_enc_ret:
+	mov	w0, #0x0
+	ret
+.size	aes_gcm_enc_192_kernel,.-aes_gcm_enc_192_kernel
+.globl	aes_gcm_dec_192_kernel
+.type	aes_gcm_dec_192_kernel,%function
+.align	4
+aes_gcm_dec_192_kernel:
+	cbz	x1, .L192_dec_ret
+	stp	x19, x20, [sp, #-112]!
+	mov	x16, x4
+	mov	x8, x5
+	stp	x21, x22, [sp, #16]
+	stp	x23, x24, [sp, #32]
+	stp	d8, d9, [sp, #48]
+	stp	d10, d11, [sp, #64]
+	stp	d12, d13, [sp, #80]
+	stp	d14, d15, [sp, #96]
+
+	add	x4, x0, x1, lsr #3   //end_input_ptr
+	ldp	x10, x11, [x16]              //ctr96_b64, ctr96_t32
+
+	ld1	{ v0.16b}, [x16]                             //special case vector load initial counter so we can start first AES block as quickly as possible
+
+	ldr	q18, [x8, #0]                                  //load rk0
+
+	lsr	x5, x1, #3              //byte_len
+	mov	x15, x5
+	ldr	q20, [x8, #32]                                 //load rk2
+
+	lsr	x12, x11, #32
+	orr	w11, w11, w11
+	fmov	d3, x10                               //CTR block 3
+
+	rev	w12, w12                                //rev_ctr32
+	fmov	d1, x10                               //CTR block 1
+
+	add	w12, w12, #1                            //increment rev_ctr32
+	ldr	q19, [x8, #16]                                 //load rk1
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 0
+	rev	w9, w12                                 //CTR block 1
+
+	add	w12, w12, #1                            //CTR block 1
+	orr	x9, x11, x9, lsl #32            //CTR block 1
+	ldr	q21, [x8, #48]                                 //load rk3
+
+	fmov	v1.d[1], x9                               //CTR block 1
+	rev	w9, w12                                 //CTR block 2
+	add	w12, w12, #1                            //CTR block 2
+
+	fmov	d2, x10                               //CTR block 2
+	orr	x9, x11, x9, lsl #32            //CTR block 2
+
+	fmov	v2.d[1], x9                               //CTR block 2
+	rev	w9, w12                                 //CTR block 3
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 1
+	orr	x9, x11, x9, lsl #32            //CTR block 3
+
+	fmov	v3.d[1], x9                               //CTR block 3
+
+	ldr	q26, [x8, #128]                                //load rk8
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 2
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 0
+	ldr	q29, [x8, #176]                               //load rk11
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 0
+	ldr	q15, [x3, #112]                        //load h4l | h4h
+	ext	v15.16b, v15.16b, v15.16b, #8
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 0
+	ldr	q13, [x3, #64]                         //load h2l | h2h
+	ext	v13.16b, v13.16b, v13.16b, #8
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 1
+	ldr	q14, [x3, #80]                         //load h3l | h3h
+	ext	v14.16b, v14.16b, v14.16b, #8
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 1
+	ldp	x13, x14, [x8, #192]                     //load rk12
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 1
+	ldr	q12, [x3, #32]                         //load h1l | h1h
+	ext	v12.16b, v12.16b, v12.16b, #8
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 2
+	ldr	q28, [x8, #160]                               //load rk10
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 3
+	ldr	q27, [x8, #144]                                //load rk9
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 2
+	ldr	q25, [x8, #112]                                //load rk7
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 2
+	ldr	q22, [x8, #64]                                 //load rk4
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 3
+	ld1	{ v11.16b}, [x3]
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 3
+	add	w12, w12, #1                            //CTR block 3
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 3
+	trn1	v9.2d, v14.2d,    v15.2d                      //h4h | h3h
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 4
+	ldr	q23, [x8, #80]                                 //load rk5
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 4
+	trn2	v17.2d,  v14.2d,    v15.2d                      //h4l | h3l
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 4
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 4
+	trn2	v16.2d,  v12.2d,    v13.2d                      //h2l | h1l
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 5
+	ldr	q24, [x8, #96]                                 //load rk6
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 5
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 5
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 6
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 6
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 6
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 7
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 7
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 7
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 6
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 8
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 7
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 9
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 9
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 8
+	sub	x5, x5, #1      //byte_len - 1
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 8
+	and	x5, x5, #0xffffffffffffffc0    //number of bytes to be processed in main loop (at least 1 byte must be handled by tail)
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 10
+	add	x5, x5, x0
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 9
+	cmp	x0, x5                   //check if we have <= 4 blocks
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 9
+	trn1	v8.2d,    v12.2d,    v13.2d                      //h2h | h1h
+
+	aese	v3.16b, v29.16b                                     //AES block 3 - round 11
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 10
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 10
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 10
+	eor	v16.16b, v16.16b, v8.16b                     //h2k | h1k
+
+	aese	v2.16b, v29.16b                                     //AES block 2 - round 11
+
+	aese	v1.16b, v29.16b                                     //AES block 1 - round 11
+	eor	v17.16b, v17.16b, v9.16b                  //h4k | h3k
+
+	aese	v0.16b, v29.16b                                     //AES block 0 - round 11
+	b.ge	.L192_dec_tail                                    //handle tail
+
+	ldr	q5, [x0, #16]                         //AES block 1 - load ciphertext
+
+	ldr	q4, [x0, #0]                          //AES block 0 - load ciphertext
+
+	eor	v1.16b, v5.16b, v1.16b                            //AES block 1 - result
+
+	eor	v0.16b, v4.16b, v0.16b                            //AES block 0 - result
+	rev	w9, w12                                 //CTR block 4
+	ldr	q7, [x0, #48]                         //AES block 3 - load ciphertext
+
+	ldr	q6, [x0, #32]                         //AES block 2 - load ciphertext
+
+	mov	x19, v1.d[0]                            //AES block 1 - mov low
+
+	mov	x20, v1.d[1]                            //AES block 1 - mov high
+
+	mov	x6, v0.d[0]                            //AES block 0 - mov low
+	orr	x9, x11, x9, lsl #32            //CTR block 4
+	add	w12, w12, #1                            //CTR block 4
+
+	mov	x7, v0.d[1]                            //AES block 0 - mov high
+	rev64	v4.16b, v4.16b                                    //GHASH block 0
+	add	x0, x0, #64                       //AES input_ptr update
+
+	fmov	d0, x10                               //CTR block 4
+	rev64	v5.16b, v5.16b                                    //GHASH block 1
+	cmp	x0, x5                   //check if we have <= 8 blocks
+
+	eor	x19, x19, x13                   //AES block 1 - round 12 low
+	fmov	v0.d[1], x9                               //CTR block 4
+	rev	w9, w12                                 //CTR block 5
+
+	orr	x9, x11, x9, lsl #32            //CTR block 5
+	fmov	d1, x10                               //CTR block 5
+	eor	x20, x20, x14                   //AES block 1 - round 12 high
+
+	add	w12, w12, #1                            //CTR block 5
+	fmov	v1.d[1], x9                               //CTR block 5
+	eor	x6, x6, x13                   //AES block 0 - round 12 low
+
+	rev	w9, w12                                 //CTR block 6
+	eor	x7, x7, x14                   //AES block 0 - round 12 high
+
+	stp	x6, x7, [x2], #16        //AES block 0 - store result
+	orr	x9, x11, x9, lsl #32            //CTR block 6
+
+	stp	x19, x20, [x2], #16        //AES block 1 - store result
+
+	add	w12, w12, #1                            //CTR block 6
+	eor	v2.16b, v6.16b, v2.16b                            //AES block 2 - result
+	b.ge	.L192_dec_prepretail                              //do prepretail
+
+.L192_dec_main_loop:	//main	loop start
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+
+	pmull	v31.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+	mov	x21, v2.d[0]                            //AES block 4k+2 - mov low
+
+	mov	x22, v2.d[1]                            //AES block 4k+2 - mov high
+	eor	v3.16b, v7.16b, v3.16b                            //AES block 4k+3 - result
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	fmov	d2, x10                               //CTR block 4k+6
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+
+	pmull2	v30.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+	fmov	v2.d[1], x9                               //CTR block 4k+6
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	mov	x24, v3.d[1]                            //AES block 4k+3 - mov high
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	mov	x23, v3.d[0]                            //AES block 4k+3 - mov low
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	fmov	d3, x10                               //CTR block 4k+7
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+	rev	w9, w12                                 //CTR block 4k+7
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+7
+
+	fmov	v3.d[1], x9                               //CTR block 4k+7
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+	mov	d4, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+	eor	x22, x22, x14                   //AES block 4k+2 - round 12 high
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	eor	v4.8b, v4.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+
+	pmull	v4.1q, v4.1d, v17.1d                          //GHASH block 4k+1 - mid
+	eor	v11.16b, v11.16b, v31.16b                         //GHASH block 4k+1 - low
+	eor	x21, x21, x13                   //AES block 4k+2 - round 12 low
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+1 - mid
+	mov	d31, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	eor	v9.16b, v9.16b, v30.16b                         //GHASH block 4k+1 - high
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+
+	pmull2	v30.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+	eor	v31.8b, v31.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	pmull	v8.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+
+	eor	v9.16b, v9.16b, v30.16b                         //GHASH block 4k+2 - high
+	mov	d30, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	pmull2	v5.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	eor	v30.8b, v30.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	ins	v31.d[1], v31.d[0]                                //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+
+	pmull	v30.1q, v30.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	v11.16b, v11.16b, v8.16b                         //GHASH block 4k+2 - low
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+
+	pmull2	v31.1q, v31.2d, v16.2d                          //GHASH block 4k+2 - mid
+	eor	v9.16b, v9.16b, v5.16b                         //GHASH block 4k+3 - high
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	movi	v8.8b, #0xc2
+
+	pmull	v6.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+	eor	v10.16b, v10.16b, v31.16b                         //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 9
+	eor	v11.16b, v11.16b, v6.16b                         //GHASH block 4k+3 - low
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+3 - mid
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 10
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 9
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 10
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+	ldr	q6, [x0, #32]                         //AES block 4k+6 - load ciphertext
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	ldr	q7, [x0, #48]                         //AES block 4k+7 - load ciphertext
+	eor	x23, x23, x13                   //AES block 4k+3 - round 12 low
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v0.16b, v29.16b                                     //AES block 4k+4 - round 11
+	add	w12, w12, #1                            //CTR block 4k+7
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+	ldr	q4, [x0, #0]                          //AES block 4k+4 - load ciphertext
+
+	aese	v1.16b, v29.16b                                     //AES block 4k+5 - round 11
+	ldr	q5, [x0, #16]                         //AES block 4k+5 - load ciphertext
+	rev	w9, w12                                 //CTR block 4k+8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+	stp	x21, x22, [x2], #16        //AES block 4k+2 - store result
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 9
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	add	x0, x0, #64                       //AES input_ptr update
+	cmp	x0, x5                   //.LOOP CONTROL
+
+	eor	v0.16b, v4.16b, v0.16b                            //AES block 4k+4 - result
+	eor	x24, x24, x14                   //AES block 4k+3 - round 12 high
+	eor	v1.16b, v5.16b, v1.16b                            //AES block 4k+5 - result
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 10
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+8
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 9
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+	mov	x19, v1.d[0]                            //AES block 4k+5 - mov low
+
+	mov	x6, v0.d[0]                            //AES block 4k+4 - mov low
+	stp	x23, x24, [x2], #16        //AES block 4k+3 - store result
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+5
+
+	aese	v2.16b, v29.16b                                     //AES block 4k+6 - round 11
+	mov	x7, v0.d[1]                            //AES block 4k+4 - mov high
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 10
+	mov	x20, v1.d[1]                            //AES block 4k+5 - mov high
+
+	fmov	d0, x10                               //CTR block 4k+8
+	add	w12, w12, #1                            //CTR block 4k+8
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	eor	v2.16b, v6.16b, v2.16b                            //AES block 4k+6 - result
+	fmov	v0.d[1], x9                               //CTR block 4k+8
+	rev	w9, w12                                 //CTR block 4k+9
+
+	eor	x6, x6, x13                   //AES block 4k+4 - round 12 low
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+9
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	fmov	d1, x10                               //CTR block 4k+9
+	add	w12, w12, #1                            //CTR block 4k+9
+	eor	x19, x19, x13                   //AES block 4k+5 - round 12 low
+
+	fmov	v1.d[1], x9                               //CTR block 4k+9
+	rev	w9, w12                                 //CTR block 4k+10
+	eor	x20, x20, x14                   //AES block 4k+5 - round 12 high
+
+	eor	x7, x7, x14                   //AES block 4k+4 - round 12 high
+	stp	x6, x7, [x2], #16        //AES block 4k+4 - store result
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+
+	add	w12, w12, #1                            //CTR block 4k+10
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k+4
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+10
+
+	aese	v3.16b, v29.16b                                     //AES block 4k+7 - round 11
+	stp	x19, x20, [x2], #16        //AES block 4k+5 - store result
+	b.lt	.L192_dec_main_loop
+
+.L192_dec_prepretail:	//PREPRETAIL
+	mov	x22, v2.d[1]                            //AES block 4k+2 - mov high
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	eor	v3.16b, v7.16b, v3.16b                            //AES block 4k+3 - result
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	mov	x21, v2.d[0]                            //AES block 4k+2 - mov low
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+	fmov	d2, x10                               //CTR block 4k+6
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	mov	x23, v3.d[0]                            //AES block 4k+3 - mov low
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	mov	x24, v3.d[1]                            //AES block 4k+3 - mov high
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+	fmov	d3, x10                               //CTR block 4k+7
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	fmov	v2.d[1], x9                               //CTR block 4k+6
+	rev	w9, w12                                 //CTR block 4k+7
+
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+7
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+	mov	d4, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	pmull	v31.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+	eor	x24, x24, x14                   //AES block 4k+3 - round 12 high
+	fmov	v3.d[1], x9                               //CTR block 4k+7
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+	eor	x21, x21, x13                   //AES block 4k+2 - round 12 low
+
+	pmull2	v30.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+	eor	x22, x22, x14                   //AES block 4k+2 - round 12 high
+	eor	v4.8b, v4.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+	eor	x23, x23, x13                   //AES block 4k+3 - round 12 low
+	stp	x21, x22, [x2], #16        //AES block 4k+2 - store result
+
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3
+	stp	x23, x24, [x2], #16        //AES block 4k+3 - store result
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	eor	v9.16b, v9.16b, v30.16b                         //GHASH block 4k+1 - high
+
+	pmull	v4.1q, v4.1d, v17.1d                          //GHASH block 4k+1 - mid
+	add	w12, w12, #1                            //CTR block 4k+7
+
+	pmull2	v30.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+	eor	v11.16b, v11.16b, v31.16b                         //GHASH block 4k+1 - low
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+1 - mid
+	mov	d31, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	eor	v9.16b, v9.16b, v30.16b                         //GHASH block 4k+2 - high
+
+	eor	v31.8b, v31.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	pmull	v8.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+	mov	d30, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	ins	v31.d[1], v31.d[0]                                //GHASH block 4k+2 - mid
+
+	pmull	v6.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	eor	v30.8b, v30.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+
+	pmull2	v31.1q, v31.2d, v16.2d                          //GHASH block 4k+2 - mid
+	eor	v11.16b, v11.16b, v8.16b                         //GHASH block 4k+2 - low
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+
+	pmull2	v5.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+	movi	v8.8b, #0xc2
+
+	pmull	v30.1q, v30.1d, v16.1d                          //GHASH block 4k+3 - mid
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	shl	d8, d8, #56               //mod_constant
+	eor	v9.16b, v9.16b, v5.16b                         //GHASH block 4k+3 - high
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	eor	v10.16b, v10.16b, v31.16b                         //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	eor	v11.16b, v11.16b, v6.16b                         //GHASH block 4k+3 - low
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	eor	v10.16b, v10.16b, v30.16b                         //GHASH block 4k+3 - mid
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 9
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 10
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 9
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 9
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 9
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 10
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 10
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 10
+
+	aese	v0.16b, v29.16b
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	aese	v2.16b, v29.16b
+
+	aese	v1.16b, v29.16b
+
+	aese	v3.16b, v29.16b
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+.L192_dec_tail:	//TAIL
+
+	sub	x5, x4, x0   //main_end_input_ptr is number of bytes left to process
+	ld1	{ v5.16b}, [x0], #16                      //AES block 4k+4 - load ciphertext
+
+	eor	v0.16b, v5.16b, v0.16b                            //AES block 4k+4 - result
+
+	mov	x7, v0.d[1]                            //AES block 4k+4 - mov high
+
+	mov	x6, v0.d[0]                            //AES block 4k+4 - mov low
+
+	ext	v8.16b, v11.16b, v11.16b, #8                     //prepare final partial tag
+
+	cmp	x5, #48
+
+	eor	x7, x7, x14                   //AES block 4k+4 - round 12 high
+
+	eor	x6, x6, x13                   //AES block 4k+4 - round 12 low
+	b.gt	.L192_dec_blocks_more_than_3
+
+	movi	v11.8b, #0
+	movi	v9.8b, #0
+
+	mov	v3.16b, v2.16b
+	mov	v2.16b, v1.16b
+	sub	w12, w12, #1
+
+	movi	v10.8b, #0
+	cmp	x5, #32
+	b.gt	.L192_dec_blocks_more_than_2
+
+	mov	v3.16b, v1.16b
+	cmp	x5, #16
+	sub	w12, w12, #1
+
+	b.gt	.L192_dec_blocks_more_than_1
+
+	sub	w12, w12, #1
+	b	.L192_dec_blocks_less_than_1
+.L192_dec_blocks_more_than_3:	//blocks	left >  3
+	rev64	v4.16b, v5.16b                                    //GHASH final-3 block
+	ld1	{ v5.16b}, [x0], #16                      //AES final-2 block - load ciphertext
+
+	stp	x6, x7, [x2], #16        //AES final-3 block  - store result
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	eor	v0.16b, v5.16b, v1.16b                            //AES final-2 block - result
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH final-3 block - low
+	mov	x6, v0.d[0]                            //AES final-2 block - mov low
+	mov	d22, v4.d[1]                                 //GHASH final-3 block - mid
+
+	mov	x7, v0.d[1]                            //AES final-2 block - mov high
+
+	mov	d10, v17.d[1]                               //GHASH final-3 block - mid
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-3 block - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH final-3 block - high
+
+	eor	x6, x6, x13                   //AES final-2 block - round 12 low
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+
+	pmull	v10.1q, v22.1d, v10.1d                    //GHASH final-3 block - mid
+	eor	x7, x7, x14                   //AES final-2 block - round 12 high
+.L192_dec_blocks_more_than_2:	//blocks	left >  2
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-2 block
+	ld1	{ v5.16b}, [x0], #16                      //AES final-1 block - load ciphertext
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+
+	eor	v0.16b, v5.16b, v2.16b                            //AES final-1 block - result
+
+	mov	d22, v4.d[1]                                 //GHASH final-2 block - mid
+
+	pmull	v21.1q, v4.1d, v14.1d                          //GHASH final-2 block - low
+
+	stp	x6, x7, [x2], #16        //AES final-2 block  - store result
+
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-2 block - mid
+	mov	x7, v0.d[1]                            //AES final-1 block - mov high
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final-2 block - low
+	mov	x6, v0.d[0]                            //AES final-1 block - mov low
+
+	pmull2	v20.1q, v4.2d, v14.2d                          //GHASH final-2 block - high
+
+	pmull	v22.1q, v22.1d, v17.1d                      //GHASH final-2 block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final-2 block - high
+	eor	x7, x7, x14                   //AES final-1 block - round 12 high
+
+	eor	x6, x6, x13                   //AES final-1 block - round 12 low
+	eor	v10.16b, v10.16b, v22.16b                       //GHASH final-2 block - mid
+.L192_dec_blocks_more_than_1:	//blocks	left >  1
+
+	rev64	v4.16b, v5.16b                                    //GHASH final-1 block
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+	ld1	{ v5.16b}, [x0], #16                      //AES final block - load ciphertext
+
+	mov	d22, v4.d[1]                                 //GHASH final-1 block - mid
+
+	pmull2	v20.1q, v4.2d, v13.2d                          //GHASH final-1 block - high
+
+	eor	v0.16b, v5.16b, v3.16b                            //AES final block - result
+	stp	x6, x7, [x2], #16        //AES final-1 block  - store result
+
+	eor	v22.8b, v22.8b, v4.8b                      //GHASH final-1 block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final-1 block - high
+
+	pmull	v21.1q, v4.1d, v13.1d                          //GHASH final-1 block - low
+	mov	x7, v0.d[1]                            //AES final block - mov high
+
+	ins	v22.d[1], v22.d[0]                            //GHASH final-1 block - mid
+	mov	x6, v0.d[0]                            //AES final block - mov low
+
+	pmull2	v22.1q, v22.2d, v16.2d                      //GHASH final-1 block - mid
+
+	movi	v8.8b, #0                                        //suppress further partial tag feed in
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final-1 block - low
+	eor	x7, x7, x14                   //AES final block - round 12 high
+
+	eor	x6, x6, x13                   //AES final block - round 12 low
+
+	eor	v10.16b, v10.16b, v22.16b                       //GHASH final-1 block - mid
+.L192_dec_blocks_less_than_1:	//blocks	left <= 1
+
+	mvn	x13, xzr                                      //rk12_l = 0xffffffffffffffff
+	ldp	x4, x5, [x2]  //load existing bytes we need to not overwrite
+	and	x1, x1, #127                    //bit_length %= 128
+
+	sub	x1, x1, #128                    //bit_length -= 128
+
+	neg	x1, x1                          //bit_length = 128 - #bits in input (in range [1,128])
+
+	and	x1, x1, #127                    //bit_length %= 128
+	mvn	x14, xzr                                      //rk12_h = 0xffffffffffffffff
+
+	lsr	x14, x14, x1                     //rk12_h is mask for top 64b of last block
+	cmp	x1, #64
+
+	csel	x9, x13, x14, lt
+	csel	x10, x14, xzr, lt
+
+	fmov	d0, x9                                   //ctr0b is mask for last block
+	and	x6, x6, x9
+	bic	x4, x4, x9           //mask out low existing bytes
+
+	orr	x6, x6, x4
+	mov	v0.d[1], x10
+
+	rev	w9, w12
+
+	and	v5.16b, v5.16b, v0.16b                            //possibly partial last block has zeroes in highest bits
+	str	w9, [x16, #12]                          //store the updated counter
+
+	rev64	v4.16b, v5.16b                                    //GHASH final block
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+	bic	x5, x5, x10 //mask out high existing bytes
+
+	and	x7, x7, x10
+
+	pmull2	v20.1q, v4.2d, v12.2d                          //GHASH final block - high
+	mov	d8, v4.d[1]                                  //GHASH final block - mid
+
+	pmull	v21.1q, v4.1d, v12.1d                          //GHASH final block - low
+
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH final block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final block - high
+
+	pmull	v8.1q, v8.1d, v16.1d                          //GHASH final block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final block - low
+
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH final block - mid
+	movi	v8.8b, #0xc2
+
+	eor	v30.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	shl	d8, d8, #56               //mod_constant
+
+	eor	v10.16b, v10.16b, v30.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v31.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	orr	x7, x7, x5
+	stp	x6, x7, [x2]
+
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	eor	v10.16b, v10.16b, v31.16b                      //MODULO - fold into mid
+
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	mov	x0, x15
+	st1	{ v11.16b }, [x3]
+
+	ldp	x21, x22, [sp, #16]
+	ldp	x23, x24, [sp, #32]
+	ldp	d8, d9, [sp, #48]
+	ldp	d10, d11, [sp, #64]
+	ldp	d12, d13, [sp, #80]
+	ldp	d14, d15, [sp, #96]
+	ldp	x19, x20, [sp], #112
+	ret
+
+.L192_dec_ret:
+	mov	w0, #0x0
+	ret
+.size	aes_gcm_dec_192_kernel,.-aes_gcm_dec_192_kernel
+.globl	aes_gcm_enc_256_kernel
+.type	aes_gcm_enc_256_kernel,%function
+.align	4
+aes_gcm_enc_256_kernel:
+	cbz	x1, .L256_enc_ret
+	stp	x19, x20, [sp, #-112]!
+	mov	x16, x4
+	mov	x8, x5
+	stp	x21, x22, [sp, #16]
+	stp	x23, x24, [sp, #32]
+	stp	d8, d9, [sp, #48]
+	stp	d10, d11, [sp, #64]
+	stp	d12, d13, [sp, #80]
+	stp	d14, d15, [sp, #96]
+
+	add	x4, x0, x1, lsr #3   //end_input_ptr
+	lsr	x5, x1, #3              //byte_len
+	mov	x15, x5
+	ldp	x10, x11, [x16]              //ctr96_b64, ctr96_t32
+
+	ld1	{ v0.16b}, [x16]                             //special case vector load initial counter so we can start first AES block as quickly as possible
+	sub	x5, x5, #1      //byte_len - 1
+
+	ldr	q18, [x8, #0]                                  //load rk0
+	and	x5, x5, #0xffffffffffffffc0 //number of bytes to be processed in main loop (at least 1 byte must be handled by tail)
+
+	ldr	q25, [x8, #112]                                //load rk7
+	add	x5, x5, x0
+
+	lsr	x12, x11, #32
+	fmov	d2, x10                               //CTR block 2
+	orr	w11, w11, w11
+
+	rev	w12, w12                                //rev_ctr32
+	cmp	x0, x5                   //check if we have <= 4 blocks
+	fmov	d1, x10                               //CTR block 1
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 0
+	add	w12, w12, #1                            //increment rev_ctr32
+
+	rev	w9, w12                                 //CTR block 1
+	fmov	d3, x10                               //CTR block 3
+
+	orr	x9, x11, x9, lsl #32            //CTR block 1
+	add	w12, w12, #1                            //CTR block 1
+	ldr	q19, [x8, #16]                                 //load rk1
+
+	fmov	v1.d[1], x9                               //CTR block 1
+	rev	w9, w12                                 //CTR block 2
+	add	w12, w12, #1                            //CTR block 2
+
+	orr	x9, x11, x9, lsl #32            //CTR block 2
+	ldr	q20, [x8, #32]                                 //load rk2
+
+	fmov	v2.d[1], x9                               //CTR block 2
+	rev	w9, w12                                 //CTR block 3
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 1
+	orr	x9, x11, x9, lsl #32            //CTR block 3
+
+	fmov	v3.d[1], x9                               //CTR block 3
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 0
+	ldr	q21, [x8, #48]                                 //load rk3
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 2
+	ldr	q24, [x8, #96]                                 //load rk6
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 0
+	ldr	q23, [x8, #80]                                 //load rk5
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 1
+	ldr	q14, [x3, #80]                         //load h3l | h3h
+	ext	v14.16b, v14.16b, v14.16b, #8
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 0
+	ldr	q31, [x8, #208]                               //load rk13
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 1
+	ldr	q22, [x8, #64]                                 //load rk4
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 2
+	ldr	q13, [x3, #64]                         //load h2l | h2h
+	ext	v13.16b, v13.16b, v13.16b, #8
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 1
+	ldr	q30, [x8, #192]                               //load rk12
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 2
+	ldr	q15, [x3, #112]                        //load h4l | h4h
+	ext	v15.16b, v15.16b, v15.16b, #8
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 3
+	ldr	q29, [x8, #176]                               //load rk11
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 2
+	ldr	q26, [x8, #128]                                //load rk8
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 3
+	add	w12, w12, #1                            //CTR block 3
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 3
+	ldp	x13, x14, [x8, #224]                     //load rk14
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 3
+	ld1	{ v11.16b}, [x3]
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 4
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 4
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 4
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 4
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 5
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 5
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 5
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 6
+	trn2	v17.2d,  v14.2d,    v15.2d                      //h4l | h3l
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 6
+	ldr	q27, [x8, #144]                                //load rk9
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 6
+	ldr	q12, [x3, #32]                         //load h1l | h1h
+	ext	v12.16b, v12.16b, v12.16b, #8
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 6
+	ldr	q28, [x8, #160]                               //load rk10
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 7
+	trn1	v9.2d, v14.2d,    v15.2d                      //h4h | h3h
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 7
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 7
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 7
+	trn2	v16.2d,  v12.2d,    v13.2d                      //h2l | h1l
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 8
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 8
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 8
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 9
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 9
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 8
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 10
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 9
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 9
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 10
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 10
+
+	aese	v1.16b, v29.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 11
+
+	aese	v2.16b, v29.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 11
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 10
+
+	aese	v1.16b, v30.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 12
+
+	aese	v2.16b, v30.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 12
+
+	aese	v0.16b, v29.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 11
+	eor	v17.16b, v17.16b, v9.16b                  //h4k | h3k
+
+	aese	v3.16b, v29.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 11
+
+	aese	v2.16b, v31.16b                                     //AES block 2 - round 13
+	trn1	v8.2d,    v12.2d,    v13.2d                      //h2h | h1h
+
+	aese	v0.16b, v30.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 12
+
+	aese	v3.16b, v30.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 12
+
+	aese	v1.16b, v31.16b                                     //AES block 1 - round 13
+
+	aese	v0.16b, v31.16b                                     //AES block 0 - round 13
+
+	aese	v3.16b, v31.16b                                     //AES block 3 - round 13
+	eor	v16.16b, v16.16b, v8.16b                     //h2k | h1k
+	b.ge	.L256_enc_tail                                    //handle tail
+
+	ldp	x19, x20, [x0, #16]           //AES block 1 - load plaintext
+
+	rev	w9, w12                                 //CTR block 4
+	ldp	x6, x7, [x0, #0]            //AES block 0 - load plaintext
+
+	ldp	x23, x24, [x0, #48]           //AES block 3 - load plaintext
+
+	ldp	x21, x22, [x0, #32]           //AES block 2 - load plaintext
+	add	x0, x0, #64                       //AES input_ptr update
+
+	eor	x19, x19, x13                     //AES block 1 - round 14 low
+	eor	x20, x20, x14                     //AES block 1 - round 14 high
+
+	fmov	d5, x19                               //AES block 1 - mov low
+	eor	x6, x6, x13                     //AES block 0 - round 14 low
+
+	eor	x7, x7, x14                     //AES block 0 - round 14 high
+	eor	x24, x24, x14                     //AES block 3 - round 14 high
+	fmov	d4, x6                               //AES block 0 - mov low
+
+	cmp	x0, x5                   //check if we have <= 8 blocks
+	fmov	v4.d[1], x7                           //AES block 0 - mov high
+	eor	x23, x23, x13                     //AES block 3 - round 14 low
+
+	eor	x21, x21, x13                     //AES block 2 - round 14 low
+	fmov	v5.d[1], x20                           //AES block 1 - mov high
+
+	fmov	d6, x21                               //AES block 2 - mov low
+	add	w12, w12, #1                            //CTR block 4
+
+	orr	x9, x11, x9, lsl #32            //CTR block 4
+	fmov	d7, x23                               //AES block 3 - mov low
+	eor	x22, x22, x14                     //AES block 2 - round 14 high
+
+	fmov	v6.d[1], x22                           //AES block 2 - mov high
+
+	eor	v4.16b, v4.16b, v0.16b                          //AES block 0 - result
+	fmov	d0, x10                               //CTR block 4
+
+	fmov	v0.d[1], x9                               //CTR block 4
+	rev	w9, w12                                 //CTR block 5
+	add	w12, w12, #1                            //CTR block 5
+
+	eor	v5.16b, v5.16b, v1.16b                          //AES block 1 - result
+	fmov	d1, x10                               //CTR block 5
+	orr	x9, x11, x9, lsl #32            //CTR block 5
+
+	fmov	v1.d[1], x9                               //CTR block 5
+	rev	w9, w12                                 //CTR block 6
+	st1	{ v4.16b}, [x2], #16                     //AES block 0 - store result
+
+	fmov	v7.d[1], x24                           //AES block 3 - mov high
+	orr	x9, x11, x9, lsl #32            //CTR block 6
+	eor	v6.16b, v6.16b, v2.16b                          //AES block 2 - result
+
+	st1	{ v5.16b}, [x2], #16                     //AES block 1 - store result
+
+	add	w12, w12, #1                            //CTR block 6
+	fmov	d2, x10                               //CTR block 6
+
+	fmov	v2.d[1], x9                               //CTR block 6
+	st1	{ v6.16b}, [x2], #16                     //AES block 2 - store result
+	rev	w9, w12                                 //CTR block 7
+
+	orr	x9, x11, x9, lsl #32            //CTR block 7
+
+	eor	v7.16b, v7.16b, v3.16b                          //AES block 3 - result
+	st1	{ v7.16b}, [x2], #16                     //AES block 3 - store result
+	b.ge	.L256_enc_prepretail                               //do prepretail
+
+.L256_enc_main_loop:	//main	loop start
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k (only t0 is free)
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	fmov	d3, x10                               //CTR block 4k+3
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	fmov	v3.d[1], x9                               //CTR block 4k+3
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	ldp	x23, x24, [x0, #48]           //AES block 4k+7 - load plaintext
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	ldp	x21, x22, [x0, #32]           //AES block 4k+6 - load plaintext
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	eor	x23, x23, x13                     //AES block 4k+7 - round 14 low
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	eor	x22, x22, x14                     //AES block 4k+6 - round 14 high
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+1 (t0 and t1 free)
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3 (t0, t1, t2 and t3 free)
+
+	pmull2	v4.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2 (t0, t1, and t2 free)
+
+	pmull	v8.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+1 - high
+	mov	d4, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	eor	v11.16b, v11.16b, v8.16b                         //GHASH block 4k+1 - low
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+	mov	d8, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	eor	v4.8b, v4.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	eor	v8.8b, v8.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	pmull	v4.1q, v4.1d, v17.1d                          //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+	ins	v8.d[1], v8.d[0]                                //GHASH block 4k+2 - mid
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+1 - mid
+
+	pmull2	v4.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+
+	pmull	v5.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+
+	pmull	v6.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+2 - high
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	ldp	x19, x20, [x0, #16]           //AES block 4k+5 - load plaintext
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+	mov	d4, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+	eor	v11.16b, v11.16b, v5.16b                         //GHASH block 4k+2 - low
+
+	pmull2	v8.1q, v8.2d, v16.2d                          //GHASH block 4k+2 - mid
+
+	pmull2	v5.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+	eor	v4.8b, v4.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	eor	x19, x19, x13                     //AES block 4k+5 - round 14 low
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 9
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	eor	x21, x21, x13                     //AES block 4k+6 - round 14 low
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 9
+	movi	v8.8b, #0xc2
+
+	pmull	v4.1q, v4.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	v9.16b, v9.16b, v5.16b                         //GHASH block 4k+3 - high
+	fmov	d5, x19                               //AES block 4k+5 - mov low
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+	ldp	x6, x7, [x0, #0]            //AES block 4k+4 - load plaintext
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 10
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+	eor	v11.16b, v11.16b, v6.16b                         //GHASH block 4k+3 - low
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 9
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 10
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 9
+	add	w12, w12, #1                            //CTR block 4k+3
+
+	aese	v0.16b, v29.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 11
+	eor	v4.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v1.16b, v29.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 11
+	add	x0, x0, #64                       //AES input_ptr update
+
+	pmull	v7.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	rev	w9, w12                                 //CTR block 4k+8
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 10
+	eor	x6, x6, x13                     //AES block 4k+4 - round 14 low
+
+	aese	v1.16b, v30.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 12
+	eor	v10.16b, v10.16b, v4.16b                         //MODULO - karatsuba tidy up
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 10
+	eor	x7, x7, x14                     //AES block 4k+4 - round 14 high
+
+	fmov	d4, x6                               //AES block 4k+4 - mov low
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+8
+	eor	v7.16b, v9.16b, v7.16b                   //MODULO - fold into mid
+
+	aese	v0.16b, v30.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 12
+	eor	x20, x20, x14                     //AES block 4k+5 - round 14 high
+
+	aese	v2.16b, v29.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 11
+	eor	x24, x24, x14                     //AES block 4k+7 - round 14 high
+
+	aese	v3.16b, v29.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 11
+	add	w12, w12, #1                            //CTR block 4k+8
+
+	aese	v0.16b, v31.16b                                     //AES block 4k+4 - round 13
+	fmov	v4.d[1], x7                           //AES block 4k+4 - mov high
+	eor	v10.16b, v10.16b, v7.16b                      //MODULO - fold into mid
+
+	aese	v2.16b, v30.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 12
+	fmov	d7, x23                               //AES block 4k+7 - mov low
+
+	aese	v1.16b, v31.16b                                     //AES block 4k+5 - round 13
+	fmov	v5.d[1], x20                           //AES block 4k+5 - mov high
+
+	fmov	d6, x21                               //AES block 4k+6 - mov low
+	cmp	x0, x5                   //.LOOP CONTROL
+
+	fmov	v6.d[1], x22                           //AES block 4k+6 - mov high
+
+	pmull	v9.1q, v10.1d, v8.1d            //MODULO - mid 64b align with low
+	eor	v4.16b, v4.16b, v0.16b                          //AES block 4k+4 - result
+	fmov	d0, x10                               //CTR block 4k+8
+
+	fmov	v0.d[1], x9                               //CTR block 4k+8
+	rev	w9, w12                                 //CTR block 4k+9
+	add	w12, w12, #1                            //CTR block 4k+9
+
+	eor	v5.16b, v5.16b, v1.16b                          //AES block 4k+5 - result
+	fmov	d1, x10                               //CTR block 4k+9
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+9
+
+	aese	v3.16b, v30.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 12
+	fmov	v1.d[1], x9                               //CTR block 4k+9
+
+	aese	v2.16b, v31.16b                                     //AES block 4k+6 - round 13
+	rev	w9, w12                                 //CTR block 4k+10
+	st1	{ v4.16b}, [x2], #16                     //AES block 4k+4 - store result
+
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+10
+	eor	v11.16b, v11.16b, v9.16b                         //MODULO - fold into low
+	fmov	v7.d[1], x24                           //AES block 4k+7 - mov high
+
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+	st1	{ v5.16b}, [x2], #16                     //AES block 4k+5 - store result
+	add	w12, w12, #1                            //CTR block 4k+10
+
+	aese	v3.16b, v31.16b                                     //AES block 4k+7 - round 13
+	eor	v6.16b, v6.16b, v2.16b                          //AES block 4k+6 - result
+	fmov	d2, x10                               //CTR block 4k+10
+
+	st1	{ v6.16b}, [x2], #16                     //AES block 4k+6 - store result
+	fmov	v2.d[1], x9                               //CTR block 4k+10
+	rev	w9, w12                                 //CTR block 4k+11
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+11
+
+	eor	v7.16b, v7.16b, v3.16b                          //AES block 4k+7 - result
+	st1	{ v7.16b}, [x2], #16                     //AES block 4k+7 - store result
+	b.lt	.L256_enc_main_loop
+
+.L256_enc_prepretail:	//PREPRETAIL
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2 (t0, t1, and t2 free)
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	fmov	d3, x10                               //CTR block 4k+3
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k (only t0 is free)
+
+	fmov	v3.d[1], x9                               //CTR block 4k+3
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+1 (t0 and t1 free)
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+
+	pmull2	v4.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+
+	pmull	v8.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+1 - high
+	mov	d4, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	eor	v11.16b, v11.16b, v8.16b                         //GHASH block 4k+1 - low
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+
+	eor	v4.8b, v4.8b, v5.8b                          //GHASH block 4k+1 - mid
+	mov	d8, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3 (t0, t1, t2 and t3 free)
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	pmull	v4.1q, v4.1d, v17.1d                          //GHASH block 4k+1 - mid
+	eor	v8.8b, v8.8b, v6.8b                          //GHASH block 4k+2 - mid
+	add	w12, w12, #1                            //CTR block 4k+3
+
+	pmull	v5.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+1 - mid
+
+	pmull2	v4.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+
+	eor	v11.16b, v11.16b, v5.16b                         //GHASH block 4k+2 - low
+	ins	v8.d[1], v8.d[0]                                //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+2 - high
+	mov	d4, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	pmull2	v8.1q, v8.2d, v16.2d                          //GHASH block 4k+2 - mid
+
+	eor	v4.8b, v4.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	pmull2	v5.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	pmull	v4.1q, v4.1d, v16.1d                          //GHASH block 4k+3 - mid
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	movi	v8.8b, #0xc2
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+	eor	v9.16b, v9.16b, v5.16b                         //GHASH block 4k+3 - high
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+3 - mid
+
+	pmull	v6.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 9
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	eor	v11.16b, v11.16b, v6.16b                         //GHASH block 4k+3 - low
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 9
+
+	eor	v10.16b, v10.16b, v9.16b                         //karatsuba tidy up
+
+	pmull	v4.1q, v9.1d, v8.1d
+	ext	v9.16b, v9.16b, v9.16b, #8
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 10
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	eor	v10.16b, v10.16b, v11.16b
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 10
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 9
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+
+	aese	v1.16b, v29.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 11
+	eor	v10.16b, v10.16b, v4.16b
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 10
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 9
+
+	aese	v1.16b, v30.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 12
+
+	aese	v0.16b, v29.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 11
+	eor	v10.16b, v10.16b, v9.16b
+
+	aese	v3.16b, v29.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 11
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 10
+
+	aese	v0.16b, v30.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 12
+
+	pmull	v4.1q, v10.1d, v8.1d
+
+	aese	v2.16b, v29.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 11
+	ext	v10.16b, v10.16b, v10.16b, #8
+
+	aese	v3.16b, v30.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 12
+
+	aese	v1.16b, v31.16b                                     //AES block 4k+5 - round 13
+	eor	v11.16b, v11.16b, v4.16b
+
+	aese	v2.16b, v30.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 12
+
+	aese	v3.16b, v31.16b                                     //AES block 4k+7 - round 13
+
+	aese	v0.16b, v31.16b                                     //AES block 4k+4 - round 13
+
+	aese	v2.16b, v31.16b                                     //AES block 4k+6 - round 13
+	eor	v11.16b, v11.16b, v10.16b
+.L256_enc_tail:	//TAIL
+
+	ext	v8.16b, v11.16b, v11.16b, #8                     //prepare final partial tag
+	sub	x5, x4, x0   //main_end_input_ptr is number of bytes left to process
+	ldp	x6, x7, [x0], #16           //AES block 4k+4 - load plaintext
+
+	eor	x6, x6, x13                     //AES block 4k+4 - round 14 low
+	eor	x7, x7, x14                     //AES block 4k+4 - round 14 high
+
+	cmp	x5, #48
+	fmov	d4, x6                               //AES block 4k+4 - mov low
+
+	fmov	v4.d[1], x7                           //AES block 4k+4 - mov high
+
+	eor	v5.16b, v4.16b, v0.16b                          //AES block 4k+4 - result
+	b.gt	.L256_enc_blocks_more_than_3
+
+	cmp	x5, #32
+	mov	v3.16b, v2.16b
+	movi	v11.8b, #0
+
+	movi	v9.8b, #0
+	sub	w12, w12, #1
+
+	mov	v2.16b, v1.16b
+	movi	v10.8b, #0
+	b.gt	.L256_enc_blocks_more_than_2
+
+	mov	v3.16b, v1.16b
+	sub	w12, w12, #1
+	cmp	x5, #16
+
+	b.gt	.L256_enc_blocks_more_than_1
+
+	sub	w12, w12, #1
+	b	.L256_enc_blocks_less_than_1
+.L256_enc_blocks_more_than_3:	//blocks	left >  3
+	st1	{ v5.16b}, [x2], #16                    //AES final-3 block  - store result
+
+	ldp	x6, x7, [x0], #16          //AES final-2 block - load input low & high
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-3 block
+
+	eor	x6, x6, x13                    //AES final-2 block - round 14 low
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	eor	x7, x7, x14                    //AES final-2 block - round 14 high
+
+	mov	d22, v4.d[1]                                //GHASH final-3 block - mid
+	fmov	d5, x6                                //AES final-2 block - mov low
+
+	fmov	v5.d[1], x7                            //AES final-2 block - mov high
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-3 block - mid
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	mov	d10, v17.d[1]                              //GHASH final-3 block - mid
+
+	pmull	v11.1q, v4.1d, v15.1d                      //GHASH final-3 block - low
+
+	pmull2	v9.1q, v4.2d, v15.2d                      //GHASH final-3 block - high
+
+	pmull	v10.1q, v22.1d, v10.1d                   //GHASH final-3 block - mid
+	eor	v5.16b, v5.16b, v1.16b                           //AES final-2 block - result
+.L256_enc_blocks_more_than_2:	//blocks	left >  2
+
+	st1	{ v5.16b}, [x2], #16                    //AES final-2 block - store result
+
+	ldp	x6, x7, [x0], #16          //AES final-1 block - load input low & high
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-2 block
+
+	eor	x6, x6, x13                    //AES final-1 block - round 14 low
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	fmov	d5, x6                                //AES final-1 block - mov low
+	eor	x7, x7, x14                    //AES final-1 block - round 14 high
+
+	fmov	v5.d[1], x7                            //AES final-1 block - mov high
+
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	pmull2	v20.1q, v4.2d, v14.2d                         //GHASH final-2 block - high
+	mov	d22, v4.d[1]                                //GHASH final-2 block - mid
+
+	pmull	v21.1q, v4.1d, v14.1d                         //GHASH final-2 block - low
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-2 block - mid
+
+	eor	v5.16b, v5.16b, v2.16b                           //AES final-1 block - result
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final-2 block - high
+
+	pmull	v22.1q, v22.1d, v17.1d                     //GHASH final-2 block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final-2 block - low
+
+	eor	v10.16b, v10.16b, v22.16b                      //GHASH final-2 block - mid
+.L256_enc_blocks_more_than_1:	//blocks	left >  1
+
+	st1	{ v5.16b}, [x2], #16                    //AES final-1 block - store result
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-1 block
+
+	ldp	x6, x7, [x0], #16          //AES final block - load input low & high
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	eor	x6, x6, x13                    //AES final block - round 14 low
+	mov	d22, v4.d[1]                                //GHASH final-1 block - mid
+
+	pmull2	v20.1q, v4.2d, v13.2d                         //GHASH final-1 block - high
+	eor	x7, x7, x14                    //AES final block - round 14 high
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-1 block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final-1 block - high
+
+	ins	v22.d[1], v22.d[0]                           //GHASH final-1 block - mid
+	fmov	d5, x6                                //AES final block - mov low
+
+	fmov	v5.d[1], x7                            //AES final block - mov high
+
+	pmull2	v22.1q, v22.2d, v16.2d                     //GHASH final-1 block - mid
+
+	pmull	v21.1q, v4.1d, v13.1d                         //GHASH final-1 block - low
+
+	eor	v5.16b, v5.16b, v3.16b                           //AES final block - result
+	eor	v10.16b, v10.16b, v22.16b                      //GHASH final-1 block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final-1 block - low
+.L256_enc_blocks_less_than_1:	//blocks	left <= 1
+
+	and	x1, x1, #127                   //bit_length %= 128
+
+	mvn	x13, xzr                                     //rk14_l = 0xffffffffffffffff
+	sub	x1, x1, #128                   //bit_length -= 128
+
+	neg	x1, x1                         //bit_length = 128 - #bits in input (in range [1,128])
+	ld1	{ v18.16b}, [x2]                           //load existing bytes where the possibly partial last block is to be stored
+
+	mvn	x14, xzr                                     //rk14_h = 0xffffffffffffffff
+	and	x1, x1, #127                   //bit_length %= 128
+
+	lsr	x14, x14, x1                    //rk14_h is mask for top 64b of last block
+	cmp	x1, #64
+
+	csel	x6, x13, x14, lt
+	csel	x7, x14, xzr, lt
+
+	fmov	d0, x6                                //ctr0b is mask for last block
+
+	fmov	v0.d[1], x7
+
+	and	v5.16b, v5.16b, v0.16b                           //possibly partial last block has zeroes in highest bits
+
+	rev64	v4.16b, v5.16b                                   //GHASH final block
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	bif	v5.16b, v18.16b, v0.16b                             //insert existing bytes in top end of result before storing
+
+	pmull2	v20.1q, v4.2d, v12.2d                         //GHASH final block - high
+	mov	d8, v4.d[1]                                 //GHASH final block - mid
+	rev	w9, w12
+
+	pmull	v21.1q, v4.1d, v12.1d                         //GHASH final block - low
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final block - high
+	eor	v8.8b, v8.8b, v4.8b                         //GHASH final block - mid
+
+	pmull	v8.1q, v8.1d, v16.1d                         //GHASH final block - mid
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final block - low
+
+	eor	v10.16b, v10.16b, v8.16b                        //GHASH final block - mid
+	movi	v8.8b, #0xc2
+
+	eor	v4.16b, v11.16b, v9.16b                        //MODULO - karatsuba tidy up
+
+	shl	d8, d8, #56              //mod_constant
+
+	eor	v10.16b, v10.16b, v4.16b                        //MODULO - karatsuba tidy up
+
+	pmull	v7.1q, v9.1d, v8.1d           //MODULO - top 64b align with mid
+
+	ext	v9.16b, v9.16b, v9.16b, #8                    //MODULO - other top alignment
+
+	eor	v10.16b, v10.16b, v7.16b                     //MODULO - fold into mid
+
+	eor	v10.16b, v10.16b, v9.16b                        //MODULO - fold into mid
+
+	pmull	v9.1q, v10.1d, v8.1d           //MODULO - mid 64b align with low
+
+	ext	v10.16b, v10.16b, v10.16b, #8                    //MODULO - other mid alignment
+
+	str	w9, [x16, #12]                         //store the updated counter
+
+	st1	{ v5.16b}, [x2]                         //store all 16B
+	eor	v11.16b, v11.16b, v9.16b                        //MODULO - fold into low
+
+	eor	v11.16b, v11.16b, v10.16b                        //MODULO - fold into low
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	mov	x0, x15
+	st1	{ v11.16b }, [x3]
+
+	ldp	x21, x22, [sp, #16]
+	ldp	x23, x24, [sp, #32]
+	ldp	d8, d9, [sp, #48]
+	ldp	d10, d11, [sp, #64]
+	ldp	d12, d13, [sp, #80]
+	ldp	d14, d15, [sp, #96]
+	ldp	x19, x20, [sp], #112
+	ret
+
+.L256_enc_ret:
+	mov	w0, #0x0
+	ret
+.size	aes_gcm_enc_256_kernel,.-aes_gcm_enc_256_kernel
+.globl	aes_gcm_dec_256_kernel
+.type	aes_gcm_dec_256_kernel,%function
+.align	4
+aes_gcm_dec_256_kernel:
+	cbz	x1, .L256_dec_ret
+	stp	x19, x20, [sp, #-112]!
+	mov	x16, x4
+	mov	x8, x5
+	stp	x21, x22, [sp, #16]
+	stp	x23, x24, [sp, #32]
+	stp	d8, d9, [sp, #48]
+	stp	d10, d11, [sp, #64]
+	stp	d12, d13, [sp, #80]
+	stp	d14, d15, [sp, #96]
+
+	lsr	x5, x1, #3              //byte_len
+	mov	x15, x5
+	ldp	x10, x11, [x16]              //ctr96_b64, ctr96_t32
+
+	ldr	q26, [x8, #128]                                //load rk8
+	sub	x5, x5, #1      //byte_len - 1
+
+	ldr	q25, [x8, #112]                                //load rk7
+	and	x5, x5, #0xffffffffffffffc0 //number of bytes to be processed in main loop (at least 1 byte must be handled by tail)
+
+	add	x4, x0, x1, lsr #3   //end_input_ptr
+	ldr	q24, [x8, #96]                                 //load rk6
+
+	lsr	x12, x11, #32
+	ldr	q23, [x8, #80]                                 //load rk5
+	orr	w11, w11, w11
+
+	ldr	q21, [x8, #48]                                 //load rk3
+	add	x5, x5, x0
+	rev	w12, w12                                //rev_ctr32
+
+	add	w12, w12, #1                            //increment rev_ctr32
+	fmov	d3, x10                               //CTR block 3
+
+	rev	w9, w12                                 //CTR block 1
+	add	w12, w12, #1                            //CTR block 1
+	fmov	d1, x10                               //CTR block 1
+
+	orr	x9, x11, x9, lsl #32            //CTR block 1
+	ld1	{ v0.16b}, [x16]                             //special case vector load initial counter so we can start first AES block as quickly as possible
+
+	fmov	v1.d[1], x9                               //CTR block 1
+	rev	w9, w12                                 //CTR block 2
+	add	w12, w12, #1                            //CTR block 2
+
+	fmov	d2, x10                               //CTR block 2
+	orr	x9, x11, x9, lsl #32            //CTR block 2
+
+	fmov	v2.d[1], x9                               //CTR block 2
+	rev	w9, w12                                 //CTR block 3
+
+	orr	x9, x11, x9, lsl #32            //CTR block 3
+	ldr	q18, [x8, #0]                                  //load rk0
+
+	fmov	v3.d[1], x9                               //CTR block 3
+	add	w12, w12, #1                            //CTR block 3
+
+	ldr	q22, [x8, #64]                                 //load rk4
+
+	ldr	q31, [x8, #208]                               //load rk13
+
+	ldr	q19, [x8, #16]                                 //load rk1
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 0
+	ldr	q14, [x3, #80]                         //load h3l | h3h
+	ext	v14.16b, v14.16b, v14.16b, #8
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 0
+	ldr	q15, [x3, #112]                        //load h4l | h4h
+	ext	v15.16b, v15.16b, v15.16b, #8
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 0
+	ldr	q13, [x3, #64]                         //load h2l | h2h
+	ext	v13.16b, v13.16b, v13.16b, #8
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 0
+	ldr	q20, [x8, #32]                                 //load rk2
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 1
+	ldp	x13, x14, [x8, #224]                     //load rk14
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 1
+	ld1	{ v11.16b}, [x3]
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 1
+	ldr	q27, [x8, #144]                                //load rk9
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 1
+	ldr	q30, [x8, #192]                               //load rk12
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 2
+	ldr	q12, [x3, #32]                         //load h1l | h1h
+	ext	v12.16b, v12.16b, v12.16b, #8
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 2
+	ldr	q28, [x8, #160]                               //load rk10
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 2
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 3
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 2
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 3
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 4
+	cmp	x0, x5                   //check if we have <= 4 blocks
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 3
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 3
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 4
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 4
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 4
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 5
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 5
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 5
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 5
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 6
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 6
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 6
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 6
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 7
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 7
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 7
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 8
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 7
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 8
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 8
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 9
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 8
+	ldr	q29, [x8, #176]                               //load rk11
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 9
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 10
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 9
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 10
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 9
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 10
+
+	aese	v0.16b, v29.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 11
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 10
+
+	aese	v3.16b, v29.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 11
+
+	aese	v1.16b, v29.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 11
+
+	aese	v2.16b, v29.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 11
+
+	trn1	v9.2d, v14.2d,    v15.2d                      //h4h | h3h
+
+	trn2	v17.2d,  v14.2d,    v15.2d                      //h4l | h3l
+
+	trn1	v8.2d,    v12.2d,    v13.2d                      //h2h | h1h
+	trn2	v16.2d,  v12.2d,    v13.2d                      //h2l | h1l
+
+	aese	v1.16b, v30.16b
+	aesmc	v1.16b, v1.16b          //AES block 1 - round 12
+
+	aese	v0.16b, v30.16b
+	aesmc	v0.16b, v0.16b          //AES block 0 - round 12
+
+	aese	v2.16b, v30.16b
+	aesmc	v2.16b, v2.16b          //AES block 2 - round 12
+
+	aese	v3.16b, v30.16b
+	aesmc	v3.16b, v3.16b          //AES block 3 - round 12
+	eor	v17.16b, v17.16b, v9.16b                  //h4k | h3k
+
+	aese	v1.16b, v31.16b                                     //AES block 1 - round 13
+
+	aese	v2.16b, v31.16b                                     //AES block 2 - round 13
+	eor	v16.16b, v16.16b, v8.16b                     //h2k | h1k
+
+	aese	v3.16b, v31.16b                                     //AES block 3 - round 13
+
+	aese	v0.16b, v31.16b                                     //AES block 0 - round 13
+	b.ge	.L256_dec_tail                                    //handle tail
+
+	ldr	q4, [x0, #0]                          //AES block 0 - load ciphertext
+
+	ldr	q5, [x0, #16]                         //AES block 1 - load ciphertext
+
+	rev	w9, w12                                 //CTR block 4
+
+	eor	v0.16b, v4.16b, v0.16b                            //AES block 0 - result
+
+	eor	v1.16b, v5.16b, v1.16b                            //AES block 1 - result
+	rev64	v5.16b, v5.16b                                    //GHASH block 1
+	ldr	q7, [x0, #48]                         //AES block 3 - load ciphertext
+
+	mov	x7, v0.d[1]                            //AES block 0 - mov high
+
+	mov	x6, v0.d[0]                            //AES block 0 - mov low
+	rev64	v4.16b, v4.16b                                    //GHASH block 0
+	add	w12, w12, #1                            //CTR block 4
+
+	fmov	d0, x10                               //CTR block 4
+	orr	x9, x11, x9, lsl #32            //CTR block 4
+
+	fmov	v0.d[1], x9                               //CTR block 4
+	rev	w9, w12                                 //CTR block 5
+	add	w12, w12, #1                            //CTR block 5
+
+	mov	x19, v1.d[0]                            //AES block 1 - mov low
+
+	orr	x9, x11, x9, lsl #32            //CTR block 5
+	mov	x20, v1.d[1]                            //AES block 1 - mov high
+	eor	x7, x7, x14                   //AES block 0 - round 14 high
+
+	eor	x6, x6, x13                   //AES block 0 - round 14 low
+	stp	x6, x7, [x2], #16        //AES block 0 - store result
+	fmov	d1, x10                               //CTR block 5
+
+	ldr	q6, [x0, #32]                         //AES block 2 - load ciphertext
+	add	x0, x0, #64                       //AES input_ptr update
+
+	fmov	v1.d[1], x9                               //CTR block 5
+	rev	w9, w12                                 //CTR block 6
+	add	w12, w12, #1                            //CTR block 6
+
+	eor	x19, x19, x13                   //AES block 1 - round 14 low
+	orr	x9, x11, x9, lsl #32            //CTR block 6
+
+	eor	x20, x20, x14                   //AES block 1 - round 14 high
+	stp	x19, x20, [x2], #16        //AES block 1 - store result
+
+	eor	v2.16b, v6.16b, v2.16b                            //AES block 2 - result
+	cmp	x0, x5                   //check if we have <= 8 blocks
+	b.ge	.L256_dec_prepretail                              //do prepretail
+
+.L256_dec_main_loop:	//main	loop start
+	mov	x21, v2.d[0]                            //AES block 4k+2 - mov low
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	eor	v3.16b, v7.16b, v3.16b                            //AES block 4k+3 - result
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	mov	x22, v2.d[1]                            //AES block 4k+2 - mov high
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	fmov	d2, x10                               //CTR block 4k+6
+
+	fmov	v2.d[1], x9                               //CTR block 4k+6
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+	rev	w9, w12                                 //CTR block 4k+7
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	mov	x24, v3.d[1]                            //AES block 4k+3 - mov high
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	mov	x23, v3.d[0]                            //AES block 4k+3 - mov low
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+	fmov	d3, x10                               //CTR block 4k+7
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+7
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	fmov	v3.d[1], x9                               //CTR block 4k+7
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	eor	x22, x22, x14                   //AES block 4k+2 - round 14 high
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+	eor	x21, x21, x13                   //AES block 4k+2 - round 14 low
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+	stp	x21, x22, [x2], #16        //AES block 4k+2 - store result
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+
+	pmull2	v4.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+	eor	x23, x23, x13                   //AES block 4k+3 - round 14 low
+
+	pmull	v8.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+	eor	x24, x24, x14                   //AES block 4k+3 - round 14 high
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+1 - high
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	mov	d4, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+	eor	v11.16b, v11.16b, v8.16b                         //GHASH block 4k+1 - low
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+	add	w12, w12, #1                            //CTR block 4k+7
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	mov	d8, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+	eor	v4.8b, v4.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	pmull	v5.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	eor	v8.8b, v8.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	eor	v11.16b, v11.16b, v5.16b                         //GHASH block 4k+2 - low
+
+	pmull	v4.1q, v4.1d, v17.1d                          //GHASH block 4k+1 - mid
+	rev	w9, w12                                 //CTR block 4k+8
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+	ins	v8.d[1], v8.d[0]                                //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	add	w12, w12, #1                            //CTR block 4k+8
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+
+	pmull2	v4.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+	mov	d6, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+
+	pmull2	v8.1q, v8.2d, v16.2d                          //GHASH block 4k+2 - mid
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+2 - high
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+
+	pmull	v4.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+8
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH block 4k+2 - mid
+
+	pmull2	v5.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 9
+	eor	v6.8b, v6.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+	eor	v9.16b, v9.16b, v5.16b                         //GHASH block 4k+3 - high
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 10
+
+	pmull	v6.1q, v6.1d, v16.1d                          //GHASH block 4k+3 - mid
+	movi	v8.8b, #0xc2
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	eor	v11.16b, v11.16b, v4.16b                         //GHASH block 4k+3 - low
+
+	aese	v0.16b, v29.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 11
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+	eor	v10.16b, v10.16b, v6.16b                         //GHASH block 4k+3 - mid
+
+	aese	v0.16b, v30.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 12
+
+	pmull	v7.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+	eor	v6.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 9
+	ldr	q4, [x0, #0]                          //AES block 4k+4 - load ciphertext
+
+	aese	v0.16b, v31.16b                                     //AES block 4k+4 - round 13
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 10
+	eor	v10.16b, v10.16b, v6.16b                         //MODULO - karatsuba tidy up
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 9
+	ldr	q5, [x0, #16]                         //AES block 4k+5 - load ciphertext
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+	eor	v0.16b, v4.16b, v0.16b                            //AES block 4k+4 - result
+
+	aese	v1.16b, v29.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 11
+	stp	x23, x24, [x2], #16        //AES block 4k+3 - store result
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 10
+	eor	v10.16b, v10.16b, v7.16b                      //MODULO - fold into mid
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 9
+	ldr	q7, [x0, #48]                         //AES block 4k+7 - load ciphertext
+
+	aese	v1.16b, v30.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 12
+	ldr	q6, [x0, #32]                         //AES block 4k+6 - load ciphertext
+
+	aese	v2.16b, v29.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 11
+	mov	x7, v0.d[1]                            //AES block 4k+4 - mov high
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 10
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	aese	v1.16b, v31.16b                                     //AES block 4k+5 - round 13
+	add	x0, x0, #64                       //AES input_ptr update
+	mov	x6, v0.d[0]                            //AES block 4k+4 - mov low
+
+	aese	v2.16b, v30.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 12
+	fmov	d0, x10                               //CTR block 4k+8
+
+	aese	v3.16b, v29.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 11
+	fmov	v0.d[1], x9                               //CTR block 4k+8
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+	eor	v1.16b, v5.16b, v1.16b                            //AES block 4k+5 - result
+	rev	w9, w12                                 //CTR block 4k+9
+
+	aese	v2.16b, v31.16b                                     //AES block 4k+6 - round 13
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+9
+	cmp	x0, x5                   //.LOOP CONTROL
+
+	add	w12, w12, #1                            //CTR block 4k+9
+
+	eor	x6, x6, x13                   //AES block 4k+4 - round 14 low
+	eor	x7, x7, x14                   //AES block 4k+4 - round 14 high
+
+	mov	x20, v1.d[1]                            //AES block 4k+5 - mov high
+	eor	v2.16b, v6.16b, v2.16b                            //AES block 4k+6 - result
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	aese	v3.16b, v30.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 12
+	mov	x19, v1.d[0]                            //AES block 4k+5 - mov low
+
+	fmov	d1, x10                               //CTR block 4k+9
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	fmov	v1.d[1], x9                               //CTR block 4k+9
+	rev	w9, w12                                 //CTR block 4k+10
+	add	w12, w12, #1                            //CTR block 4k+10
+
+	aese	v3.16b, v31.16b                                     //AES block 4k+7 - round 13
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+10
+
+	rev64	v5.16b, v5.16b                                    //GHASH block 4k+5
+	eor	x20, x20, x14                   //AES block 4k+5 - round 14 high
+	stp	x6, x7, [x2], #16        //AES block 4k+4 - store result
+
+	eor	x19, x19, x13                   //AES block 4k+5 - round 14 low
+	stp	x19, x20, [x2], #16        //AES block 4k+5 - store result
+
+	rev64	v4.16b, v4.16b                                    //GHASH block 4k+4
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	b.lt	.L256_dec_main_loop
+
+
+.L256_dec_prepretail:	//PREPRETAIL
+	ext	v11.16b, v11.16b, v11.16b, #8                     //PRE 0
+	mov	x21, v2.d[0]                            //AES block 4k+2 - mov low
+	eor	v3.16b, v7.16b, v3.16b                            //AES block 4k+3 - result
+
+	aese	v0.16b, v18.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 0
+	mov	x22, v2.d[1]                            //AES block 4k+2 - mov high
+
+	aese	v1.16b, v18.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 0
+	fmov	d2, x10                               //CTR block 4k+6
+
+	fmov	v2.d[1], x9                               //CTR block 4k+6
+	rev	w9, w12                                 //CTR block 4k+7
+	eor	v4.16b, v4.16b, v11.16b                           //PRE 1
+
+	rev64	v6.16b, v6.16b                                    //GHASH block 4k+2
+	orr	x9, x11, x9, lsl #32            //CTR block 4k+7
+	mov	x23, v3.d[0]                            //AES block 4k+3 - mov low
+
+	aese	v1.16b, v19.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 1
+	mov	x24, v3.d[1]                            //AES block 4k+3 - mov high
+
+	pmull	v11.1q, v4.1d, v15.1d                       //GHASH block 4k - low
+	mov	d8, v4.d[1]                                  //GHASH block 4k - mid
+	fmov	d3, x10                               //CTR block 4k+7
+
+	pmull2	v9.1q, v4.2d, v15.2d                       //GHASH block 4k - high
+	fmov	v3.d[1], x9                               //CTR block 4k+7
+
+	aese	v2.16b, v18.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 0
+	mov	d10, v17.d[1]                               //GHASH block 4k - mid
+
+	aese	v0.16b, v19.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 1
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH block 4k - mid
+
+	pmull2	v4.1q, v5.2d, v14.2d                          //GHASH block 4k+1 - high
+
+	aese	v2.16b, v19.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 1
+	rev64	v7.16b, v7.16b                                    //GHASH block 4k+3
+
+	aese	v3.16b, v18.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 0
+
+	pmull	v10.1q, v8.1d, v10.1d                      //GHASH block 4k - mid
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+1 - high
+
+	pmull	v8.1q, v5.1d, v14.1d                          //GHASH block 4k+1 - low
+
+	aese	v3.16b, v19.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 1
+	mov	d4, v5.d[1]                                  //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v20.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 2
+
+	aese	v1.16b, v20.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 2
+	eor	v11.16b, v11.16b, v8.16b                         //GHASH block 4k+1 - low
+
+	aese	v2.16b, v20.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 2
+
+	aese	v0.16b, v21.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 3
+	mov	d8, v6.d[1]                                  //GHASH block 4k+2 - mid
+
+	aese	v3.16b, v20.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 2
+	eor	v4.8b, v4.8b, v5.8b                          //GHASH block 4k+1 - mid
+
+	pmull	v5.1q, v6.1d, v13.1d                          //GHASH block 4k+2 - low
+
+	aese	v0.16b, v22.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 4
+
+	aese	v3.16b, v21.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 3
+	eor	v8.8b, v8.8b, v6.8b                          //GHASH block 4k+2 - mid
+
+	pmull	v4.1q, v4.1d, v17.1d                          //GHASH block 4k+1 - mid
+
+	aese	v0.16b, v23.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 5
+	eor	v11.16b, v11.16b, v5.16b                         //GHASH block 4k+2 - low
+
+	aese	v3.16b, v22.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 4
+
+	pmull2	v5.1q, v7.2d, v12.2d                          //GHASH block 4k+3 - high
+	eor	v10.16b, v10.16b, v4.16b                         //GHASH block 4k+1 - mid
+
+	pmull2	v4.1q, v6.2d, v13.2d                          //GHASH block 4k+2 - high
+
+	aese	v3.16b, v23.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 5
+	ins	v8.d[1], v8.d[0]                                //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v21.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 3
+
+	aese	v1.16b, v21.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 3
+	eor	v9.16b, v9.16b, v4.16b                         //GHASH block 4k+2 - high
+
+	pmull	v4.1q, v7.1d, v12.1d                          //GHASH block 4k+3 - low
+
+	aese	v2.16b, v22.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 4
+	mov	d6, v7.d[1]                                  //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v22.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 4
+
+	pmull2	v8.1q, v8.2d, v16.2d                          //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v23.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 5
+	eor	v6.8b, v6.8b, v7.8b                          //GHASH block 4k+3 - mid
+
+	aese	v1.16b, v23.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 5
+
+	aese	v3.16b, v24.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 6
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH block 4k+2 - mid
+
+	aese	v2.16b, v24.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 6
+
+	aese	v0.16b, v24.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 6
+	movi	v8.8b, #0xc2
+
+	aese	v1.16b, v24.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 6
+	eor	v11.16b, v11.16b, v4.16b                         //GHASH block 4k+3 - low
+
+	pmull	v6.1q, v6.1d, v16.1d                          //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v25.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 7
+	eor	v9.16b, v9.16b, v5.16b                         //GHASH block 4k+3 - high
+
+	aese	v1.16b, v25.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 7
+
+	aese	v0.16b, v25.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 7
+	eor	v10.16b, v10.16b, v6.16b                         //GHASH block 4k+3 - mid
+
+	aese	v3.16b, v26.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 8
+
+	aese	v2.16b, v25.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 7
+	eor	v6.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	aese	v1.16b, v26.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 8
+
+	aese	v0.16b, v26.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 8
+	shl	d8, d8, #56               //mod_constant
+
+	aese	v2.16b, v26.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 8
+
+	aese	v1.16b, v27.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 9
+	eor	v10.16b, v10.16b, v6.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v7.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+
+	aese	v2.16b, v27.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 9
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	aese	v3.16b, v27.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 9
+
+	aese	v0.16b, v27.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 9
+	eor	v10.16b, v10.16b, v7.16b                      //MODULO - fold into mid
+
+	aese	v2.16b, v28.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 10
+
+	aese	v3.16b, v28.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 10
+
+	aese	v0.16b, v28.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 10
+	eor	x22, x22, x14                   //AES block 4k+2 - round 14 high
+
+	aese	v1.16b, v28.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 10
+	eor	x23, x23, x13                   //AES block 4k+3 - round 14 low
+
+	aese	v2.16b, v29.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 11
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	aese	v0.16b, v29.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 11
+	add	w12, w12, #1                            //CTR block 4k+7
+
+	aese	v1.16b, v29.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 11
+	eor	x21, x21, x13                   //AES block 4k+2 - round 14 low
+
+	aese	v2.16b, v30.16b
+	aesmc	v2.16b, v2.16b          //AES block 4k+6 - round 12
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+	eor	x24, x24, x14                   //AES block 4k+3 - round 14 high
+
+	aese	v3.16b, v29.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 11
+	stp	x21, x22, [x2], #16        //AES block 4k+2 - store result
+
+	aese	v1.16b, v30.16b
+	aesmc	v1.16b, v1.16b          //AES block 4k+5 - round 12
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	aese	v0.16b, v30.16b
+	aesmc	v0.16b, v0.16b          //AES block 4k+4 - round 12
+	stp	x23, x24, [x2], #16        //AES block 4k+3 - store result
+
+	aese	v3.16b, v30.16b
+	aesmc	v3.16b, v3.16b          //AES block 4k+7 - round 12
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	aese	v1.16b, v31.16b                                     //AES block 4k+5 - round 13
+
+	aese	v0.16b, v31.16b                                     //AES block 4k+4 - round 13
+
+	aese	v3.16b, v31.16b                                     //AES block 4k+7 - round 13
+
+	aese	v2.16b, v31.16b                                     //AES block 4k+6 - round 13
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+.L256_dec_tail:	//TAIL
+
+	sub	x5, x4, x0   //main_end_input_ptr is number of bytes left to process
+	ld1	{ v5.16b}, [x0], #16                      //AES block 4k+4 - load ciphertext
+
+	eor	v0.16b, v5.16b, v0.16b                            //AES block 4k+4 - result
+
+	mov	x6, v0.d[0]                            //AES block 4k+4 - mov low
+
+	mov	x7, v0.d[1]                            //AES block 4k+4 - mov high
+	ext	v8.16b, v11.16b, v11.16b, #8                     //prepare final partial tag
+
+	cmp	x5, #48
+
+	eor	x6, x6, x13                   //AES block 4k+4 - round 14 low
+
+	eor	x7, x7, x14                   //AES block 4k+4 - round 14 high
+	b.gt	.L256_dec_blocks_more_than_3
+
+	sub	w12, w12, #1
+	mov	v3.16b, v2.16b
+	movi	v10.8b, #0
+
+	movi	v11.8b, #0
+	cmp	x5, #32
+
+	movi	v9.8b, #0
+	mov	v2.16b, v1.16b
+	b.gt	.L256_dec_blocks_more_than_2
+
+	sub	w12, w12, #1
+
+	mov	v3.16b, v1.16b
+	cmp	x5, #16
+	b.gt	.L256_dec_blocks_more_than_1
+
+	sub	w12, w12, #1
+	b	.L256_dec_blocks_less_than_1
+.L256_dec_blocks_more_than_3:	//blocks	left >  3
+	rev64	v4.16b, v5.16b                                   //GHASH final-3 block
+	ld1	{ v5.16b}, [x0], #16                     //AES final-2 block - load ciphertext
+
+	stp	x6, x7, [x2], #16       //AES final-3 block  - store result
+
+	mov	d10, v17.d[1]                              //GHASH final-3 block - mid
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+
+	eor	v0.16b, v5.16b, v1.16b                           //AES final-2 block - result
+
+	mov	d22, v4.d[1]                                //GHASH final-3 block - mid
+
+	mov	x6, v0.d[0]                           //AES final-2 block - mov low
+
+	mov	x7, v0.d[1]                           //AES final-2 block - mov high
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-3 block - mid
+
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	pmull2	v9.1q, v4.2d, v15.2d                      //GHASH final-3 block - high
+
+	pmull	v10.1q, v22.1d, v10.1d                   //GHASH final-3 block - mid
+	eor	x6, x6, x13                  //AES final-2 block - round 14 low
+
+	pmull	v11.1q, v4.1d, v15.1d                      //GHASH final-3 block - low
+	eor	x7, x7, x14                  //AES final-2 block - round 14 high
+.L256_dec_blocks_more_than_2:	//blocks	left >  2
+
+	rev64	v4.16b, v5.16b                                   //GHASH final-2 block
+	ld1	{ v5.16b}, [x0], #16                     //AES final-1 block - load ciphertext
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+	stp	x6, x7, [x2], #16       //AES final-2 block  - store result
+
+	eor	v0.16b, v5.16b, v2.16b                           //AES final-1 block - result
+
+	mov	d22, v4.d[1]                                //GHASH final-2 block - mid
+
+	pmull	v21.1q, v4.1d, v14.1d                         //GHASH final-2 block - low
+
+	pmull2	v20.1q, v4.2d, v14.2d                         //GHASH final-2 block - high
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-2 block - mid
+	mov	x6, v0.d[0]                           //AES final-1 block - mov low
+
+	mov	x7, v0.d[1]                           //AES final-1 block - mov high
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final-2 block - low
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	pmull	v22.1q, v22.1d, v17.1d                     //GHASH final-2 block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final-2 block - high
+	eor	x6, x6, x13                  //AES final-1 block - round 14 low
+
+	eor	v10.16b, v10.16b, v22.16b                      //GHASH final-2 block - mid
+	eor	x7, x7, x14                  //AES final-1 block - round 14 high
+.L256_dec_blocks_more_than_1:	//blocks	left >  1
+
+	stp	x6, x7, [x2], #16       //AES final-1 block  - store result
+	rev64	v4.16b, v5.16b                                   //GHASH final-1 block
+
+	ld1	{ v5.16b}, [x0], #16                     //AES final block - load ciphertext
+
+	eor	v4.16b, v4.16b, v8.16b                          //feed in partial tag
+	movi	v8.8b, #0                                       //suppress further partial tag feed in
+
+	mov	d22, v4.d[1]                                //GHASH final-1 block - mid
+
+	eor	v0.16b, v5.16b, v3.16b                           //AES final block - result
+
+	pmull2	v20.1q, v4.2d, v13.2d                         //GHASH final-1 block - high
+
+	eor	v22.8b, v22.8b, v4.8b                     //GHASH final-1 block - mid
+
+	pmull	v21.1q, v4.1d, v13.1d                         //GHASH final-1 block - low
+	mov	x6, v0.d[0]                           //AES final block - mov low
+
+	ins	v22.d[1], v22.d[0]                           //GHASH final-1 block - mid
+
+	mov	x7, v0.d[1]                           //AES final block - mov high
+
+	pmull2	v22.1q, v22.2d, v16.2d                     //GHASH final-1 block - mid
+	eor	x6, x6, x13                  //AES final block - round 14 low
+
+	eor	v11.16b, v11.16b, v21.16b                           //GHASH final-1 block - low
+
+	eor	v9.16b, v9.16b, v20.16b                           //GHASH final-1 block - high
+
+	eor	v10.16b, v10.16b, v22.16b                      //GHASH final-1 block - mid
+	eor	x7, x7, x14                  //AES final block - round 14 high
+.L256_dec_blocks_less_than_1:	//blocks	left <= 1
+
+	and	x1, x1, #127                   //bit_length %= 128
+	mvn	x14, xzr                                     //rk14_h = 0xffffffffffffffff
+
+	sub	x1, x1, #128                   //bit_length -= 128
+	mvn	x13, xzr                                     //rk14_l = 0xffffffffffffffff
+
+	ldp	x4, x5, [x2] //load existing bytes we need to not overwrite
+	neg	x1, x1                         //bit_length = 128 - #bits in input (in range [1,128])
+
+	and	x1, x1, #127                   //bit_length %= 128
+
+	lsr	x14, x14, x1                    //rk14_h is mask for top 64b of last block
+	cmp	x1, #64
+
+	csel	x9, x13, x14, lt
+	csel	x10, x14, xzr, lt
+
+	fmov	d0, x9                                  //ctr0b is mask for last block
+	and	x6, x6, x9
+
+	mov	v0.d[1], x10
+	bic	x4, x4, x9          //mask out low existing bytes
+
+	rev	w9, w12
+
+	bic	x5, x5, x10      //mask out high existing bytes
+
+	orr	x6, x6, x4
+
+	and	x7, x7, x10
+
+	orr	x7, x7, x5
+
+	and	v5.16b, v5.16b, v0.16b                            //possibly partial last block has zeroes in highest bits
+
+	rev64	v4.16b, v5.16b                                    //GHASH final block
+
+	eor	v4.16b, v4.16b, v8.16b                           //feed in partial tag
+
+	pmull	v21.1q, v4.1d, v12.1d                          //GHASH final block - low
+
+	mov	d8, v4.d[1]                                  //GHASH final block - mid
+
+	eor	v8.8b, v8.8b, v4.8b                          //GHASH final block - mid
+
+	pmull2	v20.1q, v4.2d, v12.2d                          //GHASH final block - high
+
+	pmull	v8.1q, v8.1d, v16.1d                          //GHASH final block - mid
+
+	eor	v9.16b, v9.16b, v20.16b                            //GHASH final block - high
+
+	eor	v11.16b, v11.16b, v21.16b                            //GHASH final block - low
+
+	eor	v10.16b, v10.16b, v8.16b                         //GHASH final block - mid
+	movi	v8.8b, #0xc2
+
+	eor	v6.16b, v11.16b, v9.16b                         //MODULO - karatsuba tidy up
+
+	shl	d8, d8, #56               //mod_constant
+
+	eor	v10.16b, v10.16b, v6.16b                         //MODULO - karatsuba tidy up
+
+	pmull	v7.1q, v9.1d, v8.1d            //MODULO - top 64b align with mid
+
+	ext	v9.16b, v9.16b, v9.16b, #8                     //MODULO - other top alignment
+
+	eor	v10.16b, v10.16b, v7.16b                      //MODULO - fold into mid
+
+	eor	v10.16b, v10.16b, v9.16b                         //MODULO - fold into mid
+
+	pmull	v8.1q, v10.1d, v8.1d     //MODULO - mid 64b align with low
+
+	ext	v10.16b, v10.16b, v10.16b, #8                     //MODULO - other mid alignment
+
+	eor	v11.16b, v11.16b, v8.16b               //MODULO - fold into low
+
+	stp	x6, x7, [x2]
+
+	str	w9, [x16, #12]                          //store the updated counter
+
+	eor	v11.16b, v11.16b, v10.16b                         //MODULO - fold into low
+	ext	v11.16b, v11.16b, v11.16b, #8
+	rev64	v11.16b, v11.16b
+	mov	x0, x15
+	st1	{ v11.16b }, [x3]
+
+	ldp	x21, x22, [sp, #16]
+	ldp	x23, x24, [sp, #32]
+	ldp	d8, d9, [sp, #48]
+	ldp	d10, d11, [sp, #64]
+	ldp	d12, d13, [sp, #80]
+	ldp	d14, d15, [sp, #96]
+	ldp	x19, x20, [sp], #112
+	ret
+
+.L256_dec_ret:
+	mov	w0, #0x0
+	ret
+.size	aes_gcm_dec_256_kernel,.-aes_gcm_dec_256_kernel
+.byte	71,72,65,83,72,32,102,111,114,32,65,82,77,118,56,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+.align	2
+.align	2
+#endif
diff --git a/os_stub/openssllib/openssl_asm/aesv8-armx.S b/os_stub/openssllib/openssl_asm/aesv8-armx.S
new file mode 100644
index 000000000..cd9cac97c
--- /dev/null
+++ b/os_stub/openssllib/openssl_asm/aesv8-armx.S
@@ -0,0 +1,3178 @@
+#include "arm_arch.h"
+
+#if __ARM_MAX_ARCH__>=7
+.arch	armv8-a+crypto
+.text
+.align	5
+.Lrcon:
+.long	0x01,0x01,0x01,0x01
+.long	0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d	// rotate-n-splat
+.long	0x1b,0x1b,0x1b,0x1b
+
+.globl	aes_v8_set_encrypt_key
+.type	aes_v8_set_encrypt_key,%function
+.align	5
+aes_v8_set_encrypt_key:
+.Lenc_key:
+	stp	x29,x30,[sp,#-16]!
+	add	x29,sp,#0
+	mov	x3,#-1
+	cmp	x0,#0
+	b.eq	.Lenc_key_abort
+	cmp	x2,#0
+	b.eq	.Lenc_key_abort
+	mov	x3,#-2
+	cmp	w1,#128
+	b.lt	.Lenc_key_abort
+	cmp	w1,#256
+	b.gt	.Lenc_key_abort
+	tst	w1,#0x3f
+	b.ne	.Lenc_key_abort
+
+	adr	x3,.Lrcon
+	cmp	w1,#192
+
+	eor	v0.16b,v0.16b,v0.16b
+	ld1	{v3.16b},[x0],#16
+	mov	w1,#8		// reuse w1
+	ld1	{v1.4s,v2.4s},[x3],#32
+
+	b.lt	.Loop128
+	b.eq	.L192
+	b	.L256
+
+.align	4
+.Loop128:
+	tbl	v6.16b,{v3.16b},v2.16b
+	ext	v5.16b,v0.16b,v3.16b,#12
+	st1	{v3.4s},[x2],#16
+	aese	v6.16b,v0.16b
+	subs	w1,w1,#1
+
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v6.16b,v6.16b,v1.16b
+	eor	v3.16b,v3.16b,v5.16b
+	shl	v1.16b,v1.16b,#1
+	eor	v3.16b,v3.16b,v6.16b
+	b.ne	.Loop128
+
+	ld1	{v1.4s},[x3]
+
+	tbl	v6.16b,{v3.16b},v2.16b
+	ext	v5.16b,v0.16b,v3.16b,#12
+	st1	{v3.4s},[x2],#16
+	aese	v6.16b,v0.16b
+
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v6.16b,v6.16b,v1.16b
+	eor	v3.16b,v3.16b,v5.16b
+	shl	v1.16b,v1.16b,#1
+	eor	v3.16b,v3.16b,v6.16b
+
+	tbl	v6.16b,{v3.16b},v2.16b
+	ext	v5.16b,v0.16b,v3.16b,#12
+	st1	{v3.4s},[x2],#16
+	aese	v6.16b,v0.16b
+
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v6.16b,v6.16b,v1.16b
+	eor	v3.16b,v3.16b,v5.16b
+	eor	v3.16b,v3.16b,v6.16b
+	st1	{v3.4s},[x2]
+	add	x2,x2,#0x50
+
+	mov	w12,#10
+	b	.Ldone
+
+.align	4
+.L192:
+	ld1	{v4.8b},[x0],#8
+	movi	v6.16b,#8			// borrow v6.16b
+	st1	{v3.4s},[x2],#16
+	sub	v2.16b,v2.16b,v6.16b	// adjust the mask
+
+.Loop192:
+	tbl	v6.16b,{v4.16b},v2.16b
+	ext	v5.16b,v0.16b,v3.16b,#12
+#ifdef __ARMEB__
+	st1	{v4.4s},[x2],#16
+	sub	x2,x2,#8
+#else
+	st1	{v4.8b},[x2],#8
+#endif
+	aese	v6.16b,v0.16b
+	subs	w1,w1,#1
+
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v3.16b,v3.16b,v5.16b
+
+	dup	v5.4s,v3.s[3]
+	eor	v5.16b,v5.16b,v4.16b
+	eor	v6.16b,v6.16b,v1.16b
+	ext	v4.16b,v0.16b,v4.16b,#12
+	shl	v1.16b,v1.16b,#1
+	eor	v4.16b,v4.16b,v5.16b
+	eor	v3.16b,v3.16b,v6.16b
+	eor	v4.16b,v4.16b,v6.16b
+	st1	{v3.4s},[x2],#16
+	b.ne	.Loop192
+
+	mov	w12,#12
+	add	x2,x2,#0x20
+	b	.Ldone
+
+.align	4
+.L256:
+	ld1	{v4.16b},[x0]
+	mov	w1,#7
+	mov	w12,#14
+	st1	{v3.4s},[x2],#16
+
+.Loop256:
+	tbl	v6.16b,{v4.16b},v2.16b
+	ext	v5.16b,v0.16b,v3.16b,#12
+	st1	{v4.4s},[x2],#16
+	aese	v6.16b,v0.16b
+	subs	w1,w1,#1
+
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v3.16b,v3.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v6.16b,v6.16b,v1.16b
+	eor	v3.16b,v3.16b,v5.16b
+	shl	v1.16b,v1.16b,#1
+	eor	v3.16b,v3.16b,v6.16b
+	st1	{v3.4s},[x2],#16
+	b.eq	.Ldone
+
+	dup	v6.4s,v3.s[3]		// just splat
+	ext	v5.16b,v0.16b,v4.16b,#12
+	aese	v6.16b,v0.16b
+
+	eor	v4.16b,v4.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v4.16b,v4.16b,v5.16b
+	ext	v5.16b,v0.16b,v5.16b,#12
+	eor	v4.16b,v4.16b,v5.16b
+
+	eor	v4.16b,v4.16b,v6.16b
+	b	.Loop256
+
+.Ldone:
+	str	w12,[x2]
+	mov	x3,#0
+
+.Lenc_key_abort:
+	mov	x0,x3			// return value
+	ldr	x29,[sp],#16
+	ret
+.size	aes_v8_set_encrypt_key,.-aes_v8_set_encrypt_key
+
+.globl	aes_v8_set_decrypt_key
+.type	aes_v8_set_decrypt_key,%function
+.align	5
+aes_v8_set_decrypt_key:
+.inst	0xd503233f		// paciasp
+	stp	x29,x30,[sp,#-16]!
+	add	x29,sp,#0
+	bl	.Lenc_key
+
+	cmp	x0,#0
+	b.ne	.Ldec_key_abort
+
+	sub	x2,x2,#240		// restore original x2
+	mov	x4,#-16
+	add	x0,x2,x12,lsl#4	// end of key schedule
+
+	ld1	{v0.4s},[x2]
+	ld1	{v1.4s},[x0]
+	st1	{v0.4s},[x0],x4
+	st1	{v1.4s},[x2],#16
+
+.Loop_imc:
+	ld1	{v0.4s},[x2]
+	ld1	{v1.4s},[x0]
+	aesimc	v0.16b,v0.16b
+	aesimc	v1.16b,v1.16b
+	st1	{v0.4s},[x0],x4
+	st1	{v1.4s},[x2],#16
+	cmp	x0,x2
+	b.hi	.Loop_imc
+
+	ld1	{v0.4s},[x2]
+	aesimc	v0.16b,v0.16b
+	st1	{v0.4s},[x0]
+
+	eor	x0,x0,x0		// return value
+.Ldec_key_abort:
+	ldp	x29,x30,[sp],#16
+.inst	0xd50323bf		// autiasp
+	ret
+.size	aes_v8_set_decrypt_key,.-aes_v8_set_decrypt_key
+.globl	aes_v8_encrypt
+.type	aes_v8_encrypt,%function
+.align	5
+aes_v8_encrypt:
+	ldr	w3,[x2,#240]
+	ld1	{v0.4s},[x2],#16
+	ld1	{v2.16b},[x0]
+	sub	w3,w3,#2
+	ld1	{v1.4s},[x2],#16
+
+.Loop_enc:
+	aese	v2.16b,v0.16b
+	aesmc	v2.16b,v2.16b
+	ld1	{v0.4s},[x2],#16
+	subs	w3,w3,#2
+	aese	v2.16b,v1.16b
+	aesmc	v2.16b,v2.16b
+	ld1	{v1.4s},[x2],#16
+	b.gt	.Loop_enc
+
+	aese	v2.16b,v0.16b
+	aesmc	v2.16b,v2.16b
+	ld1	{v0.4s},[x2]
+	aese	v2.16b,v1.16b
+	eor	v2.16b,v2.16b,v0.16b
+
+	st1	{v2.16b},[x1]
+	ret
+.size	aes_v8_encrypt,.-aes_v8_encrypt
+.globl	aes_v8_decrypt
+.type	aes_v8_decrypt,%function
+.align	5
+aes_v8_decrypt:
+	ldr	w3,[x2,#240]
+	ld1	{v0.4s},[x2],#16
+	ld1	{v2.16b},[x0]
+	sub	w3,w3,#2
+	ld1	{v1.4s},[x2],#16
+
+.Loop_dec:
+	aesd	v2.16b,v0.16b
+	aesimc	v2.16b,v2.16b
+	ld1	{v0.4s},[x2],#16
+	subs	w3,w3,#2
+	aesd	v2.16b,v1.16b
+	aesimc	v2.16b,v2.16b
+	ld1	{v1.4s},[x2],#16
+	b.gt	.Loop_dec
+
+	aesd	v2.16b,v0.16b
+	aesimc	v2.16b,v2.16b
+	ld1	{v0.4s},[x2]
+	aesd	v2.16b,v1.16b
+	eor	v2.16b,v2.16b,v0.16b
+
+	st1	{v2.16b},[x1]
+	ret
+.size	aes_v8_decrypt,.-aes_v8_decrypt
+.globl	aes_v8_ecb_encrypt
+.type	aes_v8_ecb_encrypt,%function
+.align	5
+aes_v8_ecb_encrypt:
+	subs	x2,x2,#16
+	// Original input data size bigger than 16, jump to big size processing.
+	b.ne	.Lecb_big_size
+	ld1	{v0.16b},[x0]
+	cmp	w4,#0					// en- or decrypting?
+	ldr	w5,[x3,#240]
+	ld1	{v5.4s,v6.4s},[x3],#32			// load key schedule...
+
+	b.eq	.Lecb_small_dec
+	aese	v0.16b,v5.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.4s,v17.4s},[x3],#32			// load key schedule...
+	aese	v0.16b,v6.16b
+	aesmc	v0.16b,v0.16b
+	subs	w5,w5,#10			// if rounds==10, jump to aes-128-ecb processing
+	b.eq	.Lecb_128_enc
+.Lecb_round_loop:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.4s},[x3],#16				// load key schedule...
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v17.4s},[x3],#16				// load key schedule...
+	subs	w5,w5,#2			// bias
+	b.gt	.Lecb_round_loop
+.Lecb_128_enc:
+	ld1	{v18.4s,v19.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v20.4s,v21.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v18.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v19.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v22.4s,v23.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v7.4s},[x3]
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v23.16b
+	eor	v0.16b,v0.16b,v7.16b
+	st1	{v0.16b},[x1]
+	b	.Lecb_Final_abort
+.Lecb_small_dec:
+	aesd	v0.16b,v5.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v16.4s,v17.4s},[x3],#32			// load key schedule...
+	aesd	v0.16b,v6.16b
+	aesimc	v0.16b,v0.16b
+	subs	w5,w5,#10			// bias
+	b.eq	.Lecb_128_dec
+.Lecb_dec_round_loop:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v16.4s},[x3],#16				// load key schedule...
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v17.4s},[x3],#16				// load key schedule...
+	subs	w5,w5,#2			// bias
+	b.gt	.Lecb_dec_round_loop
+.Lecb_128_dec:
+	ld1	{v18.4s,v19.4s},[x3],#32		// load key schedule...
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v20.4s,v21.4s},[x3],#32		// load key schedule...
+	aesd	v0.16b,v18.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v19.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v22.4s,v23.4s},[x3],#32		// load key schedule...
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v7.4s},[x3]
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v23.16b
+	eor	v0.16b,v0.16b,v7.16b
+	st1	{v0.16b},[x1]
+	b	.Lecb_Final_abort
+.Lecb_big_size:
+	stp	x29,x30,[sp,#-16]!
+	add	x29,sp,#0
+	mov	x8,#16
+	b.lo	.Lecb_done
+	csel	x8,xzr,x8,eq
+
+	cmp	w4,#0					// en- or decrypting?
+	ldr	w5,[x3,#240]
+	and	x2,x2,#-16
+	ld1	{v0.16b},[x0],x8
+
+	ld1	{v16.4s,v17.4s},[x3]				// load key schedule...
+	sub	w5,w5,#6
+	add	x7,x3,x5,lsl#4				// pointer to last 7 round keys
+	sub	w5,w5,#2
+	ld1	{v18.4s,v19.4s},[x7],#32
+	ld1	{v20.4s,v21.4s},[x7],#32
+	ld1	{v22.4s,v23.4s},[x7],#32
+	ld1	{v7.4s},[x7]
+
+	add	x7,x3,#32
+	mov	w6,w5
+	b.eq	.Lecb_dec
+
+	ld1	{v1.16b},[x0],#16
+	subs	x2,x2,#32				// bias
+	add	w6,w5,#2
+	orr	v3.16b,v1.16b,v1.16b
+	orr	v24.16b,v1.16b,v1.16b
+	orr	v1.16b,v0.16b,v0.16b
+	b.lo	.Lecb_enc_tail
+
+	orr	v1.16b,v3.16b,v3.16b
+	ld1	{v24.16b},[x0],#16
+	cmp	x2,#32
+	b.lo	.Loop3x_ecb_enc
+
+	ld1	{v25.16b},[x0],#16
+	ld1	{v26.16b},[x0],#16
+	sub	x2,x2,#32				// bias
+	mov	w6,w5
+
+.Loop5x_ecb_enc:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v16.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v16.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v17.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v17.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop5x_ecb_enc
+
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v16.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v16.16b
+	aesmc	v26.16b,v26.16b
+	cmp	x2,#0x40					// because .Lecb_enc_tail4x
+	sub	x2,x2,#0x50
+
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v17.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v17.16b
+	aesmc	v26.16b,v26.16b
+	csel	x6,xzr,x2,gt			// borrow x6, w6, "gt" is not typo
+	mov	x7,x3
+
+	aese	v0.16b,v18.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v18.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v18.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v18.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v18.16b
+	aesmc	v26.16b,v26.16b
+	add	x0,x0,x6				// x0 is adjusted in such way that
+							// at exit from the loop v1.16b-v26.16b
+							// are loaded with last "words"
+	add	x6,x2,#0x60		    // because .Lecb_enc_tail4x
+
+	aese	v0.16b,v19.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v19.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v19.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v19.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v19.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v20.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v20.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v21.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v21.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v22.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v22.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v23.16b
+	ld1	{v2.16b},[x0],#16
+	aese	v1.16b,v23.16b
+	ld1	{v3.16b},[x0],#16
+	aese	v24.16b,v23.16b
+	ld1	{v27.16b},[x0],#16
+	aese	v25.16b,v23.16b
+	ld1	{v28.16b},[x0],#16
+	aese	v26.16b,v23.16b
+	ld1	{v29.16b},[x0],#16
+	cbz	x6,.Lecb_enc_tail4x
+	ld1	{v16.4s},[x7],#16			// re-pre-load rndkey[0]
+	eor	v4.16b,v7.16b,v0.16b
+	orr	v0.16b,v2.16b,v2.16b
+	eor	v5.16b,v7.16b,v1.16b
+	orr	v1.16b,v3.16b,v3.16b
+	eor	v17.16b,v7.16b,v24.16b
+	orr	v24.16b,v27.16b,v27.16b
+	eor	v30.16b,v7.16b,v25.16b
+	orr	v25.16b,v28.16b,v28.16b
+	eor	v31.16b,v7.16b,v26.16b
+	st1	{v4.16b},[x1],#16
+	orr	v26.16b,v29.16b,v29.16b
+	st1	{v5.16b},[x1],#16
+	mov	w6,w5
+	st1	{v17.16b},[x1],#16
+	ld1	{v17.4s},[x7],#16			// re-pre-load rndkey[1]
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+	b.hs	.Loop5x_ecb_enc
+
+	add	x2,x2,#0x50
+	cbz	x2,.Lecb_done
+
+	add	w6,w5,#2
+	subs	x2,x2,#0x30
+	orr	v0.16b,v27.16b,v27.16b
+	orr	v1.16b,v28.16b,v28.16b
+	orr	v24.16b,v29.16b,v29.16b
+	b.lo	.Lecb_enc_tail
+
+	b	.Loop3x_ecb_enc
+
+.align	4
+.Lecb_enc_tail4x:
+	eor	v5.16b,v7.16b,v1.16b
+	eor	v17.16b,v7.16b,v24.16b
+	eor	v30.16b,v7.16b,v25.16b
+	eor	v31.16b,v7.16b,v26.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+
+	b	.Lecb_done
+.align	4
+.Loop3x_ecb_enc:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop3x_ecb_enc
+
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	subs	x2,x2,#0x30
+	csel	x6,x2,x6,lo				// x6, w6, is zero at this point
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	add	x0,x0,x6			// x0 is adjusted in such way that
+						// at exit from the loop v1.16b-v24.16b
+						// are loaded with last "words"
+	mov	x7,x3
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v2.16b},[x0],#16
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v3.16b},[x0],#16
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v27.16b},[x0],#16
+	aese	v0.16b,v23.16b
+	aese	v1.16b,v23.16b
+	aese	v24.16b,v23.16b
+	ld1	{v16.4s},[x7],#16		// re-pre-load rndkey[0]
+	add	w6,w5,#2
+	eor	v4.16b,v7.16b,v0.16b
+	eor	v5.16b,v7.16b,v1.16b
+	eor	v24.16b,v24.16b,v7.16b
+	ld1	{v17.4s},[x7],#16		// re-pre-load rndkey[1]
+	st1	{v4.16b},[x1],#16
+	orr	v0.16b,v2.16b,v2.16b
+	st1	{v5.16b},[x1],#16
+	orr	v1.16b,v3.16b,v3.16b
+	st1	{v24.16b},[x1],#16
+	orr	v24.16b,v27.16b,v27.16b
+	b.hs	.Loop3x_ecb_enc
+
+	cmn	x2,#0x30
+	b.eq	.Lecb_done
+	nop
+
+.Lecb_enc_tail:
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lecb_enc_tail
+
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	cmn	x2,#0x20
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	aese	v1.16b,v23.16b
+	aese	v24.16b,v23.16b
+	b.eq	.Lecb_enc_one
+	eor	v5.16b,v7.16b,v1.16b
+	eor	v17.16b,v7.16b,v24.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	b	.Lecb_done
+
+.Lecb_enc_one:
+	eor	v5.16b,v7.16b,v24.16b
+	st1	{v5.16b},[x1],#16
+	b	.Lecb_done
+.align	5
+.Lecb_dec:
+	ld1	{v1.16b},[x0],#16
+	subs	x2,x2,#32			// bias
+	add	w6,w5,#2
+	orr	v3.16b,v1.16b,v1.16b
+	orr	v24.16b,v1.16b,v1.16b
+	orr	v1.16b,v0.16b,v0.16b
+	b.lo	.Lecb_dec_tail
+
+	orr	v1.16b,v3.16b,v3.16b
+	ld1	{v24.16b},[x0],#16
+	cmp	x2,#32
+	b.lo	.Loop3x_ecb_dec
+
+	ld1	{v25.16b},[x0],#16
+	ld1	{v26.16b},[x0],#16
+	sub	x2,x2,#32				// bias
+	mov	w6,w5
+
+.Loop5x_ecb_dec:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v16.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v16.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v17.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v17.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop5x_ecb_dec
+
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v16.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v16.16b
+	aesimc	v26.16b,v26.16b
+	cmp	x2,#0x40				// because .Lecb_tail4x
+	sub	x2,x2,#0x50
+
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v17.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v17.16b
+	aesimc	v26.16b,v26.16b
+	csel	x6,xzr,x2,gt		// borrow x6, w6, "gt" is not typo
+	mov	x7,x3
+
+	aesd	v0.16b,v18.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v18.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v18.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v18.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v18.16b
+	aesimc	v26.16b,v26.16b
+	add	x0,x0,x6				// x0 is adjusted in such way that
+							// at exit from the loop v1.16b-v26.16b
+							// are loaded with last "words"
+	add	x6,x2,#0x60			// because .Lecb_tail4x
+
+	aesd	v0.16b,v19.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v19.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v19.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v19.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v19.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v20.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v20.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v21.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v21.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v22.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v22.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v23.16b
+	ld1	{v2.16b},[x0],#16
+	aesd	v1.16b,v23.16b
+	ld1	{v3.16b},[x0],#16
+	aesd	v24.16b,v23.16b
+	ld1	{v27.16b},[x0],#16
+	aesd	v25.16b,v23.16b
+	ld1	{v28.16b},[x0],#16
+	aesd	v26.16b,v23.16b
+	ld1	{v29.16b},[x0],#16
+	cbz	x6,.Lecb_tail4x
+	ld1	{v16.4s},[x7],#16			// re-pre-load rndkey[0]
+	eor	v4.16b,v7.16b,v0.16b
+	orr	v0.16b,v2.16b,v2.16b
+	eor	v5.16b,v7.16b,v1.16b
+	orr	v1.16b,v3.16b,v3.16b
+	eor	v17.16b,v7.16b,v24.16b
+	orr	v24.16b,v27.16b,v27.16b
+	eor	v30.16b,v7.16b,v25.16b
+	orr	v25.16b,v28.16b,v28.16b
+	eor	v31.16b,v7.16b,v26.16b
+	st1	{v4.16b},[x1],#16
+	orr	v26.16b,v29.16b,v29.16b
+	st1	{v5.16b},[x1],#16
+	mov	w6,w5
+	st1	{v17.16b},[x1],#16
+	ld1	{v17.4s},[x7],#16			// re-pre-load rndkey[1]
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+	b.hs	.Loop5x_ecb_dec
+
+	add	x2,x2,#0x50
+	cbz	x2,.Lecb_done
+
+	add	w6,w5,#2
+	subs	x2,x2,#0x30
+	orr	v0.16b,v27.16b,v27.16b
+	orr	v1.16b,v28.16b,v28.16b
+	orr	v24.16b,v29.16b,v29.16b
+	b.lo	.Lecb_dec_tail
+
+	b	.Loop3x_ecb_dec
+
+.align	4
+.Lecb_tail4x:
+	eor	v5.16b,v7.16b,v1.16b
+	eor	v17.16b,v7.16b,v24.16b
+	eor	v30.16b,v7.16b,v25.16b
+	eor	v31.16b,v7.16b,v26.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+
+	b	.Lecb_done
+.align	4
+.Loop3x_ecb_dec:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop3x_ecb_dec
+
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	subs	x2,x2,#0x30
+	csel	x6,x2,x6,lo				// x6, w6, is zero at this point
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	add	x0,x0,x6 			// x0 is adjusted in such way that
+						// at exit from the loop v1.16b-v24.16b
+						// are loaded with last "words"
+	mov	x7,x3
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v2.16b},[x0],#16
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v3.16b},[x0],#16
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v27.16b},[x0],#16
+	aesd	v0.16b,v23.16b
+	aesd	v1.16b,v23.16b
+	aesd	v24.16b,v23.16b
+	ld1	{v16.4s},[x7],#16			// re-pre-load rndkey[0]
+	add	w6,w5,#2
+	eor	v4.16b,v7.16b,v0.16b
+	eor	v5.16b,v7.16b,v1.16b
+	eor	v24.16b,v24.16b,v7.16b
+	ld1	{v17.4s},[x7],#16			// re-pre-load rndkey[1]
+	st1	{v4.16b},[x1],#16
+	orr	v0.16b,v2.16b,v2.16b
+	st1	{v5.16b},[x1],#16
+	orr	v1.16b,v3.16b,v3.16b
+	st1	{v24.16b},[x1],#16
+	orr	v24.16b,v27.16b,v27.16b
+	b.hs	.Loop3x_ecb_dec
+
+	cmn	x2,#0x30
+	b.eq	.Lecb_done
+	nop
+
+.Lecb_dec_tail:
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lecb_dec_tail
+
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	cmn	x2,#0x20
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v23.16b
+	aesd	v24.16b,v23.16b
+	b.eq	.Lecb_dec_one
+	eor	v5.16b,v7.16b,v1.16b
+	eor	v17.16b,v7.16b,v24.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	b	.Lecb_done
+
+.Lecb_dec_one:
+	eor	v5.16b,v7.16b,v24.16b
+	st1	{v5.16b},[x1],#16
+
+.Lecb_done:
+	ldr	x29,[sp],#16
+.Lecb_Final_abort:
+	ret
+.size	aes_v8_ecb_encrypt,.-aes_v8_ecb_encrypt
+.globl	aes_v8_cbc_encrypt
+.type	aes_v8_cbc_encrypt,%function
+.align	5
+aes_v8_cbc_encrypt:
+	stp	x29,x30,[sp,#-16]!
+	add	x29,sp,#0
+	subs	x2,x2,#16
+	mov	x8,#16
+	b.lo	.Lcbc_abort
+	csel	x8,xzr,x8,eq
+
+	cmp	w5,#0			// en- or decrypting?
+	ldr	w5,[x3,#240]
+	and	x2,x2,#-16
+	ld1	{v6.16b},[x4]
+	ld1	{v0.16b},[x0],x8
+
+	ld1	{v16.4s,v17.4s},[x3]		// load key schedule...
+	sub	w5,w5,#6
+	add	x7,x3,x5,lsl#4	// pointer to last 7 round keys
+	sub	w5,w5,#2
+	ld1	{v18.4s,v19.4s},[x7],#32
+	ld1	{v20.4s,v21.4s},[x7],#32
+	ld1	{v22.4s,v23.4s},[x7],#32
+	ld1	{v7.4s},[x7]
+
+	add	x7,x3,#32
+	mov	w6,w5
+	b.eq	.Lcbc_dec
+
+	cmp	w5,#2
+	eor	v0.16b,v0.16b,v6.16b
+	eor	v5.16b,v16.16b,v7.16b
+	b.eq	.Lcbc_enc128
+
+	ld1	{v2.4s,v3.4s},[x7]
+	add	x7,x3,#16
+	add	x6,x3,#16*4
+	add	x12,x3,#16*5
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	add	x14,x3,#16*6
+	add	x3,x3,#16*7
+	b	.Lenter_cbc_enc
+
+.align	4
+.Loop_cbc_enc:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	st1	{v6.16b},[x1],#16
+.Lenter_cbc_enc:
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v2.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.4s},[x6]
+	cmp	w5,#4
+	aese	v0.16b,v3.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v17.4s},[x12]
+	b.eq	.Lcbc_enc192
+
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.4s},[x14]
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v17.4s},[x3]
+	nop
+
+.Lcbc_enc192:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	subs	x2,x2,#16
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	csel	x8,xzr,x8,eq
+	aese	v0.16b,v18.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v19.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.16b},[x0],x8
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	eor	v16.16b,v16.16b,v5.16b
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v17.4s},[x7]		// re-pre-load rndkey[1]
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v23.16b
+	eor	v6.16b,v0.16b,v7.16b
+	b.hs	.Loop_cbc_enc
+
+	st1	{v6.16b},[x1],#16
+	b	.Lcbc_done
+
+.align	5
+.Lcbc_enc128:
+	ld1	{v2.4s,v3.4s},[x7]
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	b	.Lenter_cbc_enc128
+.Loop_cbc_enc128:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	st1	{v6.16b},[x1],#16
+.Lenter_cbc_enc128:
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	subs	x2,x2,#16
+	aese	v0.16b,v2.16b
+	aesmc	v0.16b,v0.16b
+	csel	x8,xzr,x8,eq
+	aese	v0.16b,v3.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v18.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v19.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.16b},[x0],x8
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	eor	v16.16b,v16.16b,v5.16b
+	aese	v0.16b,v23.16b
+	eor	v6.16b,v0.16b,v7.16b
+	b.hs	.Loop_cbc_enc128
+
+	st1	{v6.16b},[x1],#16
+	b	.Lcbc_done
+.align	5
+.Lcbc_dec:
+	ld1	{v24.16b},[x0],#16
+	subs	x2,x2,#32		// bias
+	add	w6,w5,#2
+	orr	v3.16b,v0.16b,v0.16b
+	orr	v1.16b,v0.16b,v0.16b
+	orr	v27.16b,v24.16b,v24.16b
+	b.lo	.Lcbc_dec_tail
+
+	orr	v1.16b,v24.16b,v24.16b
+	ld1	{v24.16b},[x0],#16
+	orr	v2.16b,v0.16b,v0.16b
+	orr	v3.16b,v1.16b,v1.16b
+	orr	v27.16b,v24.16b,v24.16b
+	cmp	x2,#32
+	b.lo	.Loop3x_cbc_dec
+
+	ld1	{v25.16b},[x0],#16
+	ld1	{v26.16b},[x0],#16
+	sub	x2,x2,#32		// bias
+	mov	w6,w5
+	orr	v28.16b,v25.16b,v25.16b
+	orr	v29.16b,v26.16b,v26.16b
+
+.Loop5x_cbc_dec:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v16.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v16.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v17.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v17.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop5x_cbc_dec
+
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v16.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v16.16b
+	aesimc	v26.16b,v26.16b
+	cmp	x2,#0x40		// because .Lcbc_tail4x
+	sub	x2,x2,#0x50
+
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v17.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v17.16b
+	aesimc	v26.16b,v26.16b
+	csel	x6,xzr,x2,gt		// borrow x6, w6, "gt" is not typo
+	mov	x7,x3
+
+	aesd	v0.16b,v18.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v18.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v18.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v18.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v18.16b
+	aesimc	v26.16b,v26.16b
+	add	x0,x0,x6		// x0 is adjusted in such way that
+					// at exit from the loop v1.16b-v26.16b
+					// are loaded with last "words"
+	add	x6,x2,#0x60		// because .Lcbc_tail4x
+
+	aesd	v0.16b,v19.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v19.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v19.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v19.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v19.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v20.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v20.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v21.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v21.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v22.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v22.16b
+	aesimc	v26.16b,v26.16b
+
+	eor	v4.16b,v6.16b,v7.16b
+	aesd	v0.16b,v23.16b
+	eor	v5.16b,v2.16b,v7.16b
+	ld1	{v2.16b},[x0],#16
+	aesd	v1.16b,v23.16b
+	eor	v17.16b,v3.16b,v7.16b
+	ld1	{v3.16b},[x0],#16
+	aesd	v24.16b,v23.16b
+	eor	v30.16b,v27.16b,v7.16b
+	ld1	{v27.16b},[x0],#16
+	aesd	v25.16b,v23.16b
+	eor	v31.16b,v28.16b,v7.16b
+	ld1	{v28.16b},[x0],#16
+	aesd	v26.16b,v23.16b
+	orr	v6.16b,v29.16b,v29.16b
+	ld1	{v29.16b},[x0],#16
+	cbz	x6,.Lcbc_tail4x
+	ld1	{v16.4s},[x7],#16	// re-pre-load rndkey[0]
+	eor	v4.16b,v4.16b,v0.16b
+	orr	v0.16b,v2.16b,v2.16b
+	eor	v5.16b,v5.16b,v1.16b
+	orr	v1.16b,v3.16b,v3.16b
+	eor	v17.16b,v17.16b,v24.16b
+	orr	v24.16b,v27.16b,v27.16b
+	eor	v30.16b,v30.16b,v25.16b
+	orr	v25.16b,v28.16b,v28.16b
+	eor	v31.16b,v31.16b,v26.16b
+	st1	{v4.16b},[x1],#16
+	orr	v26.16b,v29.16b,v29.16b
+	st1	{v5.16b},[x1],#16
+	mov	w6,w5
+	st1	{v17.16b},[x1],#16
+	ld1	{v17.4s},[x7],#16	// re-pre-load rndkey[1]
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+	b.hs	.Loop5x_cbc_dec
+
+	add	x2,x2,#0x50
+	cbz	x2,.Lcbc_done
+
+	add	w6,w5,#2
+	subs	x2,x2,#0x30
+	orr	v0.16b,v27.16b,v27.16b
+	orr	v2.16b,v27.16b,v27.16b
+	orr	v1.16b,v28.16b,v28.16b
+	orr	v3.16b,v28.16b,v28.16b
+	orr	v24.16b,v29.16b,v29.16b
+	orr	v27.16b,v29.16b,v29.16b
+	b.lo	.Lcbc_dec_tail
+
+	b	.Loop3x_cbc_dec
+
+.align	4
+.Lcbc_tail4x:
+	eor	v5.16b,v4.16b,v1.16b
+	eor	v17.16b,v17.16b,v24.16b
+	eor	v30.16b,v30.16b,v25.16b
+	eor	v31.16b,v31.16b,v26.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+
+	b	.Lcbc_done
+.align	4
+.Loop3x_cbc_dec:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop3x_cbc_dec
+
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	eor	v4.16b,v6.16b,v7.16b
+	subs	x2,x2,#0x30
+	eor	v5.16b,v2.16b,v7.16b
+	csel	x6,x2,x6,lo			// x6, w6, is zero at this point
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	eor	v17.16b,v3.16b,v7.16b
+	add	x0,x0,x6		// x0 is adjusted in such way that
+					// at exit from the loop v1.16b-v24.16b
+					// are loaded with last "words"
+	orr	v6.16b,v27.16b,v27.16b
+	mov	x7,x3
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v2.16b},[x0],#16
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v3.16b},[x0],#16
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v27.16b},[x0],#16
+	aesd	v0.16b,v23.16b
+	aesd	v1.16b,v23.16b
+	aesd	v24.16b,v23.16b
+	ld1	{v16.4s},[x7],#16	// re-pre-load rndkey[0]
+	add	w6,w5,#2
+	eor	v4.16b,v4.16b,v0.16b
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v24.16b,v24.16b,v17.16b
+	ld1	{v17.4s},[x7],#16	// re-pre-load rndkey[1]
+	st1	{v4.16b},[x1],#16
+	orr	v0.16b,v2.16b,v2.16b
+	st1	{v5.16b},[x1],#16
+	orr	v1.16b,v3.16b,v3.16b
+	st1	{v24.16b},[x1],#16
+	orr	v24.16b,v27.16b,v27.16b
+	b.hs	.Loop3x_cbc_dec
+
+	cmn	x2,#0x30
+	b.eq	.Lcbc_done
+	nop
+
+.Lcbc_dec_tail:
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lcbc_dec_tail
+
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	cmn	x2,#0x20
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	eor	v5.16b,v6.16b,v7.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	eor	v17.16b,v3.16b,v7.16b
+	aesd	v1.16b,v23.16b
+	aesd	v24.16b,v23.16b
+	b.eq	.Lcbc_dec_one
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v17.16b,v17.16b,v24.16b
+	orr	v6.16b,v27.16b,v27.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	b	.Lcbc_done
+
+.Lcbc_dec_one:
+	eor	v5.16b,v5.16b,v24.16b
+	orr	v6.16b,v27.16b,v27.16b
+	st1	{v5.16b},[x1],#16
+
+.Lcbc_done:
+	st1	{v6.16b},[x4]
+.Lcbc_abort:
+	ldr	x29,[sp],#16
+	ret
+.size	aes_v8_cbc_encrypt,.-aes_v8_cbc_encrypt
+.globl	aes_v8_ctr32_encrypt_blocks
+.type	aes_v8_ctr32_encrypt_blocks,%function
+.align	5
+aes_v8_ctr32_encrypt_blocks:
+	stp	x29,x30,[sp,#-16]!
+	add	x29,sp,#0
+	ldr	w5,[x3,#240]
+
+	ldr	w8, [x4, #12]
+#ifdef __ARMEB__
+	ld1	{v0.16b},[x4]
+#else
+	ld1	{v0.4s},[x4]
+#endif
+	ld1	{v16.4s,v17.4s},[x3]		// load key schedule...
+	sub	w5,w5,#4
+	mov	x12,#16
+	cmp	x2,#2
+	add	x7,x3,x5,lsl#4	// pointer to last 5 round keys
+	sub	w5,w5,#2
+	ld1	{v20.4s,v21.4s},[x7],#32
+	ld1	{v22.4s,v23.4s},[x7],#32
+	ld1	{v7.4s},[x7]
+	add	x7,x3,#32
+	mov	w6,w5
+	csel	x12,xzr,x12,lo
+#ifndef __ARMEB__
+	rev	w8, w8
+#endif
+	orr	v1.16b,v0.16b,v0.16b
+	add	w10, w8, #1
+	orr	v18.16b,v0.16b,v0.16b
+	add	w8, w8, #2
+	orr	v6.16b,v0.16b,v0.16b
+	rev	w10, w10
+	mov	v1.s[3],w10
+	b.ls	.Lctr32_tail
+	rev	w12, w8
+	sub	x2,x2,#3		// bias
+	mov	v18.s[3],w12
+	cmp	x2,#32
+	b.lo	.Loop3x_ctr32
+
+	add	w13,w8,#1
+	add	w14,w8,#2
+	orr	v24.16b,v0.16b,v0.16b
+	rev	w13,w13
+	orr	v25.16b,v0.16b,v0.16b
+	rev	w14,w14
+	mov	v24.s[3],w13
+	sub	x2,x2,#2		// bias
+	mov	v25.s[3],w14
+	add	w8,w8,#2
+	b	.Loop5x_ctr32
+
+.align	4
+.Loop5x_ctr32:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v16.16b
+	aesmc	v18.16b,v18.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v16.16b
+	aesmc	v25.16b,v25.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v17.16b
+	aesmc	v18.16b,v18.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v17.16b
+	aesmc	v25.16b,v25.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop5x_ctr32
+
+	mov	x7,x3
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v16.16b
+	aesmc	v18.16b,v18.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v16.16b
+	aesmc	v25.16b,v25.16b
+	ld1	{v16.4s},[x7],#16	// re-pre-load rndkey[0]
+
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v17.16b
+	aesmc	v18.16b,v18.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v17.16b
+	aesmc	v25.16b,v25.16b
+	ld1	{v17.4s},[x7],#16	// re-pre-load rndkey[1]
+
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	add	w9,w8,#1
+	add	w10,w8,#2
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	add	w12,w8,#3
+	add	w13,w8,#4
+	aese	v18.16b,v20.16b
+	aesmc	v18.16b,v18.16b
+	add	w14,w8,#5
+	rev	w9,w9
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	rev	w10,w10
+	rev	w12,w12
+	aese	v25.16b,v20.16b
+	aesmc	v25.16b,v25.16b
+	rev	w13,w13
+	rev	w14,w14
+
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v21.16b
+	aesmc	v18.16b,v18.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v21.16b
+	aesmc	v25.16b,v25.16b
+
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v2.16b},[x0],#16
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	ld1	{v3.16b},[x0],#16
+	aese	v18.16b,v22.16b
+	aesmc	v18.16b,v18.16b
+	ld1	{v19.16b},[x0],#16
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v26.16b},[x0],#16
+	aese	v25.16b,v22.16b
+	aesmc	v25.16b,v25.16b
+	ld1	{v27.16b},[x0],#16
+
+	aese	v0.16b,v23.16b
+	eor	v2.16b,v2.16b,v7.16b
+	aese	v1.16b,v23.16b
+	eor	v3.16b,v3.16b,v7.16b
+	aese	v18.16b,v23.16b
+	eor	v19.16b,v19.16b,v7.16b
+	aese	v24.16b,v23.16b
+	eor	v26.16b,v26.16b,v7.16b
+	aese	v25.16b,v23.16b
+	eor	v27.16b,v27.16b,v7.16b
+
+	eor	v2.16b,v2.16b,v0.16b
+	orr	v0.16b,v6.16b,v6.16b
+	eor	v3.16b,v3.16b,v1.16b
+	orr	v1.16b,v6.16b,v6.16b
+	eor	v19.16b,v19.16b,v18.16b
+	orr	v18.16b,v6.16b,v6.16b
+	eor	v26.16b,v26.16b,v24.16b
+	orr	v24.16b,v6.16b,v6.16b
+	eor	v27.16b,v27.16b,v25.16b
+	orr	v25.16b,v6.16b,v6.16b
+
+	st1	{v2.16b},[x1],#16
+	mov	v0.s[3],w9
+	st1	{v3.16b},[x1],#16
+	mov	v1.s[3],w10
+	st1	{v19.16b},[x1],#16
+	mov	v18.s[3],w12
+	st1	{v26.16b},[x1],#16
+	mov	v24.s[3],w13
+	st1	{v27.16b},[x1],#16
+	mov	v25.s[3],w14
+
+	mov	w6,w5
+	cbz	x2,.Lctr32_done
+
+	add	w8,w8,#5
+	subs	x2,x2,#5
+	b.hs	.Loop5x_ctr32
+
+	add	x2,x2,#5
+	sub	w8,w8,#5
+
+	cmp	x2,#2
+	mov	x12,#16
+	csel	x12,xzr,x12,lo
+	b.ls	.Lctr32_tail
+
+	sub	x2,x2,#3		// bias
+	add	w8,w8,#3
+	b	.Loop3x_ctr32
+
+.align	4
+.Loop3x_ctr32:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v16.16b
+	aesmc	v18.16b,v18.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v18.16b,v17.16b
+	aesmc	v18.16b,v18.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop3x_ctr32
+
+	aese	v0.16b,v16.16b
+	aesmc	v4.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v5.16b,v1.16b
+	ld1	{v2.16b},[x0],#16
+	orr	v0.16b,v6.16b,v6.16b
+	aese	v18.16b,v16.16b
+	aesmc	v18.16b,v18.16b
+	ld1	{v3.16b},[x0],#16
+	orr	v1.16b,v6.16b,v6.16b
+	aese	v4.16b,v17.16b
+	aesmc	v4.16b,v4.16b
+	aese	v5.16b,v17.16b
+	aesmc	v5.16b,v5.16b
+	ld1	{v19.16b},[x0],#16
+	mov	x7,x3
+	aese	v18.16b,v17.16b
+	aesmc	v17.16b,v18.16b
+	orr	v18.16b,v6.16b,v6.16b
+	add	w9,w8,#1
+	aese	v4.16b,v20.16b
+	aesmc	v4.16b,v4.16b
+	aese	v5.16b,v20.16b
+	aesmc	v5.16b,v5.16b
+	eor	v2.16b,v2.16b,v7.16b
+	add	w10,w8,#2
+	aese	v17.16b,v20.16b
+	aesmc	v17.16b,v17.16b
+	eor	v3.16b,v3.16b,v7.16b
+	add	w8,w8,#3
+	aese	v4.16b,v21.16b
+	aesmc	v4.16b,v4.16b
+	aese	v5.16b,v21.16b
+	aesmc	v5.16b,v5.16b
+	eor	v19.16b,v19.16b,v7.16b
+	rev	w9,w9
+	aese	v17.16b,v21.16b
+	aesmc	v17.16b,v17.16b
+	mov	v0.s[3], w9
+	rev	w10,w10
+	aese	v4.16b,v22.16b
+	aesmc	v4.16b,v4.16b
+	aese	v5.16b,v22.16b
+	aesmc	v5.16b,v5.16b
+	mov	v1.s[3], w10
+	rev	w12,w8
+	aese	v17.16b,v22.16b
+	aesmc	v17.16b,v17.16b
+	mov	v18.s[3], w12
+	subs	x2,x2,#3
+	aese	v4.16b,v23.16b
+	aese	v5.16b,v23.16b
+	aese	v17.16b,v23.16b
+
+	eor	v2.16b,v2.16b,v4.16b
+	ld1	{v16.4s},[x7],#16	// re-pre-load rndkey[0]
+	st1	{v2.16b},[x1],#16
+	eor	v3.16b,v3.16b,v5.16b
+	mov	w6,w5
+	st1	{v3.16b},[x1],#16
+	eor	v19.16b,v19.16b,v17.16b
+	ld1	{v17.4s},[x7],#16	// re-pre-load rndkey[1]
+	st1	{v19.16b},[x1],#16
+	b.hs	.Loop3x_ctr32
+
+	adds	x2,x2,#3
+	b.eq	.Lctr32_done
+	cmp	x2,#1
+	mov	x12,#16
+	csel	x12,xzr,x12,eq
+
+.Lctr32_tail:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lctr32_tail
+
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	ld1	{v2.16b},[x0],x12
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	ld1	{v3.16b},[x0]
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	eor	v2.16b,v2.16b,v7.16b
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	eor	v3.16b,v3.16b,v7.16b
+	aese	v0.16b,v23.16b
+	aese	v1.16b,v23.16b
+
+	cmp	x2,#1
+	eor	v2.16b,v2.16b,v0.16b
+	eor	v3.16b,v3.16b,v1.16b
+	st1	{v2.16b},[x1],#16
+	b.eq	.Lctr32_done
+	st1	{v3.16b},[x1]
+
+.Lctr32_done:
+	ldr	x29,[sp],#16
+	ret
+.size	aes_v8_ctr32_encrypt_blocks,.-aes_v8_ctr32_encrypt_blocks
+.globl	aes_v8_xts_encrypt
+.type	aes_v8_xts_encrypt,%function
+.align	5
+aes_v8_xts_encrypt:
+	cmp	x2,#16
+	// Original input data size bigger than 16, jump to big size processing.
+	b.ne	.Lxts_enc_big_size
+	// Encrypt the iv with key2, as the first XEX iv.
+	ldr	w6,[x4,#240]
+	ld1	{v0.16b},[x4],#16
+	ld1	{v6.16b},[x5]
+	sub	w6,w6,#2
+	ld1	{v1.16b},[x4],#16
+
+.Loop_enc_iv_enc:
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4],#16
+	subs	w6,w6,#2
+	aese	v6.16b,v1.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v1.4s},[x4],#16
+	b.gt	.Loop_enc_iv_enc
+
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4]
+	aese	v6.16b,v1.16b
+	eor	v6.16b,v6.16b,v0.16b
+
+	ld1	{v0.16b},[x0]
+	eor	v0.16b,v6.16b,v0.16b
+
+	ldr	w6,[x3,#240]
+	ld1	{v28.4s,v29.4s},[x3],#32		// load key schedule...
+
+	aese	v0.16b,v28.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.4s,v17.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v29.16b
+	aesmc	v0.16b,v0.16b
+	subs	w6,w6,#10		// if rounds==10, jump to aes-128-xts processing
+	b.eq	.Lxts_128_enc
+.Lxts_enc_round_loop:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v16.4s},[x3],#16		// load key schedule...
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v17.4s},[x3],#16		// load key schedule...
+	subs	w6,w6,#2		// bias
+	b.gt	.Lxts_enc_round_loop
+.Lxts_128_enc:
+	ld1	{v18.4s,v19.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v20.4s,v21.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v18.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v19.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v22.4s,v23.4s},[x3],#32		// load key schedule...
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	ld1	{v7.4s},[x3]
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v0.16b,v23.16b
+	eor	v0.16b,v0.16b,v7.16b
+	eor	v0.16b,v0.16b,v6.16b
+	st1	{v0.16b},[x1]
+	b	.Lxts_enc_final_abort
+
+.align	4
+.Lxts_enc_big_size:
+	stp	x19,x20,[sp,#-64]!
+	stp	x21,x22,[sp,#48]
+	stp	d8,d9,[sp,#32]
+	stp	d10,d11,[sp,#16]
+
+	// tailcnt store the tail value of length%16.
+	and	x21,x2,#0xf
+	and	x2,x2,#-16
+	subs	x2,x2,#16
+	mov	x8,#16
+	b.lo	.Lxts_abort
+	csel	x8,xzr,x8,eq
+
+	// Firstly, encrypt the iv with key2, as the first iv of XEX.
+	ldr	w6,[x4,#240]
+	ld1	{v0.4s},[x4],#16
+	ld1	{v6.16b},[x5]
+	sub	w6,w6,#2
+	ld1	{v1.4s},[x4],#16
+
+.Loop_iv_enc:
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4],#16
+	subs	w6,w6,#2
+	aese	v6.16b,v1.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v1.4s},[x4],#16
+	b.gt	.Loop_iv_enc
+
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4]
+	aese	v6.16b,v1.16b
+	eor	v6.16b,v6.16b,v0.16b
+
+	// The iv for second block
+	// x9- iv(low), x10 - iv(high)
+	// the five ivs stored into, v6.16b,v8.16b,v9.16b,v10.16b,v11.16b
+	fmov	x9,d6
+	fmov	x10,v6.d[1]
+	mov	w19,#0x87
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d8,x9
+	fmov	v8.d[1],x10
+
+	ldr	w5,[x3,#240]		// next starting point
+	ld1	{v0.16b},[x0],x8
+
+	ld1	{v16.4s,v17.4s},[x3]			// load key schedule...
+	sub	w5,w5,#6
+	add	x7,x3,x5,lsl#4		// pointer to last 7 round keys
+	sub	w5,w5,#2
+	ld1	{v18.4s,v19.4s},[x7],#32
+	ld1	{v20.4s,v21.4s},[x7],#32
+	ld1	{v22.4s,v23.4s},[x7],#32
+	ld1	{v7.4s},[x7]
+
+	add	x7,x3,#32
+	mov	w6,w5
+
+	// Encryption
+.Lxts_enc:
+	ld1	{v24.16b},[x0],#16
+	subs	x2,x2,#32			// bias
+	add	w6,w5,#2
+	orr	v3.16b,v0.16b,v0.16b
+	orr	v1.16b,v0.16b,v0.16b
+	orr	v28.16b,v0.16b,v0.16b
+	orr	v27.16b,v24.16b,v24.16b
+	orr	v29.16b,v24.16b,v24.16b
+	b.lo	.Lxts_inner_enc_tail
+	eor	v0.16b,v0.16b,v6.16b			// before encryption, xor with iv
+	eor	v24.16b,v24.16b,v8.16b
+
+	// The iv for third block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d9,x9
+	fmov	v9.d[1],x10
+
+
+	orr	v1.16b,v24.16b,v24.16b
+	ld1	{v24.16b},[x0],#16
+	orr	v2.16b,v0.16b,v0.16b
+	orr	v3.16b,v1.16b,v1.16b
+	eor	v27.16b,v24.16b,v9.16b 		// the third block
+	eor	v24.16b,v24.16b,v9.16b
+	cmp	x2,#32
+	b.lo	.Lxts_outer_enc_tail
+
+	// The iv for fourth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d10,x9
+	fmov	v10.d[1],x10
+
+	ld1	{v25.16b},[x0],#16
+	// The iv for fifth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d11,x9
+	fmov	v11.d[1],x10
+
+	ld1	{v26.16b},[x0],#16
+	eor	v25.16b,v25.16b,v10.16b		// the fourth block
+	eor	v26.16b,v26.16b,v11.16b
+	sub	x2,x2,#32			// bias
+	mov	w6,w5
+	b	.Loop5x_xts_enc
+
+.align	4
+.Loop5x_xts_enc:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v16.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v16.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v17.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v17.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Loop5x_xts_enc
+
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v16.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v16.16b
+	aesmc	v26.16b,v26.16b
+	subs	x2,x2,#0x50			// because .Lxts_enc_tail4x
+
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v17.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v17.16b
+	aesmc	v26.16b,v26.16b
+	csel	x6,xzr,x2,gt		// borrow x6, w6, "gt" is not typo
+	mov	x7,x3
+
+	aese	v0.16b,v18.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v18.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v18.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v18.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v18.16b
+	aesmc	v26.16b,v26.16b
+	add	x0,x0,x6		// x0 is adjusted in such way that
+						// at exit from the loop v1.16b-v26.16b
+						// are loaded with last "words"
+	add	x6,x2,#0x60		// because .Lxts_enc_tail4x
+
+	aese	v0.16b,v19.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v19.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v19.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v19.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v19.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v20.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v20.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v21.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v21.16b
+	aesmc	v26.16b,v26.16b
+
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	aese	v25.16b,v22.16b
+	aesmc	v25.16b,v25.16b
+	aese	v26.16b,v22.16b
+	aesmc	v26.16b,v26.16b
+
+	eor	v4.16b,v7.16b,v6.16b
+	aese	v0.16b,v23.16b
+	// The iv for first block of one iteration
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d6,x9
+	fmov	v6.d[1],x10
+	eor	v5.16b,v7.16b,v8.16b
+	ld1	{v2.16b},[x0],#16
+	aese	v1.16b,v23.16b
+	// The iv for second block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d8,x9
+	fmov	v8.d[1],x10
+	eor	v17.16b,v7.16b,v9.16b
+	ld1	{v3.16b},[x0],#16
+	aese	v24.16b,v23.16b
+	// The iv for third block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d9,x9
+	fmov	v9.d[1],x10
+	eor	v30.16b,v7.16b,v10.16b
+	ld1	{v27.16b},[x0],#16
+	aese	v25.16b,v23.16b
+	// The iv for fourth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d10,x9
+	fmov	v10.d[1],x10
+	eor	v31.16b,v7.16b,v11.16b
+	ld1	{v28.16b},[x0],#16
+	aese	v26.16b,v23.16b
+
+	// The iv for fifth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d11,x9
+	fmov	v11.d[1],x10
+
+	ld1	{v29.16b},[x0],#16
+	cbz	x6,.Lxts_enc_tail4x
+	ld1	{v16.4s},[x7],#16		// re-pre-load rndkey[0]
+	eor	v4.16b,v4.16b,v0.16b
+	eor	v0.16b,v2.16b,v6.16b
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v1.16b,v3.16b,v8.16b
+	eor	v17.16b,v17.16b,v24.16b
+	eor	v24.16b,v27.16b,v9.16b
+	eor	v30.16b,v30.16b,v25.16b
+	eor	v25.16b,v28.16b,v10.16b
+	eor	v31.16b,v31.16b,v26.16b
+	st1	{v4.16b},[x1],#16
+	eor	v26.16b,v29.16b,v11.16b
+	st1	{v5.16b},[x1],#16
+	mov	w6,w5
+	st1	{v17.16b},[x1],#16
+	ld1	{v17.4s},[x7],#16		// re-pre-load rndkey[1]
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+	b.hs	.Loop5x_xts_enc
+
+
+	// If left 4 blocks, borrow the five block's processing.
+	cmn	x2,#0x10
+	b.ne	.Loop5x_enc_after
+	orr	v11.16b,v10.16b,v10.16b
+	orr	v10.16b,v9.16b,v9.16b
+	orr	v9.16b,v8.16b,v8.16b
+	orr	v8.16b,v6.16b,v6.16b
+	fmov	x9,d11
+	fmov	x10,v11.d[1]
+	eor	v0.16b,v6.16b,v2.16b
+	eor	v1.16b,v8.16b,v3.16b
+	eor	v24.16b,v27.16b,v9.16b
+	eor	v25.16b,v28.16b,v10.16b
+	eor	v26.16b,v29.16b,v11.16b
+	b.eq	.Loop5x_xts_enc
+
+.Loop5x_enc_after:
+	add	x2,x2,#0x50
+	cbz	x2,.Lxts_enc_done
+
+	add	w6,w5,#2
+	subs	x2,x2,#0x30
+	b.lo	.Lxts_inner_enc_tail
+
+	eor	v0.16b,v6.16b,v27.16b
+	eor	v1.16b,v8.16b,v28.16b
+	eor	v24.16b,v29.16b,v9.16b
+	b	.Lxts_outer_enc_tail
+
+.align	4
+.Lxts_enc_tail4x:
+	add	x0,x0,#16
+	eor	v5.16b,v1.16b,v5.16b
+	st1	{v5.16b},[x1],#16
+	eor	v17.16b,v24.16b,v17.16b
+	st1	{v17.16b},[x1],#16
+	eor	v30.16b,v25.16b,v30.16b
+	eor	v31.16b,v26.16b,v31.16b
+	st1	{v30.16b,v31.16b},[x1],#32
+
+	b	.Lxts_enc_done
+.align	4
+.Lxts_outer_enc_tail:
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lxts_outer_enc_tail
+
+	aese	v0.16b,v16.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	eor	v4.16b,v6.16b,v7.16b
+	subs	x2,x2,#0x30
+	// The iv for first block
+	fmov	x9,d9
+	fmov	x10,v9.d[1]
+	//mov	w19,#0x87
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr#31
+	eor	x9,x11,x9,lsl#1
+	fmov	d6,x9
+	fmov	v6.d[1],x10
+	eor	v5.16b,v8.16b,v7.16b
+	csel	x6,x2,x6,lo       // x6, w6, is zero at this point
+	aese	v0.16b,v17.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	eor	v17.16b,v9.16b,v7.16b
+
+	add	x6,x6,#0x20
+	add	x0,x0,x6
+	mov	x7,x3
+
+	aese	v0.16b,v20.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	aese	v0.16b,v21.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	aese	v0.16b,v22.16b
+	aesmc	v0.16b,v0.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	aese	v0.16b,v23.16b
+	aese	v1.16b,v23.16b
+	aese	v24.16b,v23.16b
+	ld1	{v27.16b},[x0],#16
+	add	w6,w5,#2
+	ld1	{v16.4s},[x7],#16                // re-pre-load rndkey[0]
+	eor	v4.16b,v4.16b,v0.16b
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v24.16b,v24.16b,v17.16b
+	ld1	{v17.4s},[x7],#16                // re-pre-load rndkey[1]
+	st1	{v4.16b},[x1],#16
+	st1	{v5.16b},[x1],#16
+	st1	{v24.16b},[x1],#16
+	cmn	x2,#0x30
+	b.eq	.Lxts_enc_done
+.Lxts_encxor_one:
+	orr	v28.16b,v3.16b,v3.16b
+	orr	v29.16b,v27.16b,v27.16b
+	nop
+
+.Lxts_inner_enc_tail:
+	cmn	x2,#0x10
+	eor	v1.16b,v28.16b,v6.16b
+	eor	v24.16b,v29.16b,v8.16b
+	b.eq	.Lxts_enc_tail_loop
+	eor	v24.16b,v29.16b,v6.16b
+.Lxts_enc_tail_loop:
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lxts_enc_tail_loop
+
+	aese	v1.16b,v16.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v16.16b
+	aesmc	v24.16b,v24.16b
+	aese	v1.16b,v17.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v17.16b
+	aesmc	v24.16b,v24.16b
+	aese	v1.16b,v20.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v20.16b
+	aesmc	v24.16b,v24.16b
+	cmn	x2,#0x20
+	aese	v1.16b,v21.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v21.16b
+	aesmc	v24.16b,v24.16b
+	eor	v5.16b,v6.16b,v7.16b
+	aese	v1.16b,v22.16b
+	aesmc	v1.16b,v1.16b
+	aese	v24.16b,v22.16b
+	aesmc	v24.16b,v24.16b
+	eor	v17.16b,v8.16b,v7.16b
+	aese	v1.16b,v23.16b
+	aese	v24.16b,v23.16b
+	b.eq	.Lxts_enc_one
+	eor	v5.16b,v5.16b,v1.16b
+	st1	{v5.16b},[x1],#16
+	eor	v17.16b,v17.16b,v24.16b
+	orr	v6.16b,v8.16b,v8.16b
+	st1	{v17.16b},[x1],#16
+	fmov	x9,d8
+	fmov	x10,v8.d[1]
+	mov	w19,#0x87
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d6,x9
+	fmov	v6.d[1],x10
+	b	.Lxts_enc_done
+
+.Lxts_enc_one:
+	eor	v5.16b,v5.16b,v24.16b
+	orr	v6.16b,v6.16b,v6.16b
+	st1	{v5.16b},[x1],#16
+	fmov	x9,d6
+	fmov	x10,v6.d[1]
+	mov	w19,#0x87
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d6,x9
+	fmov	v6.d[1],x10
+	b	.Lxts_enc_done
+.align	5
+.Lxts_enc_done:
+	// Process the tail block with cipher stealing.
+	tst	x21,#0xf
+	b.eq	.Lxts_abort
+
+	mov	x20,x0
+	mov	x13,x1
+	sub	x1,x1,#16
+.composite_enc_loop:
+	subs	x21,x21,#1
+	ldrb	w15,[x1,x21]
+	ldrb	w14,[x20,x21]
+	strb	w15,[x13,x21]
+	strb	w14,[x1,x21]
+	b.gt	.composite_enc_loop
+.Lxts_enc_load_done:
+	ld1	{v26.16b},[x1]
+	eor	v26.16b,v26.16b,v6.16b
+
+	// Encrypt the composite block to get the last second encrypted text block
+	ldr	w6,[x3,#240]		// load key schedule...
+	ld1	{v0.16b},[x3],#16
+	sub	w6,w6,#2
+	ld1	{v1.16b},[x3],#16		// load key schedule...
+.Loop_final_enc:
+	aese	v26.16b,v0.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v0.4s},[x3],#16
+	subs	w6,w6,#2
+	aese	v26.16b,v1.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v1.4s},[x3],#16
+	b.gt	.Loop_final_enc
+
+	aese	v26.16b,v0.16b
+	aesmc	v26.16b,v26.16b
+	ld1	{v0.4s},[x3]
+	aese	v26.16b,v1.16b
+	eor	v26.16b,v26.16b,v0.16b
+	eor	v26.16b,v26.16b,v6.16b
+	st1	{v26.16b},[x1]
+
+.Lxts_abort:
+	ldp	x21,x22,[sp,#48]
+	ldp	d8,d9,[sp,#32]
+	ldp	d10,d11,[sp,#16]
+	ldp	x19,x20,[sp],#64
+.Lxts_enc_final_abort:
+	ret
+.size	aes_v8_xts_encrypt,.-aes_v8_xts_encrypt
+.globl	aes_v8_xts_decrypt
+.type	aes_v8_xts_decrypt,%function
+.align	5
+aes_v8_xts_decrypt:
+	cmp	x2,#16
+	// Original input data size bigger than 16, jump to big size processing.
+	b.ne	.Lxts_dec_big_size
+	// Encrypt the iv with key2, as the first XEX iv.
+	ldr	w6,[x4,#240]
+	ld1	{v0.16b},[x4],#16
+	ld1	{v6.16b},[x5]
+	sub	w6,w6,#2
+	ld1	{v1.16b},[x4],#16
+
+.Loop_dec_small_iv_enc:
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4],#16
+	subs	w6,w6,#2
+	aese	v6.16b,v1.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v1.4s},[x4],#16
+	b.gt	.Loop_dec_small_iv_enc
+
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4]
+	aese	v6.16b,v1.16b
+	eor	v6.16b,v6.16b,v0.16b
+
+	ld1	{v0.16b},[x0]
+	eor	v0.16b,v6.16b,v0.16b
+
+	ldr	w6,[x3,#240]
+	ld1	{v28.4s,v29.4s},[x3],#32			// load key schedule...
+
+	aesd	v0.16b,v28.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v16.4s,v17.4s},[x3],#32			// load key schedule...
+	aesd	v0.16b,v29.16b
+	aesimc	v0.16b,v0.16b
+	subs	w6,w6,#10			// bias
+	b.eq	.Lxts_128_dec
+.Lxts_dec_round_loop:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v16.4s},[x3],#16			// load key schedule...
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v17.4s},[x3],#16			// load key schedule...
+	subs	w6,w6,#2			// bias
+	b.gt	.Lxts_dec_round_loop
+.Lxts_128_dec:
+	ld1	{v18.4s,v19.4s},[x3],#32			// load key schedule...
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v20.4s,v21.4s},[x3],#32			// load key schedule...
+	aesd	v0.16b,v18.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v19.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v22.4s,v23.4s},[x3],#32			// load key schedule...
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	ld1	{v7.4s},[x3]
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v0.16b,v23.16b
+	eor	v0.16b,v0.16b,v7.16b
+	eor	v0.16b,v6.16b,v0.16b
+	st1	{v0.16b},[x1]
+	b	.Lxts_dec_final_abort
+.Lxts_dec_big_size:
+	stp	x19,x20,[sp,#-64]!
+	stp	x21,x22,[sp,#48]
+	stp	d8,d9,[sp,#32]
+	stp	d10,d11,[sp,#16]
+
+	and	x21,x2,#0xf
+	and	x2,x2,#-16
+	subs	x2,x2,#16
+	mov	x8,#16
+	b.lo	.Lxts_dec_abort
+
+	// Encrypt the iv with key2, as the first XEX iv
+	ldr	w6,[x4,#240]
+	ld1	{v0.16b},[x4],#16
+	ld1	{v6.16b},[x5]
+	sub	w6,w6,#2
+	ld1	{v1.16b},[x4],#16
+
+.Loop_dec_iv_enc:
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4],#16
+	subs	w6,w6,#2
+	aese	v6.16b,v1.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v1.4s},[x4],#16
+	b.gt	.Loop_dec_iv_enc
+
+	aese	v6.16b,v0.16b
+	aesmc	v6.16b,v6.16b
+	ld1	{v0.4s},[x4]
+	aese	v6.16b,v1.16b
+	eor	v6.16b,v6.16b,v0.16b
+
+	// The iv for second block
+	// x9- iv(low), x10 - iv(high)
+	// the five ivs stored into, v6.16b,v8.16b,v9.16b,v10.16b,v11.16b
+	fmov	x9,d6
+	fmov	x10,v6.d[1]
+	mov	w19,#0x87
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d8,x9
+	fmov	v8.d[1],x10
+
+	ldr	w5,[x3,#240]		// load rounds number
+
+	// The iv for third block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d9,x9
+	fmov	v9.d[1],x10
+
+	ld1	{v16.4s,v17.4s},[x3]			// load key schedule...
+	sub	w5,w5,#6
+	add	x7,x3,x5,lsl#4		// pointer to last 7 round keys
+	sub	w5,w5,#2
+	ld1	{v18.4s,v19.4s},[x7],#32		// load key schedule...
+	ld1	{v20.4s,v21.4s},[x7],#32
+	ld1	{v22.4s,v23.4s},[x7],#32
+	ld1	{v7.4s},[x7]
+
+	// The iv for fourth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d10,x9
+	fmov	v10.d[1],x10
+
+	add	x7,x3,#32
+	mov	w6,w5
+	b	.Lxts_dec
+
+	// Decryption
+.align	5
+.Lxts_dec:
+	tst	x21,#0xf
+	b.eq	.Lxts_dec_begin
+	subs	x2,x2,#16
+	csel	x8,xzr,x8,eq
+	ld1	{v0.16b},[x0],#16
+	b.lo	.Lxts_done
+	sub	x0,x0,#16
+.Lxts_dec_begin:
+	ld1	{v0.16b},[x0],x8
+	subs	x2,x2,#32			// bias
+	add	w6,w5,#2
+	orr	v3.16b,v0.16b,v0.16b
+	orr	v1.16b,v0.16b,v0.16b
+	orr	v28.16b,v0.16b,v0.16b
+	ld1	{v24.16b},[x0],#16
+	orr	v27.16b,v24.16b,v24.16b
+	orr	v29.16b,v24.16b,v24.16b
+	b.lo	.Lxts_inner_dec_tail
+	eor	v0.16b,v0.16b,v6.16b			// before decryt, xor with iv
+	eor	v24.16b,v24.16b,v8.16b
+
+	orr	v1.16b,v24.16b,v24.16b
+	ld1	{v24.16b},[x0],#16
+	orr	v2.16b,v0.16b,v0.16b
+	orr	v3.16b,v1.16b,v1.16b
+	eor	v27.16b,v24.16b,v9.16b			// third block xox with third iv
+	eor	v24.16b,v24.16b,v9.16b
+	cmp	x2,#32
+	b.lo	.Lxts_outer_dec_tail
+
+	ld1	{v25.16b},[x0],#16
+
+	// The iv for fifth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d11,x9
+	fmov	v11.d[1],x10
+
+	ld1	{v26.16b},[x0],#16
+	eor	v25.16b,v25.16b,v10.16b		// the fourth block
+	eor	v26.16b,v26.16b,v11.16b
+	sub	x2,x2,#32			// bias
+	mov	w6,w5
+	b	.Loop5x_xts_dec
+
+.align	4
+.Loop5x_xts_dec:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v16.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v16.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v16.4s},[x7],#16		// load key schedule...
+	subs	w6,w6,#2
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v17.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v17.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v17.4s},[x7],#16		// load key schedule...
+	b.gt	.Loop5x_xts_dec
+
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v16.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v16.16b
+	aesimc	v26.16b,v26.16b
+	subs	x2,x2,#0x50			// because .Lxts_dec_tail4x
+
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v17.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v17.16b
+	aesimc	v26.16b,v26.16b
+	csel	x6,xzr,x2,gt		// borrow x6, w6, "gt" is not typo
+	mov	x7,x3
+
+	aesd	v0.16b,v18.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v18.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v18.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v18.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v18.16b
+	aesimc	v26.16b,v26.16b
+	add	x0,x0,x6		// x0 is adjusted in such way that
+						// at exit from the loop v1.16b-v26.16b
+						// are loaded with last "words"
+	add	x6,x2,#0x60		// because .Lxts_dec_tail4x
+
+	aesd	v0.16b,v19.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v19.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v19.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v19.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v19.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v20.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v20.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v21.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v21.16b
+	aesimc	v26.16b,v26.16b
+
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v25.16b,v22.16b
+	aesimc	v25.16b,v25.16b
+	aesd	v26.16b,v22.16b
+	aesimc	v26.16b,v26.16b
+
+	eor	v4.16b,v7.16b,v6.16b
+	aesd	v0.16b,v23.16b
+	// The iv for first block of next iteration.
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d6,x9
+	fmov	v6.d[1],x10
+	eor	v5.16b,v7.16b,v8.16b
+	ld1	{v2.16b},[x0],#16
+	aesd	v1.16b,v23.16b
+	// The iv for second block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d8,x9
+	fmov	v8.d[1],x10
+	eor	v17.16b,v7.16b,v9.16b
+	ld1	{v3.16b},[x0],#16
+	aesd	v24.16b,v23.16b
+	// The iv for third block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d9,x9
+	fmov	v9.d[1],x10
+	eor	v30.16b,v7.16b,v10.16b
+	ld1	{v27.16b},[x0],#16
+	aesd	v25.16b,v23.16b
+	// The iv for fourth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d10,x9
+	fmov	v10.d[1],x10
+	eor	v31.16b,v7.16b,v11.16b
+	ld1	{v28.16b},[x0],#16
+	aesd	v26.16b,v23.16b
+
+	// The iv for fifth block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d11,x9
+	fmov	v11.d[1],x10
+
+	ld1	{v29.16b},[x0],#16
+	cbz	x6,.Lxts_dec_tail4x
+	ld1	{v16.4s},[x7],#16		// re-pre-load rndkey[0]
+	eor	v4.16b,v4.16b,v0.16b
+	eor	v0.16b,v2.16b,v6.16b
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v1.16b,v3.16b,v8.16b
+	eor	v17.16b,v17.16b,v24.16b
+	eor	v24.16b,v27.16b,v9.16b
+	eor	v30.16b,v30.16b,v25.16b
+	eor	v25.16b,v28.16b,v10.16b
+	eor	v31.16b,v31.16b,v26.16b
+	st1	{v4.16b},[x1],#16
+	eor	v26.16b,v29.16b,v11.16b
+	st1	{v5.16b},[x1],#16
+	mov	w6,w5
+	st1	{v17.16b},[x1],#16
+	ld1	{v17.4s},[x7],#16		// re-pre-load rndkey[1]
+	st1	{v30.16b},[x1],#16
+	st1	{v31.16b},[x1],#16
+	b.hs	.Loop5x_xts_dec
+
+	cmn	x2,#0x10
+	b.ne	.Loop5x_dec_after
+	// If x2(x2) equal to -0x10, the left blocks is 4.
+	// After specially processing, utilize the five blocks processing again.
+	// It will use the following IVs: v6.16b,v6.16b,v8.16b,v9.16b,v10.16b.
+	orr	v11.16b,v10.16b,v10.16b
+	orr	v10.16b,v9.16b,v9.16b
+	orr	v9.16b,v8.16b,v8.16b
+	orr	v8.16b,v6.16b,v6.16b
+	fmov	x9,d11
+	fmov	x10,v11.d[1]
+	eor	v0.16b,v6.16b,v2.16b
+	eor	v1.16b,v8.16b,v3.16b
+	eor	v24.16b,v27.16b,v9.16b
+	eor	v25.16b,v28.16b,v10.16b
+	eor	v26.16b,v29.16b,v11.16b
+	b.eq	.Loop5x_xts_dec
+
+.Loop5x_dec_after:
+	add	x2,x2,#0x50
+	cbz	x2,.Lxts_done
+
+	add	w6,w5,#2
+	subs	x2,x2,#0x30
+	b.lo	.Lxts_inner_dec_tail
+
+	eor	v0.16b,v6.16b,v27.16b
+	eor	v1.16b,v8.16b,v28.16b
+	eor	v24.16b,v29.16b,v9.16b
+	b	.Lxts_outer_dec_tail
+
+.align	4
+.Lxts_dec_tail4x:
+	add	x0,x0,#16
+	ld1	{v0.4s},[x0],#16
+	eor	v5.16b,v1.16b,v4.16b
+	st1	{v5.16b},[x1],#16
+	eor	v17.16b,v24.16b,v17.16b
+	st1	{v17.16b},[x1],#16
+	eor	v30.16b,v25.16b,v30.16b
+	eor	v31.16b,v26.16b,v31.16b
+	st1	{v30.16b,v31.16b},[x1],#32
+
+	b	.Lxts_done
+.align	4
+.Lxts_outer_dec_tail:
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lxts_outer_dec_tail
+
+	aesd	v0.16b,v16.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	eor	v4.16b,v6.16b,v7.16b
+	subs	x2,x2,#0x30
+	// The iv for first block
+	fmov	x9,d9
+	fmov	x10,v9.d[1]
+	mov	w19,#0x87
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d6,x9
+	fmov	v6.d[1],x10
+	eor	v5.16b,v8.16b,v7.16b
+	csel	x6,x2,x6,lo	// x6, w6, is zero at this point
+	aesd	v0.16b,v17.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	eor	v17.16b,v9.16b,v7.16b
+	// The iv for second block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d8,x9
+	fmov	v8.d[1],x10
+
+	add	x6,x6,#0x20
+	add	x0,x0,x6		// x0 is adjusted to the last data
+
+	mov	x7,x3
+
+	// The iv for third block
+	extr	x22,x10,x10,#32
+	extr	x10,x10,x9,#63
+	and	w11,w19,w22,asr #31
+	eor	x9,x11,x9,lsl #1
+	fmov	d9,x9
+	fmov	v9.d[1],x10
+
+	aesd	v0.16b,v20.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v0.16b,v21.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v0.16b,v22.16b
+	aesimc	v0.16b,v0.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v27.16b},[x0],#16
+	aesd	v0.16b,v23.16b
+	aesd	v1.16b,v23.16b
+	aesd	v24.16b,v23.16b
+	ld1	{v16.4s},[x7],#16		// re-pre-load rndkey[0]
+	add	w6,w5,#2
+	eor	v4.16b,v4.16b,v0.16b
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v24.16b,v24.16b,v17.16b
+	ld1	{v17.4s},[x7],#16		// re-pre-load rndkey[1]
+	st1	{v4.16b},[x1],#16
+	st1	{v5.16b},[x1],#16
+	st1	{v24.16b},[x1],#16
+
+	cmn	x2,#0x30
+	add	x2,x2,#0x30
+	b.eq	.Lxts_done
+	sub	x2,x2,#0x30
+	orr	v28.16b,v3.16b,v3.16b
+	orr	v29.16b,v27.16b,v27.16b
+	nop
+
+.Lxts_inner_dec_tail:
+	// x2 == -0x10 means two blocks left.
+	cmn	x2,#0x10
+	eor	v1.16b,v28.16b,v6.16b
+	eor	v24.16b,v29.16b,v8.16b
+	b.eq	.Lxts_dec_tail_loop
+	eor	v24.16b,v29.16b,v6.16b
+.Lxts_dec_tail_loop:
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v16.4s},[x7],#16
+	subs	w6,w6,#2
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	ld1	{v17.4s},[x7],#16
+	b.gt	.Lxts_dec_tail_loop
+
+	aesd	v1.16b,v16.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v16.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v17.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v17.16b
+	aesimc	v24.16b,v24.16b
+	aesd	v1.16b,v20.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v20.16b
+	aesimc	v24.16b,v24.16b
+	cmn	x2,#0x20
+	aesd	v1.16b,v21.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v21.16b
+	aesimc	v24.16b,v24.16b
+	eor	v5.16b,v6.16b,v7.16b
+	aesd	v1.16b,v22.16b
+	aesimc	v1.16b,v1.16b
+	aesd	v24.16b,v22.16b
+	aesimc	v24.16b,v24.16b
+	eor	v17.16b,v8.16b,v7.16b
+	aesd	v1.16b,v23.16b
+	aesd	v24.16b,v23.16b
+	b.eq	.Lxts_dec_one
+	eor	v5.16b,v5.16b,v1.16b
+	eor	v17.16b,v17.16b,v24.16b
+	orr	v6.16b,v9.16b,v9.16b
+	orr	v8.16b,v10.16b,v10.16b
+	st1	{v5.16b},[x1],#16
+	st1	{v17.16b},[x1],#16
+	add	x2,x2,#16
+	b	.Lxts_done
+
+.Lxts_dec_one:
+	eor	v5.16b,v5.16b,v24.16b
+	orr	v6.16b,v8.16b,v8.16b
+	orr	v8.16b,v9.16b,v9.16b
+	st1	{v5.16b},[x1],#16
+	add	x2,x2,#32
+
+.Lxts_done:
+	tst	x21,#0xf
+	b.eq	.Lxts_dec_abort
+	// Processing the last two blocks with cipher stealing.
+	mov	x7,x3
+	cbnz	x2,.Lxts_dec_1st_done
+	ld1	{v0.4s},[x0],#16
+
+	// Decrypt the last secod block to get the last plain text block
+.Lxts_dec_1st_done:
+	eor	v26.16b,v0.16b,v8.16b
+	ldr	w6,[x3,#240]
+	ld1	{v0.4s},[x3],#16
+	sub	w6,w6,#2
+	ld1	{v1.4s},[x3],#16
+.Loop_final_2nd_dec:
+	aesd	v26.16b,v0.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v0.4s},[x3],#16		// load key schedule...
+	subs	w6,w6,#2
+	aesd	v26.16b,v1.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v1.4s},[x3],#16		// load key schedule...
+	b.gt	.Loop_final_2nd_dec
+
+	aesd	v26.16b,v0.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v0.4s},[x3]
+	aesd	v26.16b,v1.16b
+	eor	v26.16b,v26.16b,v0.16b
+	eor	v26.16b,v26.16b,v8.16b
+	st1	{v26.16b},[x1]
+
+	mov	x20,x0
+	add	x13,x1,#16
+
+	// Composite the tailcnt "16 byte not aligned block" into the last second plain blocks
+	// to get the last encrypted block.
+.composite_dec_loop:
+	subs	x21,x21,#1
+	ldrb	w15,[x1,x21]
+	ldrb	w14,[x20,x21]
+	strb	w15,[x13,x21]
+	strb	w14,[x1,x21]
+	b.gt	.composite_dec_loop
+.Lxts_dec_load_done:
+	ld1	{v26.16b},[x1]
+	eor	v26.16b,v26.16b,v6.16b
+
+	// Decrypt the composite block to get the last second plain text block
+	ldr	w6,[x7,#240]
+	ld1	{v0.16b},[x7],#16
+	sub	w6,w6,#2
+	ld1	{v1.16b},[x7],#16
+.Loop_final_dec:
+	aesd	v26.16b,v0.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v0.4s},[x7],#16		// load key schedule...
+	subs	w6,w6,#2
+	aesd	v26.16b,v1.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v1.4s},[x7],#16		// load key schedule...
+	b.gt	.Loop_final_dec
+
+	aesd	v26.16b,v0.16b
+	aesimc	v26.16b,v26.16b
+	ld1	{v0.4s},[x7]
+	aesd	v26.16b,v1.16b
+	eor	v26.16b,v26.16b,v0.16b
+	eor	v26.16b,v26.16b,v6.16b
+	st1	{v26.16b},[x1]
+
+.Lxts_dec_abort:
+	ldp	x21,x22,[sp,#48]
+	ldp	d8,d9,[sp,#32]
+	ldp	d10,d11,[sp,#16]
+	ldp	x19,x20,[sp],#64
+
+.Lxts_dec_final_abort:
+	ret
+.size	aes_v8_xts_decrypt,.-aes_v8_xts_decrypt
+#endif
diff --git a/os_stub/openssllib/openssl_asm/arm64cpuid.S b/os_stub/openssllib/openssl_asm/arm64cpuid.S
new file mode 100644
index 000000000..7fb4c7507
--- /dev/null
+++ b/os_stub/openssllib/openssl_asm/arm64cpuid.S
@@ -0,0 +1,129 @@
+#include "arm_arch.h"
+
+.text
+.arch	armv8-a+crypto
+
+.align	5
+.globl	_armv7_neon_probe
+.type	_armv7_neon_probe,%function
+_armv7_neon_probe:
+	orr	v15.16b, v15.16b, v15.16b
+	ret
+.size	_armv7_neon_probe,.-_armv7_neon_probe
+
+.globl	_armv7_tick
+.type	_armv7_tick,%function
+_armv7_tick:
+#ifdef	__APPLE__
+	mrs	x0, CNTPCT_EL0
+#else
+	mrs	x0, CNTVCT_EL0
+#endif
+	ret
+.size	_armv7_tick,.-_armv7_tick
+
+.globl	_armv8_aes_probe
+.type	_armv8_aes_probe,%function
+_armv8_aes_probe:
+	aese	v0.16b, v0.16b
+	ret
+.size	_armv8_aes_probe,.-_armv8_aes_probe
+
+.globl	_armv8_sha1_probe
+.type	_armv8_sha1_probe,%function
+_armv8_sha1_probe:
+	sha1h	s0, s0
+	ret
+.size	_armv8_sha1_probe,.-_armv8_sha1_probe
+
+.globl	_armv8_sha256_probe
+.type	_armv8_sha256_probe,%function
+_armv8_sha256_probe:
+	sha256su0	v0.4s, v0.4s
+	ret
+.size	_armv8_sha256_probe,.-_armv8_sha256_probe
+
+.globl	_armv8_pmull_probe
+.type	_armv8_pmull_probe,%function
+_armv8_pmull_probe:
+	pmull	v0.1q, v0.1d, v0.1d
+	ret
+.size	_armv8_pmull_probe,.-_armv8_pmull_probe
+
+.globl	_armv8_sha512_probe
+.type	_armv8_sha512_probe,%function
+_armv8_sha512_probe:
+.long	0xcec08000	// sha512su0	v0.2d,v0.2d
+	ret
+.size	_armv8_sha512_probe,.-_armv8_sha512_probe
+
+.globl	_armv8_cpuid_probe
+.type	_armv8_cpuid_probe,%function
+_armv8_cpuid_probe:
+	mrs	x0, midr_el1
+	ret
+.size	_armv8_cpuid_probe,.-_armv8_cpuid_probe
+
+.globl	OPENSSL_cleanse
+.type	OPENSSL_cleanse,%function
+.align	5
+OPENSSL_cleanse:
+	cbz	x1,.Lret	// len==0?
+	cmp	x1,#15
+	b.hi	.Lot		// len>15
+	nop
+.Little:
+	strb	wzr,[x0],#1	// store byte-by-byte
+	subs	x1,x1,#1
+	b.ne	.Little
+.Lret:	ret
+
+.align	4
+.Lot:	tst	x0,#7
+	b.eq	.Laligned	// inp is aligned
+	strb	wzr,[x0],#1	// store byte-by-byte
+	sub	x1,x1,#1
+	b	.Lot
+
+.align	4
+.Laligned:
+	str	xzr,[x0],#8	// store word-by-word
+	sub	x1,x1,#8
+	tst	x1,#-8
+	b.ne	.Laligned	// len>=8
+	cbnz	x1,.Little	// len!=0?
+	ret
+.size	OPENSSL_cleanse,.-OPENSSL_cleanse
+
+.globl	CRYPTO_memcmp
+.type	CRYPTO_memcmp,%function
+.align	4
+CRYPTO_memcmp:
+	eor	w3,w3,w3
+	cbz	x2,.Lno_data	// len==0?
+	cmp	x2,#16
+	b.ne	.Loop_cmp
+	ldp	x8,x9,[x0]
+	ldp	x10,x11,[x1]
+	eor	x8,x8,x10
+	eor	x9,x9,x11
+	orr	x8,x8,x9
+	mov	x0,#1
+	cmp	x8,#0
+	csel	x0,xzr,x0,eq
+	ret
+
+.align	4
+.Loop_cmp:
+	ldrb	w4,[x0],#1
+	ldrb	w5,[x1],#1
+	eor	w4,w4,w5
+	orr	w3,w3,w4
+	subs	x2,x2,#1
+	b.ne	.Loop_cmp
+
+.Lno_data:
+	neg	w0,w3
+	lsr	w0,w0,#31
+	ret
+.size	CRYPTO_memcmp,.-CRYPTO_memcmp
diff --git a/os_stub/openssllib/openssl_asm/ghashv8-armx.S b/os_stub/openssllib/openssl_asm/ghashv8-armx.S
new file mode 100644
index 000000000..acd52eb95
--- /dev/null
+++ b/os_stub/openssllib/openssl_asm/ghashv8-armx.S
@@ -0,0 +1,552 @@
+#include "arm_arch.h"
+
+#if __ARM_MAX_ARCH__>=7
+.arch	armv8-a+crypto
+.text
+.globl	gcm_init_v8
+.type	gcm_init_v8,%function
+.align	4
+gcm_init_v8:
+	ld1	{v17.2d},[x1]		//load input H
+	movi	v19.16b,#0xe1
+	shl	v19.2d,v19.2d,#57		//0xc2.0
+	ext	v3.16b,v17.16b,v17.16b,#8
+	ushr	v18.2d,v19.2d,#63
+	dup	v17.4s,v17.s[1]
+	ext	v16.16b,v18.16b,v19.16b,#8		//t0=0xc2....01
+	ushr	v18.2d,v3.2d,#63
+	sshr	v17.4s,v17.4s,#31		//broadcast carry bit
+	and	v18.16b,v18.16b,v16.16b
+	shl	v3.2d,v3.2d,#1
+	ext	v18.16b,v18.16b,v18.16b,#8
+	and	v16.16b,v16.16b,v17.16b
+	orr	v3.16b,v3.16b,v18.16b		//H<<<=1
+	eor	v20.16b,v3.16b,v16.16b		//twisted H
+	st1	{v20.2d},[x0],#16		//store Htable[0]
+
+	//calculate H^2
+	ext	v16.16b,v20.16b,v20.16b,#8		//Karatsuba pre-processing
+	pmull	v0.1q,v20.1d,v20.1d
+	eor	v16.16b,v16.16b,v20.16b
+	pmull2	v2.1q,v20.2d,v20.2d
+	pmull	v1.1q,v16.1d,v16.1d
+
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	eor	v1.16b,v1.16b,v18.16b
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase
+
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	eor	v0.16b,v1.16b,v18.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v22.16b,v0.16b,v18.16b
+
+	ext	v17.16b,v22.16b,v22.16b,#8		//Karatsuba pre-processing
+	eor	v17.16b,v17.16b,v22.16b
+	ext	v21.16b,v16.16b,v17.16b,#8		//pack Karatsuba pre-processed
+	st1	{v21.2d,v22.2d},[x0],#32	//store Htable[1..2]
+	//calculate H^3 and H^4
+	pmull	v0.1q,v20.1d, v22.1d
+	pmull	v5.1q,v22.1d,v22.1d
+	pmull2	v2.1q,v20.2d, v22.2d
+	pmull2	v7.1q,v22.2d,v22.2d
+	pmull	v1.1q,v16.1d,v17.1d
+	pmull	v6.1q,v17.1d,v17.1d
+
+	ext	v16.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	ext	v17.16b,v5.16b,v7.16b,#8
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v16.16b
+	eor	v4.16b,v5.16b,v7.16b
+	eor	v6.16b,v6.16b,v17.16b
+	eor	v1.16b,v1.16b,v18.16b
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase
+	eor	v6.16b,v6.16b,v4.16b
+	pmull	v4.1q,v5.1d,v19.1d
+
+	ins	v2.d[0],v1.d[1]
+	ins	v7.d[0],v6.d[1]
+	ins	v1.d[1],v0.d[0]
+	ins	v6.d[1],v5.d[0]
+	eor	v0.16b,v1.16b,v18.16b
+	eor	v5.16b,v6.16b,v4.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase
+	ext	v4.16b,v5.16b,v5.16b,#8
+	pmull	v0.1q,v0.1d,v19.1d
+	pmull	v5.1q,v5.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v4.16b,v4.16b,v7.16b
+	eor	v20.16b, v0.16b,v18.16b		//H^3
+	eor	v22.16b,v5.16b,v4.16b		//H^4
+
+	ext	v16.16b,v20.16b, v20.16b,#8		//Karatsuba pre-processing
+	ext	v17.16b,v22.16b,v22.16b,#8
+	eor	v16.16b,v16.16b,v20.16b
+	eor	v17.16b,v17.16b,v22.16b
+	ext	v21.16b,v16.16b,v17.16b,#8		//pack Karatsuba pre-processed
+	st1	{v20.2d,v21.2d,v22.2d},[x0]		//store Htable[3..5]
+	ret
+.size	gcm_init_v8,.-gcm_init_v8
+.globl	gcm_gmult_v8
+.type	gcm_gmult_v8,%function
+.align	4
+gcm_gmult_v8:
+	ld1	{v17.2d},[x0]		//load Xi
+	movi	v19.16b,#0xe1
+	ld1	{v20.2d,v21.2d},[x1]	//load twisted H, ...
+	shl	v19.2d,v19.2d,#57
+#ifndef __ARMEB__
+	rev64	v17.16b,v17.16b
+#endif
+	ext	v3.16b,v17.16b,v17.16b,#8
+
+	pmull	v0.1q,v20.1d,v3.1d		//H.loXi.lo
+	eor	v17.16b,v17.16b,v3.16b		//Karatsuba pre-processing
+	pmull2	v2.1q,v20.2d,v3.2d		//H.hiXi.hi
+	pmull	v1.1q,v21.1d,v17.1d		//(H.lo+H.hi)(Xi.lo+Xi.hi)
+
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	eor	v1.16b,v1.16b,v18.16b
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	eor	v0.16b,v1.16b,v18.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v0.16b,v0.16b,v18.16b
+
+#ifndef __ARMEB__
+	rev64	v0.16b,v0.16b
+#endif
+	ext	v0.16b,v0.16b,v0.16b,#8
+	st1	{v0.2d},[x0]		//write out Xi
+
+	ret
+.size	gcm_gmult_v8,.-gcm_gmult_v8
+.globl	gcm_ghash_v8
+.type	gcm_ghash_v8,%function
+.align	4
+gcm_ghash_v8:
+	cmp	x3,#64
+	b.hs	.Lgcm_ghash_v8_4x
+	ld1	{v0.2d},[x0]		//load [rotated] Xi
+						//"[rotated]" means that
+						//loaded value would have
+						//to be rotated in order to
+						//make it appear as in
+						//algorithm specification
+	subs	x3,x3,#32		//see if x3 is 32 or larger
+	mov	x12,#16		//x12 is used as post-
+						//increment for input pointer;
+						//as loop is modulo-scheduled
+						//x12 is zeroed just in time
+						//to preclude overstepping
+						//inp[len], which means that
+						//last block[s] are actually
+						//loaded twice, but last
+						//copy is not processed
+	ld1	{v20.2d,v21.2d},[x1],#32	//load twisted H, ..., H^2
+	movi	v19.16b,#0xe1
+	ld1	{v22.2d},[x1]
+	csel	x12,xzr,x12,eq			//is it time to zero x12?
+	ext	v0.16b,v0.16b,v0.16b,#8		//rotate Xi
+	ld1	{v16.2d},[x2],#16	//load [rotated] I[0]
+	shl	v19.2d,v19.2d,#57		//compose 0xc2.0 constant
+#ifndef __ARMEB__
+	rev64	v16.16b,v16.16b
+	rev64	v0.16b,v0.16b
+#endif
+	ext	v3.16b,v16.16b,v16.16b,#8		//rotate I[0]
+	b.lo	.Lodd_tail_v8		//x3 was less than 32
+	ld1	{v17.2d},[x2],x12	//load [rotated] I[1]
+#ifndef __ARMEB__
+	rev64	v17.16b,v17.16b
+#endif
+	ext	v7.16b,v17.16b,v17.16b,#8
+	eor	v3.16b,v3.16b,v0.16b		//I[i]^=Xi
+	pmull	v4.1q,v20.1d,v7.1d		//HIi+1
+	eor	v17.16b,v17.16b,v7.16b		//Karatsuba pre-processing
+	pmull2	v6.1q,v20.2d,v7.2d
+	b	.Loop_mod2x_v8
+
+.align	4
+.Loop_mod2x_v8:
+	ext	v18.16b,v3.16b,v3.16b,#8
+	subs	x3,x3,#32		//is there more data?
+	pmull	v0.1q,v22.1d,v3.1d		//H^2.loXi.lo
+	csel	x12,xzr,x12,lo			//is it time to zero x12?
+
+	pmull	v5.1q,v21.1d,v17.1d
+	eor	v18.16b,v18.16b,v3.16b		//Karatsuba pre-processing
+	pmull2	v2.1q,v22.2d,v3.2d		//H^2.hiXi.hi
+	eor	v0.16b,v0.16b,v4.16b		//accumulate
+	pmull2	v1.1q,v21.2d,v18.2d		//(H^2.lo+H^2.hi)(Xi.lo+Xi.hi)
+	ld1	{v16.2d},[x2],x12	//load [rotated] I[i+2]
+
+	eor	v2.16b,v2.16b,v6.16b
+	csel	x12,xzr,x12,eq			//is it time to zero x12?
+	eor	v1.16b,v1.16b,v5.16b
+
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	ld1	{v17.2d},[x2],x12	//load [rotated] I[i+3]
+#ifndef __ARMEB__
+	rev64	v16.16b,v16.16b
+#endif
+	eor	v1.16b,v1.16b,v18.16b
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+
+#ifndef __ARMEB__
+	rev64	v17.16b,v17.16b
+#endif
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	ext	v7.16b,v17.16b,v17.16b,#8
+	ext	v3.16b,v16.16b,v16.16b,#8
+	eor	v0.16b,v1.16b,v18.16b
+	pmull	v4.1q,v20.1d,v7.1d		//HIi+1
+	eor	v3.16b,v3.16b,v2.16b		//accumulate v3.16b early
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v3.16b,v3.16b,v18.16b
+	eor	v17.16b,v17.16b,v7.16b		//Karatsuba pre-processing
+	eor	v3.16b,v3.16b,v0.16b
+	pmull2	v6.1q,v20.2d,v7.2d
+	b.hs	.Loop_mod2x_v8		//there was at least 32 more bytes
+
+	eor	v2.16b,v2.16b,v18.16b
+	ext	v3.16b,v16.16b,v16.16b,#8		//re-construct v3.16b
+	adds	x3,x3,#32		//re-construct x3
+	eor	v0.16b,v0.16b,v2.16b		//re-construct v0.16b
+	b.eq	.Ldone_v8		//is x3 zero?
+.Lodd_tail_v8:
+	ext	v18.16b,v0.16b,v0.16b,#8
+	eor	v3.16b,v3.16b,v0.16b		//inp^=Xi
+	eor	v17.16b,v16.16b,v18.16b		//v17.16b is rotated inp^Xi
+
+	pmull	v0.1q,v20.1d,v3.1d		//H.loXi.lo
+	eor	v17.16b,v17.16b,v3.16b		//Karatsuba pre-processing
+	pmull2	v2.1q,v20.2d,v3.2d		//H.hiXi.hi
+	pmull	v1.1q,v21.1d,v17.1d		//(H.lo+H.hi)(Xi.lo+Xi.hi)
+
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	eor	v1.16b,v1.16b,v18.16b
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	eor	v0.16b,v1.16b,v18.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v0.16b,v0.16b,v18.16b
+
+.Ldone_v8:
+#ifndef __ARMEB__
+	rev64	v0.16b,v0.16b
+#endif
+	ext	v0.16b,v0.16b,v0.16b,#8
+	st1	{v0.2d},[x0]		//write out Xi
+
+	ret
+.size	gcm_ghash_v8,.-gcm_ghash_v8
+.type	gcm_ghash_v8_4x,%function
+.align	4
+gcm_ghash_v8_4x:
+.Lgcm_ghash_v8_4x:
+	ld1	{v0.2d},[x0]		//load [rotated] Xi
+	ld1	{v20.2d,v21.2d,v22.2d},[x1],#48	//load twisted H, ..., H^2
+	movi	v19.16b,#0xe1
+	ld1	{v26.2d,v27.2d,v28.2d},[x1]	//load twisted H^3, ..., H^4
+	shl	v19.2d,v19.2d,#57		//compose 0xc2.0 constant
+
+	ld1	{v4.2d,v5.2d,v6.2d,v7.2d},[x2],#64
+#ifndef __ARMEB__
+	rev64	v0.16b,v0.16b
+	rev64	v5.16b,v5.16b
+	rev64	v6.16b,v6.16b
+	rev64	v7.16b,v7.16b
+	rev64	v4.16b,v4.16b
+#endif
+	ext	v25.16b,v7.16b,v7.16b,#8
+	ext	v24.16b,v6.16b,v6.16b,#8
+	ext	v23.16b,v5.16b,v5.16b,#8
+
+	pmull	v29.1q,v20.1d,v25.1d		//HIi+3
+	eor	v7.16b,v7.16b,v25.16b
+	pmull2	v31.1q,v20.2d,v25.2d
+	pmull	v30.1q,v21.1d,v7.1d
+
+	pmull	v16.1q,v22.1d,v24.1d		//H^2Ii+2
+	eor	v6.16b,v6.16b,v24.16b
+	pmull2	v24.1q,v22.2d,v24.2d
+	pmull2	v6.1q,v21.2d,v6.2d
+
+	eor	v29.16b,v29.16b,v16.16b
+	eor	v31.16b,v31.16b,v24.16b
+	eor	v30.16b,v30.16b,v6.16b
+
+	pmull	v7.1q,v26.1d,v23.1d		//H^3Ii+1
+	eor	v5.16b,v5.16b,v23.16b
+	pmull2	v23.1q,v26.2d,v23.2d
+	pmull	v5.1q,v27.1d,v5.1d
+
+	eor	v29.16b,v29.16b,v7.16b
+	eor	v31.16b,v31.16b,v23.16b
+	eor	v30.16b,v30.16b,v5.16b
+
+	subs	x3,x3,#128
+	b.lo	.Ltail4x
+
+	b	.Loop4x
+
+.align	4
+.Loop4x:
+	eor	v16.16b,v4.16b,v0.16b
+	ld1	{v4.2d,v5.2d,v6.2d,v7.2d},[x2],#64
+	ext	v3.16b,v16.16b,v16.16b,#8
+#ifndef __ARMEB__
+	rev64	v5.16b,v5.16b
+	rev64	v6.16b,v6.16b
+	rev64	v7.16b,v7.16b
+	rev64	v4.16b,v4.16b
+#endif
+
+	pmull	v0.1q,v28.1d,v3.1d		//H^4(Xi+Ii)
+	eor	v16.16b,v16.16b,v3.16b
+	pmull2	v2.1q,v28.2d,v3.2d
+	ext	v25.16b,v7.16b,v7.16b,#8
+	pmull2	v1.1q,v27.2d,v16.2d
+
+	eor	v0.16b,v0.16b,v29.16b
+	eor	v2.16b,v2.16b,v31.16b
+	ext	v24.16b,v6.16b,v6.16b,#8
+	eor	v1.16b,v1.16b,v30.16b
+	ext	v23.16b,v5.16b,v5.16b,#8
+
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	pmull	v29.1q,v20.1d,v25.1d		//HIi+3
+	eor	v7.16b,v7.16b,v25.16b
+	eor	v1.16b,v1.16b,v17.16b
+	pmull2	v31.1q,v20.2d,v25.2d
+	eor	v1.16b,v1.16b,v18.16b
+	pmull	v30.1q,v21.1d,v7.1d
+
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	pmull	v16.1q,v22.1d,v24.1d		//H^2Ii+2
+	eor	v6.16b,v6.16b,v24.16b
+	pmull2	v24.1q,v22.2d,v24.2d
+	eor	v0.16b,v1.16b,v18.16b
+	pmull2	v6.1q,v21.2d,v6.2d
+
+	eor	v29.16b,v29.16b,v16.16b
+	eor	v31.16b,v31.16b,v24.16b
+	eor	v30.16b,v30.16b,v6.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	pmull	v7.1q,v26.1d,v23.1d		//H^3Ii+1
+	eor	v5.16b,v5.16b,v23.16b
+	eor	v18.16b,v18.16b,v2.16b
+	pmull2	v23.1q,v26.2d,v23.2d
+	pmull	v5.1q,v27.1d,v5.1d
+
+	eor	v0.16b,v0.16b,v18.16b
+	eor	v29.16b,v29.16b,v7.16b
+	eor	v31.16b,v31.16b,v23.16b
+	ext	v0.16b,v0.16b,v0.16b,#8
+	eor	v30.16b,v30.16b,v5.16b
+
+	subs	x3,x3,#64
+	b.hs	.Loop4x
+
+.Ltail4x:
+	eor	v16.16b,v4.16b,v0.16b
+	ext	v3.16b,v16.16b,v16.16b,#8
+
+	pmull	v0.1q,v28.1d,v3.1d		//H^4(Xi+Ii)
+	eor	v16.16b,v16.16b,v3.16b
+	pmull2	v2.1q,v28.2d,v3.2d
+	pmull2	v1.1q,v27.2d,v16.2d
+
+	eor	v0.16b,v0.16b,v29.16b
+	eor	v2.16b,v2.16b,v31.16b
+	eor	v1.16b,v1.16b,v30.16b
+
+	adds	x3,x3,#64
+	b.eq	.Ldone4x
+
+	cmp	x3,#32
+	b.lo	.Lone
+	b.eq	.Ltwo
+.Lthree:
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	ld1	{v4.2d,v5.2d,v6.2d},[x2]
+	eor	v1.16b,v1.16b,v18.16b
+#ifndef	__ARMEB__
+	rev64	v5.16b,v5.16b
+	rev64	v6.16b,v6.16b
+	rev64	v4.16b,v4.16b
+#endif
+
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	ext	v24.16b,v6.16b,v6.16b,#8
+	ext	v23.16b,v5.16b,v5.16b,#8
+	eor	v0.16b,v1.16b,v18.16b
+
+	pmull	v29.1q,v20.1d,v24.1d		//HIi+2
+	eor	v6.16b,v6.16b,v24.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	pmull2	v31.1q,v20.2d,v24.2d
+	pmull	v30.1q,v21.1d,v6.1d
+	eor	v0.16b,v0.16b,v18.16b
+	pmull	v7.1q,v22.1d,v23.1d		//H^2Ii+1
+	eor	v5.16b,v5.16b,v23.16b
+	ext	v0.16b,v0.16b,v0.16b,#8
+
+	pmull2	v23.1q,v22.2d,v23.2d
+	eor	v16.16b,v4.16b,v0.16b
+	pmull2	v5.1q,v21.2d,v5.2d
+	ext	v3.16b,v16.16b,v16.16b,#8
+
+	eor	v29.16b,v29.16b,v7.16b
+	eor	v31.16b,v31.16b,v23.16b
+	eor	v30.16b,v30.16b,v5.16b
+
+	pmull	v0.1q,v26.1d,v3.1d		//H^3(Xi+Ii)
+	eor	v16.16b,v16.16b,v3.16b
+	pmull2	v2.1q,v26.2d,v3.2d
+	pmull	v1.1q,v27.1d,v16.1d
+
+	eor	v0.16b,v0.16b,v29.16b
+	eor	v2.16b,v2.16b,v31.16b
+	eor	v1.16b,v1.16b,v30.16b
+	b	.Ldone4x
+
+.align	4
+.Ltwo:
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	ld1	{v4.2d,v5.2d},[x2]
+	eor	v1.16b,v1.16b,v18.16b
+#ifndef	__ARMEB__
+	rev64	v5.16b,v5.16b
+	rev64	v4.16b,v4.16b
+#endif
+
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	ext	v23.16b,v5.16b,v5.16b,#8
+	eor	v0.16b,v1.16b,v18.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v0.16b,v0.16b,v18.16b
+	ext	v0.16b,v0.16b,v0.16b,#8
+
+	pmull	v29.1q,v20.1d,v23.1d		//HIi+1
+	eor	v5.16b,v5.16b,v23.16b
+
+	eor	v16.16b,v4.16b,v0.16b
+	ext	v3.16b,v16.16b,v16.16b,#8
+
+	pmull2	v31.1q,v20.2d,v23.2d
+	pmull	v30.1q,v21.1d,v5.1d
+
+	pmull	v0.1q,v22.1d,v3.1d		//H^2(Xi+Ii)
+	eor	v16.16b,v16.16b,v3.16b
+	pmull2	v2.1q,v22.2d,v3.2d
+	pmull2	v1.1q,v21.2d,v16.2d
+
+	eor	v0.16b,v0.16b,v29.16b
+	eor	v2.16b,v2.16b,v31.16b
+	eor	v1.16b,v1.16b,v30.16b
+	b	.Ldone4x
+
+.align	4
+.Lone:
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	ld1	{v4.2d},[x2]
+	eor	v1.16b,v1.16b,v18.16b
+#ifndef	__ARMEB__
+	rev64	v4.16b,v4.16b
+#endif
+
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	eor	v0.16b,v1.16b,v18.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v0.16b,v0.16b,v18.16b
+	ext	v0.16b,v0.16b,v0.16b,#8
+
+	eor	v16.16b,v4.16b,v0.16b
+	ext	v3.16b,v16.16b,v16.16b,#8
+
+	pmull	v0.1q,v20.1d,v3.1d
+	eor	v16.16b,v16.16b,v3.16b
+	pmull2	v2.1q,v20.2d,v3.2d
+	pmull	v1.1q,v21.1d,v16.1d
+
+.Ldone4x:
+	ext	v17.16b,v0.16b,v2.16b,#8		//Karatsuba post-processing
+	eor	v18.16b,v0.16b,v2.16b
+	eor	v1.16b,v1.16b,v17.16b
+	eor	v1.16b,v1.16b,v18.16b
+
+	pmull	v18.1q,v0.1d,v19.1d		//1st phase of reduction
+	ins	v2.d[0],v1.d[1]
+	ins	v1.d[1],v0.d[0]
+	eor	v0.16b,v1.16b,v18.16b
+
+	ext	v18.16b,v0.16b,v0.16b,#8		//2nd phase of reduction
+	pmull	v0.1q,v0.1d,v19.1d
+	eor	v18.16b,v18.16b,v2.16b
+	eor	v0.16b,v0.16b,v18.16b
+	ext	v0.16b,v0.16b,v0.16b,#8
+
+#ifndef __ARMEB__
+	rev64	v0.16b,v0.16b
+#endif
+	st1	{v0.2d},[x0]		//write out Xi
+
+	ret
+.size	gcm_ghash_v8_4x,.-gcm_ghash_v8_4x
+.byte	71,72,65,83,72,32,102,111,114,32,65,82,77,118,56,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+.align	2
+.align	2
+#endif
diff --git a/os_stub/openssllib/process_files.pl b/os_stub/openssllib/process_files.pl
index 579d4e65a..69aed34db 100644
--- a/os_stub/openssllib/process_files.pl
+++ b/os_stub/openssllib/process_files.pl
@@ -35,7 +35,6 @@ BEGIN {
         "./Configure",
         "UEFI",
         "no-afalgeng",
-        "no-asm",
         "no-async",
         "no-autoalginit",
         "no-autoerrinit",
-- 
2.17.1

